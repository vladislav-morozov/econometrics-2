---
title: "Consistency of the OLS Estimator"
subtitle: "Convergence as Sample Size Grows"
author: Vladislav Morozov  
format:
  revealjs:
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "A Deeper Look at Linear Regression: Consistency"
    footer-logo-link: "https://vladislav-morozov.github.io/econometrics-2/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#045D5D"
    data-footer: " "
filters:
  - reveal-header  
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
---



## Introduction {background="#00100F"}
  
### Lecture Info {background="#43464B" visibility="uncounted"}


#### Learning Outcomes

This lecture is about a consistency in general and consistency of the OLS estimator

<br>

By the end, you should be able to

- Provide definitions of convergence in probability and consistency 
- Handle the question of invertibility of $\bX'\bX$
- Derive consistency results for the OLS estimator 


```{python}
import matplotlib.animation as animation
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np


from pathlib import Path
from scipy.ndimage import uniform_filter1d
from statsmodels.distributions.empirical_distribution import ECDF


plt.rcParams["font.family"] = "sans-serif"
plt.rcParams["font.sans-serif"] = ["Arial"]

BG_COLOR = "whitesmoke"

```

#### Textbook References
 

::: {.nonincremental}

 
- Refresher on probability: 
    - Your favorite probability textbook (e.g. chapter 5 in @Bertsekas2008IntroductionProbability)
    - Sections B-C in @Wooldridge2020IntroductoryEconometricsModern
- Consistency of the OLS estimator
    - 7.1-7.2 in @Hansen2022Econometrics
    - *Or* 5.1 and E4 in @Wooldridge2020IntroductoryEconometricsModern
    

  
::: 

#### Consistency as Basic Requirement
 
Want estimators with good properties 

. . . 

<br>

<span class =  "highlight">Consistency</span> is a minimal required property for a "sensible" estimator

. . .

<br>

Informally:
<div class="rounded-box">
  An estimation procedure is consistent if it get the target parameter right as sample size grows infinite large
</div>


## Probability Background {background="#00100F"}

### Definitions {background="#43464B" visibility="uncounted"}


#### Reminder: Convergence of a Deterministic Sequence


Recall: 

<div class="rounded-box">

::: {#def-vector-consistency-deterministic-convergence}

Let $\bx_1, \bx_2, \dots$ be a sequence of vectors in $\R^p$. Then $\bx_n$ converges to some $\bx\in \R^p$  if for any $\varepsilon>0$ there exists an $N_0$ such that for all $N\geq N_0$
$$
\norm{ \bx_N - \bx } < \varepsilon
$$

:::

</div>
 
Here $\norm{\cdot}$ is the Euclidean norm on $\R^p$: if $\by = (y_1, \dots, y_p)$, then $\norm{\by} = \sqrt{ \sum_{i=1}^p y_i^2   }$
 


#### Towards Formalizing Convergence

- Let $\theta\in \R^p$
- Sample of size $N$ is $(X_1, \dots, X_N)$
- Recall: estimators $\curl{\hat{\theta}_k}_{k=\min N}^{\infty}$ are a sequence of functions. $N$th estimator maps $(X_1, \dots, X_N)$ to $\R^p$ to produce *estimates*

. . .

<br> 

Sample is random $\Rightarrow$ each $\hat{\theta}_N(X_1, \dots, X_N)$ is random

. . .

$\Rightarrow$ How to formalize convergence?

::: footer
See [Wikipedia](https://en.wikipedia.org/wiki/Convergence_of_random_variables)
:::
 

#### Convergence in Probability: Definition

<br> 

<div class="rounded-box">

::: {#def-vector-consistency-convergence-ip}

Let $\bX_1, \bX_2, \dots$ be a sequence of random matrices in $\R^{k\times p}$. Then $\bX_N$ converges to some $\bX\in \R^{k\times p}$ <span class="highlight">in probability</span> if for any $\varepsilon>0$  it holds that 
$$
\lim_{N\to\infty} P(\norm{\bX_N - \bX}>\varepsilon) = 0
$$

:::

</div>

#### Convergence in Probability: Discussion

<br>


- The limit $\bX$ can be random or deterministic 
- Convergence in probability written $\bX_n\xrightarrow{p}\bX$
- $\bX_N\xrightarrow{p} \bX$ is the same as $\bX_N-\bX\xrightarrow{p} 0$

#### Two Important Characterizations 


<div class="rounded-box">

::: {#prp-vector-consistency-characterizations-vector}

Let $\bA_N, \bA$ be a $m \times n$ matrices with $(i, j)$th element $a_{ij, N}$ and $a_{ij}$. Then
$$
\bA_N\xrightarrow{p}\bA   \Leftrightarrow a_{ij, N} \xrightarrow a_{ij}
$$
:::

</div>

. . . 

<br>

<div class="rounded-box">

::: {#prp-vector-consistency-characterizations-open}

$\bX_N\xrightarrow{p}\bX$ if and only if $P(\bX_N\in U)\to 1$ for any open set $U$ that contains $\bX$
:::

</div>

 



#### Definition of Consistency

<div class="rounded-box">

::: {#def-vector-consistency-consistency}

The estimator (sequence) $\hat{\theta}_N$ is consistent for $\theta$ if as $N\to\infty$ 
$$
\hat{\theta}_N(X_1, \dots, X_N) \xrightarrow{p} \theta
$$

:::

</div>

Note: we usually use the word "estimator" to refer to the whole sequence $\curl{\hat{\theta}_N}$

### Tools for Showing Consistency {background="#43464B" visibility="uncounted"}

#### Two Approaches To Showing Consistency

1. Qualitative: just that convergence happens
  - Relies on laws of large numbers (LLNs) and related results
  - Approach in this course
2. Quantitive: shows that convergence happens and answers *how fast*
  - Usually based on <span class="highlight">concentration inequalities</span>
  - Check out chapter 2 in @Wainwright2019HighDimensionalStatisticsNonAsymptotic


::: footer
Wikipedia gives a [list](https://en.wikipedia.org/wiki/Concentration_inequality) of some concentration inequalities 
:::

#### Tool: (Weak) Law of Large Numbers
 

<div class="rounded-box">

::: {#prp-vector-consistency-lln}

Let $\bX_1, \bX_2, \dots$ be a sequence of random vectors such that

1. $\bX_i$ are independently and identically distributed (IID)
2. $\E[\norm{\bX_i}]<\infty$

Then 
$$ \small
\dfrac{1}{N} \sum_{i=1}^N \bX_i \xrightarrow{p} \E[\bX_i]
$$
:::

</div>
 
 

#### Tool: Continuous Mapping Theorem



<div class="rounded-box">

::: {#prp-vector-consistency-cmt}

Let $\bX_N\xrightarrow{p}\bX$, and let $f(\cdot)$ be continuous is some neighborhood of all the possible values of  $\bX$.

Then  
$$
f(\bX_N) \xrightarrow{p} f(\bX)
$$
:::

</div>
In words: convergence in probability is preserved under continuous transformations


#### CMT Examples

Simple examples

1. If $X_N$ is scalar and $X_n\xrightarrow{p}X$, then 
   - $X_N^2 \xrightarrow{p} X^2$
   - $\max\curl{0, X_N}\xrightarrow \max\curl{0, X}$
2. If $\bX_N\to \bX$, $\bX_N\in \R^p$ and $\bA_N\xrightarrow{p}\bA$, $\bA_N\in\R^{k\times p}$, then 
   $$
   \bA_N\bX_N\xrightarrow{p} \bA\bX
   $$


#### Visualizing the Law of Large Numbers

```{python}
#| eval: false

# Parameters
a, b = 2, 5  # Parameters for the beta distribution
sample_sizes = np.concatenate(
    [
        np.arange(1, 101),  # Every number between 1 and 100
        np.arange(100, 201, 4),  # Every 2nd number between 100 and 200
        np.arange(200, 501, 5),  # Every 3rd number between 200 and 500
        np.arange(500, 1001, 8),  # Every 5th number between 500 and 1000
    ]
).astype(int)
num_samples = 70000

# Generate data from a beta distribution
data = np.random.beta(a, b, (num_samples, max(sample_sizes)))

# Standardize the data
data_mean = np.mean(data)
data_std = np.std(data)
standardized_data = (data - data_mean) / data_std

# Compute sample means on the standardized data
sample_means = [np.mean(standardized_data[:, :n], axis=1) for n in sample_sizes]

# Compute probabilities for the right subplot
xvals = np.linspace(-0.5, 0.5, 100)

prob_01 = [np.mean(np.abs(means) > 0.1) for means in sample_means]
prob_001 = [np.mean(np.abs(means) > 0.01) for means in sample_means]

# Set up the figure and the axes
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
fig.patch.set_facecolor(BG_COLOR)
fig.patch.set_edgecolor("teal")
fig.patch.set_linewidth(5)
 
def animate(i):
    ax1.clear()
    ecdf = ECDF(sample_means[i])
    ax1.axvline(x=0, color="gainsboro", linestyle="--")  
    ax1.plot(xvals, ecdf(xvals), "g-")
    ax1.set_xlim(xvals[[0, -1]])
    ax1.set_ylim(-0.03, 1.03)
    ax1.set_xlabel("Standard deviations of the data")
    ax1.set_title(
        "Cumulative Distribution Function of Sample Mean",
        loc="left",
    )
    ax1.set_facecolor(BG_COLOR)

    ax2.clear()
    ax2.plot(
        sample_sizes[: i + 1],
        prob_01[: i + 1],
        color='teal',
        label="$P(|\\bar{X} - \\mu| > 0.1\\sigma)$",
    )
    ax2.plot(
        sample_sizes[: i + 1],
        prob_001[: i + 1],
        color='darkorange',
        label="$P(|\\bar{X} - \\mu| > 0.01\\sigma)$",
    )
    ax2.set_xlim(min(sample_sizes), max(sample_sizes))
    ax2.set_ylim(-0.03, 1.03)
    ax2.set_xlabel("Sample Size")
    ax2.set_title(
        "Probability of Deviations from True Mean",
        loc="left",
    )
    ax2.legend(loc="upper right")
    ax2.set_facecolor(BG_COLOR)

# Call the animator
frames_total = len(sample_sizes) 
ani = animation.FuncAnimation(fig, animate, frames=frames_total, repeat=True)

 
WriterMP4 = animation.writers['ffmpeg']
writer_mp4 = WriterMP4(fps=50, metadata=dict(artist='bww'), bitrate=1800) 
ani.save(
     Path("images").resolve() / "lln.mp4",
     writer=writer_mp4
)
plt.close()

```

<video width="99%" style="display: block; margin: 0 auto;" controls muted autoplay loop>
  <source src="images/lln.mp4 " type="video/webm">
</video> 
 

## Consistency of the OLS Estimator {background="#00100F"}
   

#### Returning to the OLS Estimator

Let's go back to the OLS estimator based on <span class="highlight"> IID</span>  sample $(\bX_1, Y_1), \dots, (\bX_N, Y_N)$

. . . 

<br>

<div class="rounded-box">

Is the OLS estimator consistent? 

</div>

<br>

Consistent for what?


### Convergence Without a Causal Model {background="#43464B" visibility="uncounted"}



#### Invertibility of $\bX'\bX$

First an unpleasant technical issue

<div class="rounded-box">

How to handle non-invertible $\bX'\bX$? 

</div>

<br>

Some fall-back known value $\bc$ 
$$
\hat{\bbeta} = \begin{cases}
(\bX'\bX)^{-1}\bX'\bY, & \bX'\bX \text{ invertible}\\
\bc, & \bX'\bX \text{ not invertible}
\end{cases}
$$ 
Now $\hat{\bbeta}$ is always defined, but does $\bc$ matter?

#### Representation in Terms of Averages

First lecture shows that 
$$
\bX'\bX = \sum_{i=1}^N \bX_i\bX_i', \quad \bX'\bY = \sum_{i=1}^N \bX_i Y_i
$$

Then we can write (under invertibility)
$$
(\bX'\bX)\bX'\bY = \left(\textcolor{teal}{\dfrac{1}{N}} \sum_{i=1}^N \bX_i\bX_i'\right)^{-1} \left( \textcolor{teal}{\dfrac{1}{N}} \sum_{i=1}^N \bX_i Y_i\right)
$$

#### Limits of Averages

Can handle averages. If 

- $\E[\norm{\bX_i\bX_i'}]<\infty$
- $\E[\norm{\bX_iY_i}]<\infty$

then by the WLLN  (@prp-vector-consistency-lln)
$$
\begin{aligned}
\dfrac{1}{N} \sum_{i=1}^N \bX_i\bX_i' \xrightarrow{p} \E[\bX_i\bX_i'], \quad 
\dfrac{1}{N} \sum_{i=1}^N \bX_iY_i \xrightarrow{p} \E[\bX_i Y_i]
\end{aligned}
$$

 

#### Handling the Inverse  of $\bX'\bX$

Two facts: 

- The inverse function $\bA\to \bA^{-1}$ is continuous on the space of invertible matrices
- The set of invertible matrices is open
  
. . . 

So if $\E[\bX_i\bX_i']$ is invertible, then by @prp-vector-consistency-characterizations-open  and the CMT (@prp-vector-consistency-cmt)

- $P(\frac{1}{N} \sum_{i=1}^N \bX_i\bX_i' \text{ is invertible})\to 1$
- $(\frac{1}{N}\sum_{i=1}^N \bX_i\bX_i')^{-1}  \xrightarrow{p} \left(\E[\bX_i\bX_i']\right)^{-1}$




  


::: footer
See [this StackExchange discussion](https://math.stackexchange.com/questions/84392/why-do-the-n-times-n-non-singular-matrices-form-an-open-set) for more details
:::

#### $\bc$ Does Not Matter

Since $\frac{1}{N} \sum_{i=1}^N \bX_i\bX_i'$ is invertible with probability approaching 1 (w.p.a. 1), then w.p.a.1 it holds
$$
\hat{\bbeta} =  (\bX'\bX)^{-1}\bX'\bY =  \left( \dfrac{1}{N} \sum_{i=1}^N \bX_i\bX_i'\right)^{-1} \left( \dfrac{1}{N} \sum_{i=1}^N \bX_i Y_i\right)
$$

. . . 

It follows that if $\bc\neq \E[\bX_i\bX_i']^{-1}\E[\bX_iY_i]$, then
$$
P(\hat{\bbeta}= \bc) \to 0
$$

#### Combining Together

<div class="rounded-box">

::: {#prp-vector-consistency-ols-model-free}

Let

1. $(\bX_i, Y_i)$ be IID
2. $\E[\norm{\bX_i\bX_i'}]<\infty$, $\E[\norm{\bX_iY_i}]<\infty$
3. $\E[\bX_i\bX_i']$ be invertible

Then
$$
\hat{\bbeta} \xrightarrow{p} \left( \E[\bX_i\bX_i'] \right)^{-1} \E[\bX_iY_i]
$$


:::

</div>

#### Quicker Way of Writing
 
It is common and acceptable (also on the exam) to directly write
$$
\hat{\bbeta}  = (\bX'\bX)^{-1}\bX'\bY
$$
<span class = "highlight"> provided that you </span>

1. Are talking about asymptotic properties (consistency, asymptotic distributions, asymptotic confidence intervals)
2. Make the assumption that $\E[\bX_i\bX_i']$ is invertible and say that $\bX'\bX$ is invertible w.p.a.1

#### Convergence of Estimator and Objective Function

```{python}
#| eval: false

# Parameters
BETA_1 = 0
NUM_OBS = 10000  # Large number of observations to approximate the population SSR
rng = np.random.default_rng(1)

# Generate data
X1 = rng.normal(size=NUM_OBS)
Y = BETA_1 * X1 + 0.5 * rng.normal(size=NUM_OBS)


# Define the SSR function
def ssr(b1, X1, Y):
    return np.mean((Y - b1 * X1) ** 2)


# Generate a grid of beta values for the SSR plot
ref_b_val = 1.5
b1_vals = np.linspace(-ref_b_val, ref_b_val, 1000)
SSR_vals_population = np.array([ssr(b1, X1, Y) for b1 in b1_vals])

# Generate a sequence of growing samples
sample_sizes = np.concatenate(
    [
        np.arange(5, 101),  # Every number between 1 and 100
        np.arange(100, 201, 4),  # Every 2nd number between 100 and 200
        np.arange(200, 501, 5),  # Every 3rd number between 200 and 500
        np.arange(500, 1001, 8),  # Every 5th number between 500 and 1000
    ]
).astype(int)

# Initialize the SSR values for the moving average
SSR_vals_sample_history = []

# Set up the figure and the axes
fig, ax = plt.subplots(figsize=(12, 6))
fig.patch.set_facecolor(BG_COLOR)
fig.patch.set_edgecolor("teal")
fig.patch.set_linewidth(5)

# Animation function: this is called sequentially
def animate(i):
    ax.clear()

    # Plot the population SSR function
    ax.plot(
        b1_vals,
        SSR_vals_population,
        color="teal",
        label="Population Objective Function",
    )

    n = sample_sizes[i]
    X1_sample = X1[:n]
    Y_sample = Y[:n]

    # Compute the sample SSR function
    SSR_vals_sample = np.array([ssr(b1, X1_sample, Y_sample) for b1 in b1_vals])

    # Apply moving average to smooth the SSR values
    if i == 0:
        SSR_vals_sample_smoothed = SSR_vals_sample
    else:
        SSR_vals_sample_history.append(SSR_vals_sample)
        SSR_vals_sample_smoothed = uniform_filter1d(
            np.array(SSR_vals_sample_history),
            size=min(5, len(SSR_vals_sample_history)),
            axis=0,
            mode="reflect",
        )[-1]

    # Plot the sample SSR function
    ax.plot(
        b1_vals,
        SSR_vals_sample_smoothed,
        color="darkorange",
        label=f"Sample Objective Function (Sample Size={n})",
    )

    # Find the minimizer of the sample SSR
    min_idx = np.argmin(SSR_vals_sample_smoothed)
    b1_min = b1_vals[min_idx]
    ssr_min = SSR_vals_sample_smoothed[min_idx]

    # Plot the broken line from the minimizer to the x-axis
    ax.plot([b1_min, b1_min], [0, ssr_min], "k--")
    ax.text(
        b1_min, 0, r"$\hat{\beta}$", ha="center", va="top", fontsize=12, color="black"
    )

    # Find the minimizer of the population SSR
    pop_min_idx = np.argmin(SSR_vals_population)
    pop_b1_min = b1_vals[pop_min_idx]
    pop_ssr_min = SSR_vals_population[pop_min_idx]

    # Plot the pale broken line from the minimizer of the population SSR to the x-axis
    ax.plot([pop_b1_min, pop_b1_min], [0, pop_ssr_min], "k--", alpha=0.3)

    ax.set_xlim(b1_vals[[0, -1]])
    ax.set_ylim(0, np.max(SSR_vals_population))
    ax.set_xlabel("$b_1$", loc="right")
    ax.set_ylabel("Sum of squared residuals")
    # ax.set_title('Convergence of Objective Function and the OLS Estimator')
    ax.legend(loc="upper center")

    # Remove x and y tick labels
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_facecolor(BG_COLOR)


# Call the animator
WriterMP4 = animation.writers['ffmpeg']
writer_mp4 = WriterMP4(fps=50, metadata=dict(artist='bww'), bitrate=1800) 
ani = animation.FuncAnimation(
    fig,
    animate,
    frames=len(sample_sizes),
    repeat=True,
)
ani.save(
     Path("images").resolve() / "ols-conv-1d.mp4",
     writer=writer_mp4
)
plt.close()

```

<video width="99%" style="display: block; margin: 0 auto;" controls muted autoplay loop>
  <source src="images/ols-conv-1d.mp4 " type="video/webm">
</video>


#### Discussion
 

<br>

 
- @prp-vector-consistency-ols-model-free: no causal framework
- OLS just measures covariances in general
- Limit $\left( \E[\bX_i\bX_i'] \right)^{-1} \E[\bX_iY_i]$ is called "population projection of $Y_i$ on $\bX_i$"

::: footer

See [Wikipedia](https://en.wikipedia.org/wiki/Inner_product_space#Random_variables) on inner products of random variables

:::

### Convergence under Causal Model with Exogeneity and Homogeneous Effects  {background="#43464B" visibility="uncounted"}


 

#### Potential Outcomes Framework


Let's go back to our causal framework to add a causal part to @prp-vector-consistency-ols-model-free

. . .

- Treatment $\bX_i$ with possible values $\bx$
- Potential outcome under $\bX_i=\bx$:
  $$ 
  Y_i^{\bx} = \bx'\bbeta + U_i
  $$ {#eq-vector-consistency-causal-homogeneous}

. . .

Observed outcomes $Y_i$ satisfy
$$
Y_i = \bX_i'\bbeta + U_i
$$

#### Sampling Error Representation

Can now <span class="highlight">substitute the equation for $Y_i$</span> (+invertibility assumptions) to get   
$$
\begin{aligned}
\hat{\bbeta} & = \left( \dfrac{1}{N}\sum_{i=1}^N \bX_i\bX_i' \right)^{-1} \dfrac{1}{N}\sum_{i=1}^N \bX_iY_i\\
& = \bbeta + \left( \dfrac{1}{N}\sum_{i=1}^N \bX_i\bX_i' \right)^{-1} \dfrac{1}{N}\sum_{i=1}^N \bX_i U_i
\end{aligned}
$$
Last line — <span class="highlight">sampling error form</span>
 

#### Consistency of the OLS Estimator


<div class="rounded-box">

::: {#prp-vector-consistency-ols-model-causal-homogeneous}

Let

1. $(\bX_i, U_i)$ be IID and model ([-@eq-vector-consistency-causal-homogeneous]) hold
2. $\E[\norm{\bX_i\bX_i'}]<\infty$, $\E[\norm{\bX_iU_i}]<\infty$, $\E[\bX_iU_i]=0$
3. $\E[\bX_i\bX_i']$ be invertible

Then
$$
\hat{\bbeta} \xrightarrow{p} \bbeta
$$


:::

</div>
 
#### Discussion of Assumptions
 

- @prp-vector-consistency-ols-model-free: assumptions on $(\bX_i, Y_i)$
- @prp-vector-consistency-ols-model-causal-homogeneous: assumptions on $(\bX_i, U_i)$

. . . 

<br> 

Need assumptions for causal interpretation:

- Assumptions on the assignment mechanism (IID $\bX_i$ and exogeneity/orthogonality)
- Same assumptions as for identification of $\bbeta$ (why can you use @prp-vector-consistency-ols-model-causal-homogeneous to prove identification?)

 



#### Orthogonality vs Strict Exogeneity

Recall: 

<div class="rounded-box">
If $\E[U_i|\bX_i]=0$, then $\E[\bX_iU_i]=0$
</div>

- $\E[U_i|\bX_i]=0$ is stronger than $\E[\bX_iU_i]=0$
- If $\E[U_i|\bX_i]=0$ holds, then $\E[\hat{\bbeta}] = \bbeta$ (unbiased)
- If $\E[U_i|\bX_i]\neq 0$ but $\E[\bX_i U_i]=0$, then $\hat{\bbeta}$ may be biased in finite samples, but still converges to the correct $\bbeta$
- What if both fail? 


### Convergence under Causal Model with Heterogeneous Effects  {background="#43464B" visibility="uncounted"}

#### Allowing Heterogeneous Causal Effects

Before concluding, let's go back to model ([-@eq-vector-consistency-causal-homogeneous])

- Causal effects are <span class = "highlight"> homogeneous </span> (same for everyone)
- Might not be realistic

. . . 

More general potential outcome equation
$$
Y_i^\bx = \bx_i'\bbeta_{\textcolor{teal}{i}} + U_i
$$ {#eq-vector-consistency-causal-heterogeneous}
Causal effect of shift from $\bx_1$ to $\bx_2$ for unit $i$:
$$
(\bx_2-\bx_1)'\bbeta_i
$$

::: footer

:::

#### Parameters of Interest In Model ([-@eq-vector-consistency-causal-heterogeneous])

1. Average $\E[\bbeta_i]$ — enough to compute all average treatment effects
2. Variance, other moments of $\bbeta_i$
3. Distribution of $\bbeta_i$
4. Other objects? 

#### OLS Under Model ([-@eq-vector-consistency-causal-heterogeneous])


We can still apply OLS under ([-@eq-vector-consistency-causal-heterogeneous]), but the representation in terms of $U_i$ and $\bbeta_i$ is different:
$$ \small
\begin{aligned}
\hat{\bbeta} & = \left( \dfrac{1}{N} \sum_{i=1}^N \bX_i\bX_i'  \right)\dfrac{1}{N} \sum_{i=1}^N \bX_i Y_i \\
& = \left( \dfrac{1}{N} \sum_{i=1}^N \bX_i\bX_i'  \right)\dfrac{1}{N} \sum_{i=1}^N \bX_i\bX_i'\bbeta_i \\
& \quad + \left( \dfrac{1}{N} \sum_{i=1}^N \bX_i\bX_i'  \right)\dfrac{1}{N} \sum_{i=1}^N \bX_i U_i
\end{aligned}
$$




#### Limit of the OLS Estimator Under Model ([-@eq-vector-consistency-causal-heterogeneous])


<div class="rounded-box">

::: {#prp-vector-consistency-ols-model-causal-heterogeneous}

Let

1. $X_i$ be scalar, $(X_i, \bbeta_i, U_i)$ be IID and model ([-@eq-vector-consistency-causal-heterogeneous]) hold as 
$$ \small
Y_i^x = \beta_i x + U_i
$$
1. $\E[X_i^2] \in (0,\infty)$, $\E[X_iU_i]=0$

Then
$$ \small
\hat{\bbeta} \xrightarrow{p} \E[W(X_i)\beta_i], \quad W(X_i) = X_i^2/\E[X_i^2]
$$


:::

</div>
 
::: footer


:::

#### @prp-vector-consistency-ols-model-causal-heterogeneous: Discussion


<br>

- @prp-vector-consistency-ols-model-causal-heterogeneous: OLS is estimating a *weighted* average of individual $\beta_i$
- Weights $W(X_i)$ are non-negative and $\E[W(X_i)]=1$ (population version of summing to 1)
- Still a "causal" parameter, but maybe hard to interpret
- Not equal to $\E[\bbeta_i]$ without further assumptions

#### Can You Identify $\E[\bbeta_i]$?

<br>

Only under restrictions:

- Independence of $\bbeta_i$ and $\bX_i$ (experimental settings). Show this! 
- With instruments

## Recap and Conclusions {background="#00100F"}
  
#### Recap

In this lecture we

1. Reviewed convergence in probability and consistency
2. Discussed consistency of the OLS estimator 
   - Without a causal framework (covariances)
   - In a causal model with homogeneous effects
   - In a causal model with heterogeneous effects


#### Next Questions

<br>

- How was fast does $\hat{\bbeta}$ converge?
- Distributional properties of $\hat{\bbeta}$?
- Inference on $\bbeta$


#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::