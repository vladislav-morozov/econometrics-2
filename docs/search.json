[
  {
    "objectID": "slides/vector/ols-limit-distribution.html#introduction",
    "href": "slides/vector/ols-limit-distribution.html#introduction",
    "title": "Limit Distribution of the OLS Estimator",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Linear Regression II",
      "Limit Distribution of the OLS Estimator"
    ]
  },
  {
    "objectID": "slides/vector/ols-limit-distribution.html#motivation",
    "href": "slides/vector/ols-limit-distribution.html#motivation",
    "title": "Limit Distribution of the OLS Estimator",
    "section": "Motivation",
    "text": "Motivation",
    "crumbs": [
      "Linear Regression II",
      "Limit Distribution of the OLS Estimator"
    ]
  },
  {
    "objectID": "slides/vector/ols-limit-distribution.html#probability-background",
    "href": "slides/vector/ols-limit-distribution.html#probability-background",
    "title": "Limit Distribution of the OLS Estimator",
    "section": "Probability Background",
    "text": "Probability Background",
    "crumbs": [
      "Linear Regression II",
      "Limit Distribution of the OLS Estimator"
    ]
  },
  {
    "objectID": "slides/vector/ols-limit-distribution.html#asymptotic-normality-of-the-ols-estimator",
    "href": "slides/vector/ols-limit-distribution.html#asymptotic-normality-of-the-ols-estimator",
    "title": "Limit Distribution of the OLS Estimator",
    "section": "Asymptotic Normality of the OLS Estimator",
    "text": "Asymptotic Normality of the OLS Estimator",
    "crumbs": [
      "Linear Regression II",
      "Limit Distribution of the OLS Estimator"
    ]
  },
  {
    "objectID": "slides/vector/ols-limit-distribution.html#recap-and-conclusions",
    "href": "slides/vector/ols-limit-distribution.html#recap-and-conclusions",
    "title": "Limit Distribution of the OLS Estimator",
    "section": "Recap and Conclusions",
    "text": "Recap and Conclusions",
    "crumbs": [
      "Linear Regression II",
      "Limit Distribution of the OLS Estimator"
    ]
  },
  {
    "objectID": "slides/vector/ols-delta-method.html#introduction",
    "href": "slides/vector/ols-delta-method.html#introduction",
    "title": "Inference II: Nonlinear Hypotheses",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Asymptotic Inference",
      "Inference II: Nonlinear Hypotheses"
    ]
  },
  {
    "objectID": "slides/vector/ols-delta-method.html#the-delta-method",
    "href": "slides/vector/ols-delta-method.html#the-delta-method",
    "title": "Inference II: Nonlinear Hypotheses",
    "section": "The Delta Method",
    "text": "The Delta Method",
    "crumbs": [
      "Asymptotic Inference",
      "Inference II: Nonlinear Hypotheses"
    ]
  },
  {
    "objectID": "slides/vector/ols-delta-method.html#inference-on-nonlinear-transformations",
    "href": "slides/vector/ols-delta-method.html#inference-on-nonlinear-transformations",
    "title": "Inference II: Nonlinear Hypotheses",
    "section": "Inference on Nonlinear Transformations",
    "text": "Inference on Nonlinear Transformations",
    "crumbs": [
      "Asymptotic Inference",
      "Inference II: Nonlinear Hypotheses"
    ]
  },
  {
    "objectID": "slides/vector/ols-delta-method.html#recap-and-conclusions",
    "href": "slides/vector/ols-delta-method.html#recap-and-conclusions",
    "title": "Inference II: Nonlinear Hypotheses",
    "section": "Recap and Conclusions",
    "text": "Recap and Conclusions",
    "crumbs": [
      "Asymptotic Inference",
      "Inference II: Nonlinear Hypotheses"
    ]
  },
  {
    "objectID": "slides/vector/identification-inference.html#introduction",
    "href": "slides/vector/identification-inference.html#introduction",
    "title": "Identification, Estimation and Inference",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Linear Regression II",
      "Identification, Estimation and Inference"
    ]
  },
  {
    "objectID": "slides/vector/identification-inference.html#identification",
    "href": "slides/vector/identification-inference.html#identification",
    "title": "Identification, Estimation and Inference",
    "section": "Identification",
    "text": "Identification",
    "crumbs": [
      "Linear Regression II",
      "Identification, Estimation and Inference"
    ]
  },
  {
    "objectID": "slides/vector/identification-inference.html#estimation-and-inference",
    "href": "slides/vector/identification-inference.html#estimation-and-inference",
    "title": "Identification, Estimation and Inference",
    "section": "Estimation and Inference",
    "text": "Estimation and Inference",
    "crumbs": [
      "Linear Regression II",
      "Identification, Estimation and Inference"
    ]
  },
  {
    "objectID": "slides/vector/identification-inference.html#recap-and-conclusions",
    "href": "slides/vector/identification-inference.html#recap-and-conclusions",
    "title": "Identification, Estimation and Inference",
    "section": "Recap and Conclusions",
    "text": "Recap and Conclusions",
    "crumbs": [
      "Linear Regression II",
      "Identification, Estimation and Inference"
    ]
  },
  {
    "objectID": "slides/panel/fe.html#introduction",
    "href": "slides/panel/fe.html#introduction",
    "title": "Fixed Effect Estimation",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Panel Data",
      "Fixed Effect Estimation"
    ]
  },
  {
    "objectID": "slides/panel/fe.html#fixed-effect-estimation",
    "href": "slides/panel/fe.html#fixed-effect-estimation",
    "title": "Fixed Effect Estimation",
    "section": "Fixed Effect Estimation",
    "text": "Fixed Effect Estimation",
    "crumbs": [
      "Panel Data",
      "Fixed Effect Estimation"
    ]
  },
  {
    "objectID": "slides/panel/fe.html#causal-properties-with-general-treatment",
    "href": "slides/panel/fe.html#causal-properties-with-general-treatment",
    "title": "Fixed Effect Estimation",
    "section": "Causal Properties with General Treatment",
    "text": "Causal Properties with General Treatment",
    "crumbs": [
      "Panel Data",
      "Fixed Effect Estimation"
    ]
  },
  {
    "objectID": "slides/panel/fe.html#empirical-application",
    "href": "slides/panel/fe.html#empirical-application",
    "title": "Fixed Effect Estimation",
    "section": "Empirical Application",
    "text": "Empirical Application",
    "crumbs": [
      "Panel Data",
      "Fixed Effect Estimation"
    ]
  },
  {
    "objectID": "slides/panel/fe.html#recap-and-conclusions",
    "href": "slides/panel/fe.html#recap-and-conclusions",
    "title": "Fixed Effect Estimation",
    "section": "Recap and Conclusions",
    "text": "Recap and Conclusions",
    "crumbs": [
      "Panel Data",
      "Fixed Effect Estimation"
    ]
  },
  {
    "objectID": "slides/panel/did.html#introduction",
    "href": "slides/panel/did.html#introduction",
    "title": "Difference-in-Differences",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Panel Data",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "slides/panel/did.html#identification",
    "href": "slides/panel/did.html#identification",
    "title": "Difference-in-Differences",
    "section": "Identification",
    "text": "Identification",
    "crumbs": [
      "Panel Data",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "slides/panel/did.html#estimation-and-regression-view",
    "href": "slides/panel/did.html#estimation-and-regression-view",
    "title": "Difference-in-Differences",
    "section": "Estimation and Regression View",
    "text": "Estimation and Regression View",
    "crumbs": [
      "Panel Data",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "slides/panel/did.html#some-extensions",
    "href": "slides/panel/did.html#some-extensions",
    "title": "Difference-in-Differences",
    "section": "Some Extensions",
    "text": "Some Extensions",
    "crumbs": [
      "Panel Data",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "slides/panel/did.html#empirical-application",
    "href": "slides/panel/did.html#empirical-application",
    "title": "Difference-in-Differences",
    "section": "Empirical Application",
    "text": "Empirical Application",
    "crumbs": [
      "Panel Data",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "slides/panel/did.html#recap-and-conclusions",
    "href": "slides/panel/did.html#recap-and-conclusions",
    "title": "Difference-in-Differences",
    "section": "Recap and Conclusions",
    "text": "Recap and Conclusions",
    "crumbs": [
      "Panel Data",
      "Difference-in-Differences"
    ]
  },
  {
    "objectID": "slides/organizational/intro.html#content-and-motivation",
    "href": "slides/organizational/intro.html#content-and-motivation",
    "title": "Course Introduction",
    "section": "Content and Motivation",
    "text": "Content and Motivation",
    "crumbs": [
      "Course Info",
      "Course Introduction"
    ]
  },
  {
    "objectID": "slides/organizational/intro.html#course-logistics",
    "href": "slides/organizational/intro.html#course-logistics",
    "title": "Course Introduction",
    "section": "Course Logistics",
    "text": "Course Logistics",
    "crumbs": [
      "Course Info",
      "Course Introduction"
    ]
  },
  {
    "objectID": "exercises/exercises-linear-inference.html",
    "href": "exercises/exercises-linear-inference.html",
    "title": "Exercises: Asymptotic Inference",
    "section": "",
    "text": "Let the outcome \\(Y_i\\), the covariates \\(\\bX_i\\), and an unobserved component \\(U_i\\) be linked through the linear potential outcomes model \\[\nY_i^{\\bx} = \\bx'\\bbeta + U_i.\n\\] Suppose that we observe an IID sample of data on \\(Y_i, \\bX_i\\), that \\(\\E[U_i|\\bX_i]=0\\), that \\(\\E[\\bX_i\\bX_i']\\) is invertible, and that \\(\\E[U_i^2\\bX_i\\bX_i']\\) has maximal rank.\n\nConsider the hypotheses \\(H_0: \\beta_k = c\\) and \\(H_1: \\beta_k\\neq c\\), where \\(\\beta_k\\) is the \\(k\\)th coordinate of the \\(\\bbeta\\) vector. Propose a consistent test for \\(H_0\\) vs \\(H_1\\) that has asymptotic size \\(\\alpha\\).\nNow let \\(\\ba\\neq 0\\) be some known constant vector of the same dimension as \\(\\bbeta\\). Consider the hypotheses \\(H_0: \\ba'\\bbeta = c\\) and \\(H_1: \\ba'\\bbeta\\neq c\\). Propose a consistent \\(t\\)-test for \\(H_0\\) vs \\(H_1\\) that has asymptotic size \\(\\alpha\\).\nWhy do we require that \\(\\ba\\neq 0\\) in the previous question?\n\nIn both cases remember to show that your test is consistent and has the desired asymptotic size.\n\n\nClick to see the solution\n\nFirst subquestion: to choose our test statistic, we observe two facts:\n\nWe are dealing with a scalar hypothesis,\nThe OLS estimator is consistent and asymptotically normal (why?).\n\nAccordingly, we can use the t-test. The t-statistic is given by \\[\nt = \\dfrac{\\hat{\\bbeta}_k - c}{ \\sqrt{ \\widehat{\\avar}(\\hat{\\bbeta})/N  } },\n\\] where \\(\\hat{\\bbeta}\\) is the OLS estimator, \\(\\widehat{\\avar}(\\hat{\\bbeta})\\) is some consistent estimator of \\(\\avar(\\bbeta)\\) (e.g. the HC0 estimator from the lectures)\nOur test is based on the following decision rule. Let \\(z_{1-\\alpha/2}\\) be the \\((1-\\alpha/2)\\)th quantile of the standard normal distribution. Then:\n\nIf \\(\\abs{t}&gt;z_{1-\\alpha/2}\\), we reject \\(H_0\\).\nIf \\(\\abs{t}\\leq z_{1-\\alpha/2}\\), we do not reject \\(H_0\\).\n\nWe now need to show that this test is consistent and has the desired asymptotic size.\n\nConsistency: We need to show that the probability of rejecting \\(H_0\\) converges to 1 when \\(H_0\\) is false. Let \\(\\beta_k\\) be the true value of the coefficient of interest, and write \\[\nt = \\dfrac{\\hat{\\bbeta}_k - \\beta_k}{ \\sqrt{ \\widehat{\\avar}(\\hat{\\bbeta})/N  } } + \\dfrac{ \\beta_k- c}{ \\sqrt{ \\widehat{\\avar}(\\hat{\\bbeta})/N  } }.\n\\] By our asymptotic normality results, the first term converges in distribution to a \\(N(0, 1)\\) random variable. By our assumptions, \\(\\widehat{\\avar}(\\hat{\\bbeta})\\xrightarrow{p} \\avar(\\hat{\\bbeta})\\neq 0\\). Under the alternative, \\(\\beta_k\\neq c\\), and so the second term diverges to \\(\\pm \\infty\\). It then follows that with probability approaching one \\(\\abs{t}&gt; z_{1-\\alpha/2}\\) for any \\(\\beta_k\\neq c\\). In other words, consistency holds.\nAsymptotic size: We need to show that the probability of rejecting \\(H_0\\) converges to \\(\\alpha\\) when \\(H_0\\) is true. Under \\(H_0\\) it holds that \\(\\beta_k=c\\), and thus our asymptotic results and Slutsky’s theorem imply that \\[\nt = \\dfrac{\\hat{\\bbeta}_k - \\beta_k}{ \\sqrt{ \\widehat{\\avar}(\\hat{\\bbeta})/N  } } \\xrightarrow{d} N(0, 1).\n\\] By definition of convergence of probability, definition of \\(z_{1-\\alpha/2}\\) and the fact that \\(z_{1-\\alpha/2} = -z_{\\alpha/2}\\), it holds that \\[\n\\begin{aligned}\n& P\\left(\\text{Reject} H_0|H_0 \\right) = P\\left(\\abs{t}&gt;z_{1-\\alpha/2} |H_0\\right) \\\\\n& = P\\left( \\abs{ \\dfrac{\\hat{\\beta}_k-c}{\\sqrt{ \\widehat{\\avar}(\\hat{\\beta}_k)/N }  }}&gt; z_{1-\\alpha/2}\\Bigg|H_0 \\right)\\\\\n& \\to \\Phi(z_{\\alpha/2}) + (1- \\Phi(z_{1-\\alpha/2})) = \\alpha\n\\end{aligned}.\n\\] The test has asymptotic size \\(\\alpha\\).\n\n\nSecond subquestion: the question explicitly asks for a \\(t\\)-test, and so we use the following \\(t\\)-statistic as the basis for our test: \\[\nt = \\dfrac{\\ba'\\hat{\\bbeta} - c}{ \\sqrt{ \\widehat{\\avar}(\\ba'\\hat{\\bbeta})/N  } },\n\\tag{1}\\] The key question is how to construct a suitable estimator \\(\\widehat{\\avar}(\\ba'\\bbeta)\\) for \\(\\avar(\\ba'\\bbeta)\\).\nBy the continuous mapping theorem it holds that \\[\n\\sqrt{N}(\\ba'\\hat{\\bbeta}- \\ba'\\bbeta) \\xrightarrow{d} N(0, \\ba'\\avar(\\bbeta)\\ba).\n\\] By the continuous mapping theorem again: \\[\n\\ba'\\widehat{\\avar}(\\hat{\\bbeta})\\ba \\xrightarrow{p}\\ba'{\\avar}(\\hat{\\bbeta})\\ba  = \\avar(\\ba'\\hat{\\bbeta})\n\\] Hence, we can use \\(\\ba'\\widehat{\\avar}(\\hat{\\bbeta})\\ba\\) as \\(\\widehat{\\avar}(\\ba'\\bbeta)\\) in Equation 1. With this choice, it follows by Slutsky’s theorem that \\[\n\\dfrac{\\ba'\\hat{\\bbeta} - \\ba'\\bbeta}{ \\sqrt{ \\widehat{\\avar}(\\ba'\\hat{\\bbeta})/N  } } \\xrightarrow{d} N(0, 1).\n\\tag{2}\\]\nOur decision rule is analogous to the above one:\n\nIf \\(\\abs{t}&gt;z_{1-\\alpha/2}\\), we reject \\(H_0\\).\nIf \\(\\abs{t}\\leq z_{1-\\alpha/2}\\), we do not reject \\(H_0\\).\n\nConsistency and asymptotic size can be shown entirely analogously to the above case by using Equation 2 (show them regardless to practice!).\n\nThird subquestion: if \\(\\ba=0\\), then the null hypothesis is trivially true and reduces to \\(H_0: 0=c\\). It is either trivially true or trivially false, depending on \\(c\\).\n\n\n\n\n\nLet the outcome \\(Y_i\\), the covariates \\(\\bX_i\\), and an unobserved component \\(U_i\\) be linked through the linear potential outcomes model \\[\nY_i^{\\bx} = \\bx'\\bbeta + U_i.\n\\] Suppose that we observe an IID sample of data on \\(Y_i, \\bX_i\\), that \\(\\E[U_i|\\bX_i]=0\\), that \\(\\E[\\bX_i\\bX_i']\\) is invertible, and that \\(\\E[U_i^2\\bX_i\\bX_i']\\) has maximal rank.\nLet \\(\\bbeta = (\\beta_1, \\beta_2, \\dots, \\beta_p)\\) with \\(p\\geq 4\\). Consider the following two hypotheses on \\(\\bbeta\\): \\[\nH_0: \\begin{cases}\n\\beta_1 = 0, \\\\\n\\beta_2 - \\beta_3 = 1, \\\\\n\\beta_2 = 4\\beta_4 + 5,\n\\end{cases} \\quad H_1: \\text{at least one equality in $H_0$ fails}\n\\] Propose a consistent test for \\(H_0\\) vs. \\(H_1\\) with asymptotic size \\(\\alpha\\). Show that the test possesses these properties.\n\n\nClick to see the solution\n\nFirst, we write the null hypothesis in matrix form. We can do this by stacking the three equations in \\(H_0\\) into a single vector equation: \\[\n\\begin{aligned}\nH_0: & \\bR\\bbeta = \\bq, \\\\\n\\bR & = \\begin{pmatrix}\n1 & 0 & 0 & 0 & \\cdots\\\\\n0 & 1 & -1 & 0 & \\cdots\\\\\n0 & 1 & 0 & -4 & \\cdots\n\\end{pmatrix}, \\quad \\bq = \\begin{pmatrix}\n0\\\\\n1\\\\\n5\n\\end{pmatrix},\n\\end{aligned}\n\\] where \\(\\bR\\) has zero columns starting from the fifth column.\nWe can construct a Wald test for \\(H_0\\) vs. \\(H_1\\). The Wald statistic is defined as \\[\nW = N\\left(\\bR\\hat{\\bbeta} - \\bq \\right)' \\left(\\bR\\widehat{\\avar}(\\hat{\\bbeta})\\bR' \\right)^{-1} \\left( \\bR\\hat{\\bbeta} - \\bq  \\right)\n\\]\nWe propose the following test. Let \\(c_{1-\\alpha}\\) be the \\((1-\\alpha)\\)th quantile of the \\(\\chi^2_{3}\\) distribution (3 is the number of constraints in \\(H_0\\)). Then\n\nIf \\(W&gt;c_{1-\\alpha}\\), we reject \\(H_0\\).\nIf \\(W\\leq c_{1-\\alpha}\\), we do not reject \\(H_0\\).\n\nWe now need to show that this test is consistent and has the desired asymptotic size.\n\nAsymptotic size: Under \\(H_0\\) it holds that \\(\\bR\\bbeta=\\bq\\), and so by our asymptotic results for the OLS estimator and the continuous mapping theorem under \\(H_0\\) \\[\n\\sqrt{N}(\\bR\\hat{\\bbeta}-\\bq) \\xrightarrow{d} N(0, \\bR\\avar(\\hat{\\bbeta})\\bR').\n\\] By Slutsky’s theorem and the definition of \\(\\chi^2_{\\cdot}\\) random variables, it holds under \\(H_0\\) that \\[\nW \\xrightarrow{d} \\chi^2_3.\n\\] By definition of \\(c_{1-\\alpha}\\) and the definition of convergence in distribution \\[\n\\begin{aligned}\n& P\\left(\\text{Reject} H_0|H_0 \\right) = P\\left(W&gt;c_{1-\\alpha} |H_0\\right) \\\\\n& \\to P(\\chi^2_3&gt; c_{1-\\alpha}) = \\alpha .\n\\end{aligned}\n\\] The test has asymptotic size \\(\\alpha\\).\nConsistency: Under \\(H_1\\) we have that \\(\\bR\\hat{\\bbeta}\\xrightarrow{p} \\bR\\bbeta\\neq \\bq\\). In words, the outside terms in \\(W\\) converge to something \\(\\neq 0\\). At the same time \\(\\left(\\bR\\widehat{\\avar}(\\hat{\\bbeta})\\bR' \\right)^{-1} \\xrightarrow{p} \\left(\\bR\\avar(\\hat{\\bbeta})\\bR'\\right)^{-1}\\) — a positive definite matrix. We conclude that \\[\n\\begin{aligned}\n& \\left(\\bR\\hat{\\bbeta} - \\bq \\right)' \\left(\\bR\\widehat{\\avar}(\\hat{\\bbeta})\\bR' \\right)^{-1} \\left( \\bR\\hat{\\bbeta} - \\bq  \\right) \\\\\n& \\xrightarrow{p} \\left(\\bR\\bbeta -\\bq\\right)' \\left(\\bR\\avar(\\hat{\\bbeta})\\bR'\\right)^{-1}(\\bR\\bbeta-\\bq) \\\\\n& &gt;0,\n\\end{aligned}\n\\] where we use the definition of positive definitiness. Finally, recall that there is also an \\(N\\) term in \\(W\\). We conclude that overall \\[\nW\\xrightarrow{p} \\infty\n\\] It follows that the probability of rejecting tends to 1 for any \\(\\bbeta\\) in \\(H_1\\). In other words, consistency holds.\n\n\n\n\n\nLet the outcome \\(Y_i\\), the covariates \\(\\bX_i\\), and an unobserved component \\(U_i\\) be linked through the linear potential outcomes model \\[\nY_i^{\\bx} = \\bx'\\bbeta + U_i.\n\\] Suppose that we observe an IID sample of data on \\(Y_i, \\bX_i\\), that \\(\\E[U_i|\\bX_i]=0\\), that \\(\\E[\\bX_i\\bX_i']\\) is invertible, and that \\(\\E[U_i^2\\bX_i\\bX_i']\\) has maximal rank. Also suppose that \\(\\bbeta\\) has \\(p\\geq 2\\) components, that \\(\\beta_1&gt;0\\) and \\(\\beta_2&gt;0\\), and that you are interested in \\[\n\\gamma = \\sqrt{\\beta_1\\beta_2}.\n\\]\n\nConstruct a confidence interval for \\(\\gamma\\) with asymptotic coverage \\((1-\\alpha)\\).\nConsider \\(H_0: \\gamma=1\\) vs. \\(H_1:\\gamma\\neq 1\\). Construct a consistent test for \\(H_0\\) vs. \\(H_1\\) with asymptotic size \\(\\alpha\\).\n\nRemember to prove coverage, consistency, and size properties.\n\n\nClick to see the solution\n\nFirst subquestion: we can use the delta method to construct a confidence interval for \\(\\gamma\\). The delta method states that if \\(\\hat{\\bbeta}\\) is consistent and asymptotically normal, then \\(g(\\hat{\\bbeta})\\) is also consistent and asymptotically normal for any continuously differentiable function \\(g(\\cdot)\\).\nFor our \\(\\gamma\\), we take \\(g(\\bw) = \\sqrt{w_1w_2}\\). This \\(g(\\cdot)\\) differentiable in a neighborhood of \\(\\bbeta\\). The Jacobian \\(\\bG(\\cdot)\\) of \\(g(\\cdot)\\) is a \\(1\\times p\\) matrix given by \\[\n\\bG(\\bw) = \\left( \\dfrac{w_2}{2\\sqrt{w_1w_2}}, \\dfrac{w_1}{2\\sqrt{w_1w_2}}, 0, \\dots, 0 \\right).\n\\] By assumptions of the problem, \\(G(\\bbeta)\\) has maximal rank.\nSince \\(g(\\bbeta) = \\gamma\\), the delta method tells us that \\[  \n\\begin{aligned}\n\\sqrt{N}(g(\\hat{\\bbeta}) -  \\gamma) & \\xrightarrow{d} N(0, \\bG(\\bbeta)\\avar(\\hat{\\bbeta})\\bG(\\bbeta)').\n\\end{aligned}\n\\tag{3}\\]\nWe can now construct a confidence interval in a standard way by using \\(g(\\hat{\\bbeta})\\) as the estimator and Equation 3 as the distributional base. Consider the following interval: \\[\n\\begin{aligned}\nS & = \\left[g(\\hat{\\bbeta}) - z_{1-\\alpha/2}\\sqrt{\\dfrac{\\widehat{\\avar}(g(\\hat{\\bbeta})) }{N}  }, g(\\hat{\\bbeta}) + z_{1-\\alpha/2}\\sqrt{\\dfrac{\\widehat{\\avar}(g(\\hat{\\bbeta})) }{N}  } \\right],\n\\end{aligned}\n\\] where \\(z_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)\\)th quantile of the standard normal distribution and \\[\n\\widehat{\\avar}(g(\\hat{\\bbeta})) = \\bG(\\hat{\\bbeta})\\widehat{\\avar}(\\hat{\\bbeta})G(\\hat{\\bbeta})',\n\\] for some consistent estimator \\(\\widehat{\\avar}(\\hat{\\bbeta})\\). It follows that \\(\\widehat{\\avar}(g(\\hat{\\bbeta})) \\xrightarrow{p} {\\avar}(g(\\hat{\\bbeta}))\\)\nTo compute asymptotic coverage, we first observe that by Slutsky’s theorem and Equation 3 it holds that \\[\n\\sqrt{N}\\dfrac{g(\\hat{\\bbeta})- \\gamma }{\\sqrt{\\widehat{\\avar}(g(\\hat{\\bbeta}))} } \\xrightarrow{d} N(0, 1)\n\\] Then by definition of convergence in distribution \\[\n\\begin{aligned}\n& P\\left(\\gamma \\in S  \\right) = P(g(\\bbeta)\\in S)\\\\\n&   = P\\left( -z_{1-\\alpha/2} \\leq  \\sqrt{N}\\dfrac{g(\\hat{\\bbeta})- g(\\bbeta) }{\\sqrt{\\widehat{\\avar}(g(\\hat{\\bbeta}))} } \\leq  z_{1-\\alpha/2}  \\right)   \\\\\n%\n&  \\to \\Phi(z_{1-\\alpha/2})  - \\Phi(-z_{1-\\alpha/2}) \\\\\n& = 1-\\alpha.\n\\end{aligned}\n\\] In other words, \\(S\\) has asymptotic coverage \\(1-\\alpha\\).\n\nSecond subquestion: we are dealing with a scalar transformation, and so we can use a \\(t\\)-test. Define the following statistic: \\[\nt = \\dfrac{ \\sqrt{ \\hat{\\beta}_1\\hat{\\beta}_2} - 1}{\\sqrt{\\widehat{\\avar}(g(\\hat{\\bbeta}))/N} }.\n\\] Here \\(\\sqrt{ \\hat{\\beta}_1\\hat{\\beta}_2 } =g(\\hat{\\bbeta})\\).\nOur test proceeds as follows:\n\nIf \\(\\abs{t}&gt;z_{1-\\alpha/2}\\), then we reject \\(H_0\\).\nIf \\(\\abs{t}\\leq z_{1-\\alpha/2}\\), then we do not reject \\(H_0\\).\n\nConsistency and asymptotic size are established exactly as before. As always, the key and only ingredient is the asymptotic normality result (3). Please write out the proofs in detail.\nNote: if the transformation were vector-valued, we would only be able to use the Wald test. The Wald test can also be used here, please solve this question using a Wald test like in the lectures to practice!\n\n\n\n\nLet the outcome \\(Y_i\\), the scalar covariate \\(X_i\\), and an unobserved component \\(U_i\\) be linked through the linear potential outcomes model \\[\nY_i^{\\bx} = \\beta X_i + U_i\n\\tag{4}\\] Suppose that we observe an IID sample of data on \\(Y_i, X_i\\), that \\(\\E[U_i|X_i]=0\\), that \\(\\E[X_i^2]\\neq 0\\), and that \\(\\E[U_i^2 X_i^2]\\) exists. Let \\(\\hat{\\beta}\\) be the OLS estimator obtained by regressing \\(Y_i\\) on \\(X_i\\).\nRecall the HC0 (White 1980) estimator for \\(\\avar(\\hat{\\beta})\\). In the scalar model (4) it is given by \\[\n\\begin{aligned}\n\\widehat{\\avar}(\\hat{\\beta}) & = \\dfrac{  N^{-1} \\sum_{i=1}^N \\hat{U_i}^2 X_i^2 }{  \\left( N^{-1}\\sum_{i=1}^N X_i^2 \\right)^2  }.\n\\end{aligned}\n\\] Show that \\[\n\\widehat{\\avar}(\\hat{\\beta}) \\xrightarrow{p} \\avar(\\hat{\\beta}) \\equiv \\dfrac{ \\E[U_i^2X_i^2]  }{\\left(\\E[X_i^2] \\right)^2 }.\n\\] State explicitly any additional moment assumptions you make.\n\n\nClick to see the solution\n\nFirst, we substitute our model (4) in place of \\(Y_i\\) in \\(\\hat{U}_i\\): \\[\n\\begin{aligned}\n\\hat{U}_i^2 & = (Y_i - X_i\\hat{\\beta})^2 =  (X_i(\\beta-\\hat{\\beta}) + U_i)^2\\\\\n& = U_i^2 + 2(\\beta-\\hat{\\beta})  U_iX_i+ (\\beta-\\hat{\\beta})^2 X_i\n\\end{aligned}\n\\]\nWe then substitute this expression for \\(\\hat{U}_i\\) into the middle term of our asymptotic variance estimator: \\[\n\\begin{aligned}\n& \\dfrac{1}{N} \\sum_{i=1}^N \\hat{U}_i^2 X_i^2 \\\\\n& = \\dfrac{1}{N} \\sum_{i=1}^N U_i^2X_i^2 + (\\beta-\\hat{\\beta}) \\dfrac{2}{N}\\sum_{i=1}^N U_i X_i^3 \\\\\n& \\quad  + (\\beta-\\hat{\\beta})^2 \\dfrac{1}{N}\\sum_{i=1}^N X_i^4.\n\\end{aligned}\n\\tag{5}\\]\nTo handle the averages in Equation 5, we use the law of large numbers. By assumption, \\(\\E[U_i^2X_i^2]\\) exists. We also assume that \\(\\E[U_iX_i^3]\\) and \\(\\E[X_i^4]\\) also exist. Then by the law of large numbers it holds that \\[\n\\begin{aligned}\n\\dfrac{1}{N} \\sum_{i=1}^N U_i^2X_i^2 & \\xrightarrow{p} \\E[U_i^2 X_i^2],\\\\\n\\dfrac{1}{N}\\sum_{i=1}^N U_i X_i^3 & \\xrightarrow{p} \\E[U_iX_i^3], \\\\  \\dfrac{1}{N}\\sum_{i=1}^N X_i^4  & \\xrightarrow{p} \\E[X_i^4].\n\\end{aligned}\n\\] At the same time, all the conditions of our consistency result hold, and so \\(\\hat{\\beta}-\\beta \\xrightarrow{p} 0\\). By the continuous mapping theorem \\[\n\\begin{aligned}\n(\\beta-\\hat{\\beta}) \\dfrac{2}{N}\\sum_{i=1}^N U_i X_i^3 \\xrightarrow{p} 0, \\\\\n(\\beta-\\hat{\\beta})^2 \\dfrac{1}{N}\\sum_{i=1}^N X_i^4 \\xrightarrow{p} 0.\n\\end{aligned}\n\\] By combining the above convergence results, the result on equivalence of separate and joint convergence in probability, and the continuous mapping theorem, we conclude that \\[\n\\dfrac{1}{N} \\sum_{i=1}^N \\hat{U}_i^2 X_i^2 \\xrightarrow{p}  \\E[U_i^2X_i^2].\n\\] At the same time, by the law of large numbers, continuous mapping theorem, and the assumption that \\(\\E[X_i^2]\\neq 0\\) it holds that \\[\n\\dfrac{1}{ \\left(N^{-1}\\sum_{i=1}^N X_i^2\\right)^2 } \\xrightarrow{p} \\dfrac{1}{\\left(\n    \\E[X_i^2]\n\\right)^2\n}.\n\\] Combining the previous two equations with the continuous mapping theorem, we obtain the desired result. In terms of assumptions, we have assumed the existence of \\(\\E[U_iX_i^3]\\) and \\(\\E[X_i^4]\\), in addition to the assumptions of the problem.",
    "crumbs": [
      "Asymptotic Inference",
      "Exercises: Asymptotic Inference"
    ]
  },
  {
    "objectID": "exercises/exercises-linear-inference.html#theoretical-exercises",
    "href": "exercises/exercises-linear-inference.html#theoretical-exercises",
    "title": "Exercises: Asymptotic Inference",
    "section": "",
    "text": "Let the outcome \\(Y_i\\), the covariates \\(\\bX_i\\), and an unobserved component \\(U_i\\) be linked through the linear potential outcomes model \\[\nY_i^{\\bx} = \\bx'\\bbeta + U_i.\n\\] Suppose that we observe an IID sample of data on \\(Y_i, \\bX_i\\), that \\(\\E[U_i|\\bX_i]=0\\), that \\(\\E[\\bX_i\\bX_i']\\) is invertible, and that \\(\\E[U_i^2\\bX_i\\bX_i']\\) has maximal rank.\n\nConsider the hypotheses \\(H_0: \\beta_k = c\\) and \\(H_1: \\beta_k\\neq c\\), where \\(\\beta_k\\) is the \\(k\\)th coordinate of the \\(\\bbeta\\) vector. Propose a consistent test for \\(H_0\\) vs \\(H_1\\) that has asymptotic size \\(\\alpha\\).\nNow let \\(\\ba\\neq 0\\) be some known constant vector of the same dimension as \\(\\bbeta\\). Consider the hypotheses \\(H_0: \\ba'\\bbeta = c\\) and \\(H_1: \\ba'\\bbeta\\neq c\\). Propose a consistent \\(t\\)-test for \\(H_0\\) vs \\(H_1\\) that has asymptotic size \\(\\alpha\\).\nWhy do we require that \\(\\ba\\neq 0\\) in the previous question?\n\nIn both cases remember to show that your test is consistent and has the desired asymptotic size.\n\n\nClick to see the solution\n\nFirst subquestion: to choose our test statistic, we observe two facts:\n\nWe are dealing with a scalar hypothesis,\nThe OLS estimator is consistent and asymptotically normal (why?).\n\nAccordingly, we can use the t-test. The t-statistic is given by \\[\nt = \\dfrac{\\hat{\\bbeta}_k - c}{ \\sqrt{ \\widehat{\\avar}(\\hat{\\bbeta})/N  } },\n\\] where \\(\\hat{\\bbeta}\\) is the OLS estimator, \\(\\widehat{\\avar}(\\hat{\\bbeta})\\) is some consistent estimator of \\(\\avar(\\bbeta)\\) (e.g. the HC0 estimator from the lectures)\nOur test is based on the following decision rule. Let \\(z_{1-\\alpha/2}\\) be the \\((1-\\alpha/2)\\)th quantile of the standard normal distribution. Then:\n\nIf \\(\\abs{t}&gt;z_{1-\\alpha/2}\\), we reject \\(H_0\\).\nIf \\(\\abs{t}\\leq z_{1-\\alpha/2}\\), we do not reject \\(H_0\\).\n\nWe now need to show that this test is consistent and has the desired asymptotic size.\n\nConsistency: We need to show that the probability of rejecting \\(H_0\\) converges to 1 when \\(H_0\\) is false. Let \\(\\beta_k\\) be the true value of the coefficient of interest, and write \\[\nt = \\dfrac{\\hat{\\bbeta}_k - \\beta_k}{ \\sqrt{ \\widehat{\\avar}(\\hat{\\bbeta})/N  } } + \\dfrac{ \\beta_k- c}{ \\sqrt{ \\widehat{\\avar}(\\hat{\\bbeta})/N  } }.\n\\] By our asymptotic normality results, the first term converges in distribution to a \\(N(0, 1)\\) random variable. By our assumptions, \\(\\widehat{\\avar}(\\hat{\\bbeta})\\xrightarrow{p} \\avar(\\hat{\\bbeta})\\neq 0\\). Under the alternative, \\(\\beta_k\\neq c\\), and so the second term diverges to \\(\\pm \\infty\\). It then follows that with probability approaching one \\(\\abs{t}&gt; z_{1-\\alpha/2}\\) for any \\(\\beta_k\\neq c\\). In other words, consistency holds.\nAsymptotic size: We need to show that the probability of rejecting \\(H_0\\) converges to \\(\\alpha\\) when \\(H_0\\) is true. Under \\(H_0\\) it holds that \\(\\beta_k=c\\), and thus our asymptotic results and Slutsky’s theorem imply that \\[\nt = \\dfrac{\\hat{\\bbeta}_k - \\beta_k}{ \\sqrt{ \\widehat{\\avar}(\\hat{\\bbeta})/N  } } \\xrightarrow{d} N(0, 1).\n\\] By definition of convergence of probability, definition of \\(z_{1-\\alpha/2}\\) and the fact that \\(z_{1-\\alpha/2} = -z_{\\alpha/2}\\), it holds that \\[\n\\begin{aligned}\n& P\\left(\\text{Reject} H_0|H_0 \\right) = P\\left(\\abs{t}&gt;z_{1-\\alpha/2} |H_0\\right) \\\\\n& = P\\left( \\abs{ \\dfrac{\\hat{\\beta}_k-c}{\\sqrt{ \\widehat{\\avar}(\\hat{\\beta}_k)/N }  }}&gt; z_{1-\\alpha/2}\\Bigg|H_0 \\right)\\\\\n& \\to \\Phi(z_{\\alpha/2}) + (1- \\Phi(z_{1-\\alpha/2})) = \\alpha\n\\end{aligned}.\n\\] The test has asymptotic size \\(\\alpha\\).\n\n\nSecond subquestion: the question explicitly asks for a \\(t\\)-test, and so we use the following \\(t\\)-statistic as the basis for our test: \\[\nt = \\dfrac{\\ba'\\hat{\\bbeta} - c}{ \\sqrt{ \\widehat{\\avar}(\\ba'\\hat{\\bbeta})/N  } },\n\\tag{1}\\] The key question is how to construct a suitable estimator \\(\\widehat{\\avar}(\\ba'\\bbeta)\\) for \\(\\avar(\\ba'\\bbeta)\\).\nBy the continuous mapping theorem it holds that \\[\n\\sqrt{N}(\\ba'\\hat{\\bbeta}- \\ba'\\bbeta) \\xrightarrow{d} N(0, \\ba'\\avar(\\bbeta)\\ba).\n\\] By the continuous mapping theorem again: \\[\n\\ba'\\widehat{\\avar}(\\hat{\\bbeta})\\ba \\xrightarrow{p}\\ba'{\\avar}(\\hat{\\bbeta})\\ba  = \\avar(\\ba'\\hat{\\bbeta})\n\\] Hence, we can use \\(\\ba'\\widehat{\\avar}(\\hat{\\bbeta})\\ba\\) as \\(\\widehat{\\avar}(\\ba'\\bbeta)\\) in Equation 1. With this choice, it follows by Slutsky’s theorem that \\[\n\\dfrac{\\ba'\\hat{\\bbeta} - \\ba'\\bbeta}{ \\sqrt{ \\widehat{\\avar}(\\ba'\\hat{\\bbeta})/N  } } \\xrightarrow{d} N(0, 1).\n\\tag{2}\\]\nOur decision rule is analogous to the above one:\n\nIf \\(\\abs{t}&gt;z_{1-\\alpha/2}\\), we reject \\(H_0\\).\nIf \\(\\abs{t}\\leq z_{1-\\alpha/2}\\), we do not reject \\(H_0\\).\n\nConsistency and asymptotic size can be shown entirely analogously to the above case by using Equation 2 (show them regardless to practice!).\n\nThird subquestion: if \\(\\ba=0\\), then the null hypothesis is trivially true and reduces to \\(H_0: 0=c\\). It is either trivially true or trivially false, depending on \\(c\\).\n\n\n\n\n\nLet the outcome \\(Y_i\\), the covariates \\(\\bX_i\\), and an unobserved component \\(U_i\\) be linked through the linear potential outcomes model \\[\nY_i^{\\bx} = \\bx'\\bbeta + U_i.\n\\] Suppose that we observe an IID sample of data on \\(Y_i, \\bX_i\\), that \\(\\E[U_i|\\bX_i]=0\\), that \\(\\E[\\bX_i\\bX_i']\\) is invertible, and that \\(\\E[U_i^2\\bX_i\\bX_i']\\) has maximal rank.\nLet \\(\\bbeta = (\\beta_1, \\beta_2, \\dots, \\beta_p)\\) with \\(p\\geq 4\\). Consider the following two hypotheses on \\(\\bbeta\\): \\[\nH_0: \\begin{cases}\n\\beta_1 = 0, \\\\\n\\beta_2 - \\beta_3 = 1, \\\\\n\\beta_2 = 4\\beta_4 + 5,\n\\end{cases} \\quad H_1: \\text{at least one equality in $H_0$ fails}\n\\] Propose a consistent test for \\(H_0\\) vs. \\(H_1\\) with asymptotic size \\(\\alpha\\). Show that the test possesses these properties.\n\n\nClick to see the solution\n\nFirst, we write the null hypothesis in matrix form. We can do this by stacking the three equations in \\(H_0\\) into a single vector equation: \\[\n\\begin{aligned}\nH_0: & \\bR\\bbeta = \\bq, \\\\\n\\bR & = \\begin{pmatrix}\n1 & 0 & 0 & 0 & \\cdots\\\\\n0 & 1 & -1 & 0 & \\cdots\\\\\n0 & 1 & 0 & -4 & \\cdots\n\\end{pmatrix}, \\quad \\bq = \\begin{pmatrix}\n0\\\\\n1\\\\\n5\n\\end{pmatrix},\n\\end{aligned}\n\\] where \\(\\bR\\) has zero columns starting from the fifth column.\nWe can construct a Wald test for \\(H_0\\) vs. \\(H_1\\). The Wald statistic is defined as \\[\nW = N\\left(\\bR\\hat{\\bbeta} - \\bq \\right)' \\left(\\bR\\widehat{\\avar}(\\hat{\\bbeta})\\bR' \\right)^{-1} \\left( \\bR\\hat{\\bbeta} - \\bq  \\right)\n\\]\nWe propose the following test. Let \\(c_{1-\\alpha}\\) be the \\((1-\\alpha)\\)th quantile of the \\(\\chi^2_{3}\\) distribution (3 is the number of constraints in \\(H_0\\)). Then\n\nIf \\(W&gt;c_{1-\\alpha}\\), we reject \\(H_0\\).\nIf \\(W\\leq c_{1-\\alpha}\\), we do not reject \\(H_0\\).\n\nWe now need to show that this test is consistent and has the desired asymptotic size.\n\nAsymptotic size: Under \\(H_0\\) it holds that \\(\\bR\\bbeta=\\bq\\), and so by our asymptotic results for the OLS estimator and the continuous mapping theorem under \\(H_0\\) \\[\n\\sqrt{N}(\\bR\\hat{\\bbeta}-\\bq) \\xrightarrow{d} N(0, \\bR\\avar(\\hat{\\bbeta})\\bR').\n\\] By Slutsky’s theorem and the definition of \\(\\chi^2_{\\cdot}\\) random variables, it holds under \\(H_0\\) that \\[\nW \\xrightarrow{d} \\chi^2_3.\n\\] By definition of \\(c_{1-\\alpha}\\) and the definition of convergence in distribution \\[\n\\begin{aligned}\n& P\\left(\\text{Reject} H_0|H_0 \\right) = P\\left(W&gt;c_{1-\\alpha} |H_0\\right) \\\\\n& \\to P(\\chi^2_3&gt; c_{1-\\alpha}) = \\alpha .\n\\end{aligned}\n\\] The test has asymptotic size \\(\\alpha\\).\nConsistency: Under \\(H_1\\) we have that \\(\\bR\\hat{\\bbeta}\\xrightarrow{p} \\bR\\bbeta\\neq \\bq\\). In words, the outside terms in \\(W\\) converge to something \\(\\neq 0\\). At the same time \\(\\left(\\bR\\widehat{\\avar}(\\hat{\\bbeta})\\bR' \\right)^{-1} \\xrightarrow{p} \\left(\\bR\\avar(\\hat{\\bbeta})\\bR'\\right)^{-1}\\) — a positive definite matrix. We conclude that \\[\n\\begin{aligned}\n& \\left(\\bR\\hat{\\bbeta} - \\bq \\right)' \\left(\\bR\\widehat{\\avar}(\\hat{\\bbeta})\\bR' \\right)^{-1} \\left( \\bR\\hat{\\bbeta} - \\bq  \\right) \\\\\n& \\xrightarrow{p} \\left(\\bR\\bbeta -\\bq\\right)' \\left(\\bR\\avar(\\hat{\\bbeta})\\bR'\\right)^{-1}(\\bR\\bbeta-\\bq) \\\\\n& &gt;0,\n\\end{aligned}\n\\] where we use the definition of positive definitiness. Finally, recall that there is also an \\(N\\) term in \\(W\\). We conclude that overall \\[\nW\\xrightarrow{p} \\infty\n\\] It follows that the probability of rejecting tends to 1 for any \\(\\bbeta\\) in \\(H_1\\). In other words, consistency holds.\n\n\n\n\n\nLet the outcome \\(Y_i\\), the covariates \\(\\bX_i\\), and an unobserved component \\(U_i\\) be linked through the linear potential outcomes model \\[\nY_i^{\\bx} = \\bx'\\bbeta + U_i.\n\\] Suppose that we observe an IID sample of data on \\(Y_i, \\bX_i\\), that \\(\\E[U_i|\\bX_i]=0\\), that \\(\\E[\\bX_i\\bX_i']\\) is invertible, and that \\(\\E[U_i^2\\bX_i\\bX_i']\\) has maximal rank. Also suppose that \\(\\bbeta\\) has \\(p\\geq 2\\) components, that \\(\\beta_1&gt;0\\) and \\(\\beta_2&gt;0\\), and that you are interested in \\[\n\\gamma = \\sqrt{\\beta_1\\beta_2}.\n\\]\n\nConstruct a confidence interval for \\(\\gamma\\) with asymptotic coverage \\((1-\\alpha)\\).\nConsider \\(H_0: \\gamma=1\\) vs. \\(H_1:\\gamma\\neq 1\\). Construct a consistent test for \\(H_0\\) vs. \\(H_1\\) with asymptotic size \\(\\alpha\\).\n\nRemember to prove coverage, consistency, and size properties.\n\n\nClick to see the solution\n\nFirst subquestion: we can use the delta method to construct a confidence interval for \\(\\gamma\\). The delta method states that if \\(\\hat{\\bbeta}\\) is consistent and asymptotically normal, then \\(g(\\hat{\\bbeta})\\) is also consistent and asymptotically normal for any continuously differentiable function \\(g(\\cdot)\\).\nFor our \\(\\gamma\\), we take \\(g(\\bw) = \\sqrt{w_1w_2}\\). This \\(g(\\cdot)\\) differentiable in a neighborhood of \\(\\bbeta\\). The Jacobian \\(\\bG(\\cdot)\\) of \\(g(\\cdot)\\) is a \\(1\\times p\\) matrix given by \\[\n\\bG(\\bw) = \\left( \\dfrac{w_2}{2\\sqrt{w_1w_2}}, \\dfrac{w_1}{2\\sqrt{w_1w_2}}, 0, \\dots, 0 \\right).\n\\] By assumptions of the problem, \\(G(\\bbeta)\\) has maximal rank.\nSince \\(g(\\bbeta) = \\gamma\\), the delta method tells us that \\[  \n\\begin{aligned}\n\\sqrt{N}(g(\\hat{\\bbeta}) -  \\gamma) & \\xrightarrow{d} N(0, \\bG(\\bbeta)\\avar(\\hat{\\bbeta})\\bG(\\bbeta)').\n\\end{aligned}\n\\tag{3}\\]\nWe can now construct a confidence interval in a standard way by using \\(g(\\hat{\\bbeta})\\) as the estimator and Equation 3 as the distributional base. Consider the following interval: \\[\n\\begin{aligned}\nS & = \\left[g(\\hat{\\bbeta}) - z_{1-\\alpha/2}\\sqrt{\\dfrac{\\widehat{\\avar}(g(\\hat{\\bbeta})) }{N}  }, g(\\hat{\\bbeta}) + z_{1-\\alpha/2}\\sqrt{\\dfrac{\\widehat{\\avar}(g(\\hat{\\bbeta})) }{N}  } \\right],\n\\end{aligned}\n\\] where \\(z_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)\\)th quantile of the standard normal distribution and \\[\n\\widehat{\\avar}(g(\\hat{\\bbeta})) = \\bG(\\hat{\\bbeta})\\widehat{\\avar}(\\hat{\\bbeta})G(\\hat{\\bbeta})',\n\\] for some consistent estimator \\(\\widehat{\\avar}(\\hat{\\bbeta})\\). It follows that \\(\\widehat{\\avar}(g(\\hat{\\bbeta})) \\xrightarrow{p} {\\avar}(g(\\hat{\\bbeta}))\\)\nTo compute asymptotic coverage, we first observe that by Slutsky’s theorem and Equation 3 it holds that \\[\n\\sqrt{N}\\dfrac{g(\\hat{\\bbeta})- \\gamma }{\\sqrt{\\widehat{\\avar}(g(\\hat{\\bbeta}))} } \\xrightarrow{d} N(0, 1)\n\\] Then by definition of convergence in distribution \\[\n\\begin{aligned}\n& P\\left(\\gamma \\in S  \\right) = P(g(\\bbeta)\\in S)\\\\\n&   = P\\left( -z_{1-\\alpha/2} \\leq  \\sqrt{N}\\dfrac{g(\\hat{\\bbeta})- g(\\bbeta) }{\\sqrt{\\widehat{\\avar}(g(\\hat{\\bbeta}))} } \\leq  z_{1-\\alpha/2}  \\right)   \\\\\n%\n&  \\to \\Phi(z_{1-\\alpha/2})  - \\Phi(-z_{1-\\alpha/2}) \\\\\n& = 1-\\alpha.\n\\end{aligned}\n\\] In other words, \\(S\\) has asymptotic coverage \\(1-\\alpha\\).\n\nSecond subquestion: we are dealing with a scalar transformation, and so we can use a \\(t\\)-test. Define the following statistic: \\[\nt = \\dfrac{ \\sqrt{ \\hat{\\beta}_1\\hat{\\beta}_2} - 1}{\\sqrt{\\widehat{\\avar}(g(\\hat{\\bbeta}))/N} }.\n\\] Here \\(\\sqrt{ \\hat{\\beta}_1\\hat{\\beta}_2 } =g(\\hat{\\bbeta})\\).\nOur test proceeds as follows:\n\nIf \\(\\abs{t}&gt;z_{1-\\alpha/2}\\), then we reject \\(H_0\\).\nIf \\(\\abs{t}\\leq z_{1-\\alpha/2}\\), then we do not reject \\(H_0\\).\n\nConsistency and asymptotic size are established exactly as before. As always, the key and only ingredient is the asymptotic normality result (3). Please write out the proofs in detail.\nNote: if the transformation were vector-valued, we would only be able to use the Wald test. The Wald test can also be used here, please solve this question using a Wald test like in the lectures to practice!\n\n\n\n\nLet the outcome \\(Y_i\\), the scalar covariate \\(X_i\\), and an unobserved component \\(U_i\\) be linked through the linear potential outcomes model \\[\nY_i^{\\bx} = \\beta X_i + U_i\n\\tag{4}\\] Suppose that we observe an IID sample of data on \\(Y_i, X_i\\), that \\(\\E[U_i|X_i]=0\\), that \\(\\E[X_i^2]\\neq 0\\), and that \\(\\E[U_i^2 X_i^2]\\) exists. Let \\(\\hat{\\beta}\\) be the OLS estimator obtained by regressing \\(Y_i\\) on \\(X_i\\).\nRecall the HC0 (White 1980) estimator for \\(\\avar(\\hat{\\beta})\\). In the scalar model (4) it is given by \\[\n\\begin{aligned}\n\\widehat{\\avar}(\\hat{\\beta}) & = \\dfrac{  N^{-1} \\sum_{i=1}^N \\hat{U_i}^2 X_i^2 }{  \\left( N^{-1}\\sum_{i=1}^N X_i^2 \\right)^2  }.\n\\end{aligned}\n\\] Show that \\[\n\\widehat{\\avar}(\\hat{\\beta}) \\xrightarrow{p} \\avar(\\hat{\\beta}) \\equiv \\dfrac{ \\E[U_i^2X_i^2]  }{\\left(\\E[X_i^2] \\right)^2 }.\n\\] State explicitly any additional moment assumptions you make.\n\n\nClick to see the solution\n\nFirst, we substitute our model (4) in place of \\(Y_i\\) in \\(\\hat{U}_i\\): \\[\n\\begin{aligned}\n\\hat{U}_i^2 & = (Y_i - X_i\\hat{\\beta})^2 =  (X_i(\\beta-\\hat{\\beta}) + U_i)^2\\\\\n& = U_i^2 + 2(\\beta-\\hat{\\beta})  U_iX_i+ (\\beta-\\hat{\\beta})^2 X_i\n\\end{aligned}\n\\]\nWe then substitute this expression for \\(\\hat{U}_i\\) into the middle term of our asymptotic variance estimator: \\[\n\\begin{aligned}\n& \\dfrac{1}{N} \\sum_{i=1}^N \\hat{U}_i^2 X_i^2 \\\\\n& = \\dfrac{1}{N} \\sum_{i=1}^N U_i^2X_i^2 + (\\beta-\\hat{\\beta}) \\dfrac{2}{N}\\sum_{i=1}^N U_i X_i^3 \\\\\n& \\quad  + (\\beta-\\hat{\\beta})^2 \\dfrac{1}{N}\\sum_{i=1}^N X_i^4.\n\\end{aligned}\n\\tag{5}\\]\nTo handle the averages in Equation 5, we use the law of large numbers. By assumption, \\(\\E[U_i^2X_i^2]\\) exists. We also assume that \\(\\E[U_iX_i^3]\\) and \\(\\E[X_i^4]\\) also exist. Then by the law of large numbers it holds that \\[\n\\begin{aligned}\n\\dfrac{1}{N} \\sum_{i=1}^N U_i^2X_i^2 & \\xrightarrow{p} \\E[U_i^2 X_i^2],\\\\\n\\dfrac{1}{N}\\sum_{i=1}^N U_i X_i^3 & \\xrightarrow{p} \\E[U_iX_i^3], \\\\  \\dfrac{1}{N}\\sum_{i=1}^N X_i^4  & \\xrightarrow{p} \\E[X_i^4].\n\\end{aligned}\n\\] At the same time, all the conditions of our consistency result hold, and so \\(\\hat{\\beta}-\\beta \\xrightarrow{p} 0\\). By the continuous mapping theorem \\[\n\\begin{aligned}\n(\\beta-\\hat{\\beta}) \\dfrac{2}{N}\\sum_{i=1}^N U_i X_i^3 \\xrightarrow{p} 0, \\\\\n(\\beta-\\hat{\\beta})^2 \\dfrac{1}{N}\\sum_{i=1}^N X_i^4 \\xrightarrow{p} 0.\n\\end{aligned}\n\\] By combining the above convergence results, the result on equivalence of separate and joint convergence in probability, and the continuous mapping theorem, we conclude that \\[\n\\dfrac{1}{N} \\sum_{i=1}^N \\hat{U}_i^2 X_i^2 \\xrightarrow{p}  \\E[U_i^2X_i^2].\n\\] At the same time, by the law of large numbers, continuous mapping theorem, and the assumption that \\(\\E[X_i^2]\\neq 0\\) it holds that \\[\n\\dfrac{1}{ \\left(N^{-1}\\sum_{i=1}^N X_i^2\\right)^2 } \\xrightarrow{p} \\dfrac{1}{\\left(\n    \\E[X_i^2]\n\\right)^2\n}.\n\\] Combining the previous two equations with the continuous mapping theorem, we obtain the desired result. In terms of assumptions, we have assumed the existence of \\(\\E[U_iX_i^3]\\) and \\(\\E[X_i^4]\\), in addition to the assumptions of the problem.",
    "crumbs": [
      "Asymptotic Inference",
      "Exercises: Asymptotic Inference"
    ]
  },
  {
    "objectID": "exercises/exercises-linear-inference.html#applied-exercises",
    "href": "exercises/exercises-linear-inference.html#applied-exercises",
    "title": "Exercises: Asymptotic Inference",
    "section": "Applied Exercises",
    "text": "Applied Exercises\nApplied exercises are from Wooldridge (2020). In all cases, use asymptotic \\(t\\)- and Wald tests with robust standard errors:\n\nC9 in chapter 4,\nC4 and C6 in chapter 5,\nC8 in chapter 7.\n\nFor some more code examples and discussion, look at chapters 4, 5, 7 in Heiss and Brunner (2024)",
    "crumbs": [
      "Asymptotic Inference",
      "Exercises: Asymptotic Inference"
    ]
  },
  {
    "objectID": "exercises/exercises-linear-asymptotic.html",
    "href": "exercises/exercises-linear-asymptotic.html",
    "title": "Exercises: Vector Linear Model and Asymptotics",
    "section": "",
    "text": "Suppose that we observe data \\((Y_1, \\bX_1), \\dots (Y_N, \\bX_N)\\). Let \\(\\bY\\) be the \\(N\\times 1\\) vector of outcomes, \\(\\bX\\) be the data matrix, and suppose that \\(\\bX'\\bX\\) is invertible.\nLet the vector \\(\\hat{\\bY}\\) of fitted values and the vector \\(\\hat{\\be}\\) of errors be given by \\[\n\\begin{aligned}\n\\hat{\\bY} & = \\bX\\hat{\\bbeta},\\\\\n\\hat{\\be} & = \\bY- \\hat{\\bY},\n\\end{aligned}\n\\] where \\(\\hat{\\bbeta}\\) is the OLS estimator.\nFind the OLS coefficient vector from\n\nRegressing \\(\\hat{Y}_i\\) on \\(\\bX_i\\).\nRegressing \\(\\hat{e}_i\\) on \\(\\bX_i\\).\n\nIn both cases, express the OLS estimator in terms of \\(\\bY\\) and \\(\\bX\\). Then express it in terms of \\((Y_i, \\bX_i)\\), \\(i=1, \\dots, N\\).\n\n\nClick to see the solution\n\nFirst subquestion: First consider the question of regressing \\(\\hat{\\bY}\\) on \\(\\bX\\). Let \\(\\tilde{\\bbeta}\\) be the OLS estimator of regressing \\(\\hat{\\bY}\\) on \\(\\bX\\). We can use the usual formula for OLS, but replace \\(\\bY\\) with \\(\\hat{\\bY}\\). We can then use the definition of \\(\\hat{\\bY}\\) and \\(\\hat{\\bbeta}\\) to express the coefficients in terms of the original data: \\[\n\\begin{aligned}\n\\tilde{\\bbeta} & = (\\bX'\\bX)^{-1}\\bX'\\hat{\\bY} = (\\bX'\\bX)^{-1}\\bX'\\bX\\hat{\\bbeta} \\\\\n& = \\hat{\\bbeta} = (\\bX'\\bX)^{-1}\\bX\\bY = \\left(\\sum_{i=1}^N \\bX_i\\bX_i' \\right)^{-1}\\sum_{i=1}^N\\bX_iY_i,\n\\end{aligned}\n\\] One way to interpret the above result is that applying OLS more than once does not change anything. The first application already extracts all the information that can be linearly explained by \\(\\bX\\) (a property sometimes called idempotency).\n\nSecond subquestion: We can proceed similarly with \\(\\hat{\\be}\\). Let \\(\\check{\\bbeta}\\) be the OLS estimator for regressing \\(\\hat{\\bbeta}\\) on \\(\\bX\\). We can again use the general expression for the OLS estimator and substitute the definition of \\(\\hat{\\bbeta}\\): \\[\n\\begin{aligned}\n\\check{\\bbeta} & = (\\bX'\\bX)^{-1}\\bX'\\hat{\\be} = (\\bX'\\bX)^{-1}\\bX'(\\bY-\\hat{\\bY})\\\\\n& = \\hat{\\bbeta} -\\tilde{\\bbeta} = \\hat{\\bbeta}- \\hat{\\bbeta}\\\\\n& = 0,\n\\end{aligned}\n\\] where we have used the first result of the problem.\n\n\n\n\nLet \\(X_i\\) and \\(Y_i\\) be scalar variables. Let \\(X_i\\) satisfy \\[\nX_i = \\begin{cases}\n1, & i = 1, \\\\\n0, & i &gt; 1.\n\\end{cases}\n\\tag{1}\\]\n\nSuppose we have a sample of \\(N\\) units: \\((Y_1, X_1), \\dots, (Y_N, X_N)\\). Can we compute the OLS estimator for regressing \\(Y_i\\) on \\(X_i\\) (without a constant)? If yes, express the estimator in terms of \\((Y_i, X_i)\\), \\(i=1,\\dots, N\\). If not, explain why.\nSuppose that \\(X_i\\) and \\(Y_i\\) are linked through the linear causal model \\[\nY_i^x = \\beta x + U_i,\n\\tag{2}\\] where \\(Y_i^x\\) is a potential outcome, \\(U_i\\) is independent of \\(X_i\\) with \\(\\E[U_i]=0\\). Why does the OLS estimator of \\(\\beta\\) not converge to \\(\\beta\\) without stronger assumptions on \\(U_i\\)? Informally, which of the conditions of our consistency results fail?\nProvide an informal empirical interpretation of the above data-generating process for \\(X_i\\).\n\n\n\nClick to see the solution\n\nFirst subquestion: The computability of the OLS estimator is determined by one key question: is \\(\\bX'\\bX\\) invertible? If \\(\\bX'\\bX\\) is invertible, then the answer is positive.\nIn this case, there is only one scalar covariate. \\(\\bX'\\bX\\) is itself then scalar (\\(1\\times 1\\)) and given by \\[\n\\bX'\\bX = \\sum_{i=1}^N X_i^2\n\\] By Equation 1, we have that \\(\\sum_{i=1}^N X_i^2=1\\), which is invertible. It follows that we can compute \\(\\hat{\\beta}=(\\bX'\\bX)^{-1}\\bX'\\bY\\): \\[\n\\hat{\\beta} = (\\bX'\\bX)^{-1}\\bX'\\bY = \\dfrac{\\sum_{i=1}^N X_iY_i}{\\sum_{i=1}^N X_i^2} = Y_1.\n\\tag{3}\\]\n\nSecond subquestion: To answer this question formally, we use the same technique we use in the lectures — substituting the underlying model into the estimator. By Equation 2, the realized outcomes satisfy \\[\nY_i = \\beta X_i + U_i\n\\]\nWe substitute this expression for realized values into the OLS estimator (3) to obtain \\[\n\\hat{\\beta} = \\beta + U_1,\n\\] where we have used that \\(X_1 =1\\) by Equation 1.\nWe now see that the value of \\(\\hat{\\beta}\\) does not depend on sample size \\(N\\). The full value of \\(U_1\\) will always be present in \\(\\hat{\\beta}\\): as \\(N\\to\\infty\\) \\[\n\\hat{\\beta} \\xrightarrow{p} \\beta + U_1\n\\] The only case where \\(\\hat{\\beta}\\xrightarrow{p}\\beta\\) is when \\(U_1=0\\) — an additional stronger condition.\nWhich conditions of our consistency results fail? There are two conditions that hold:\n\nOrthogonality: \\(\\E[X_iU_i] =0\\) holds.\nIndependence of units\n\nThere are two conditions that do not hold:\n\nIdentical distributions: unit 1 is different to the rest.\nInvertibility of the limit of \\(N^{-1}\\sum_{i=1}^N\\bX_i\\bX_i'\\). By Equation 1, this sum is equal to \\(N^{-1}\\to 0\\).\n\nOf these two failing conditions, the first one is usually not a big issue. It concerns only a single point, and in general we have tools for handling non-identical distributions. It is the second condition that creates a problem in the limit.\nThe message of this problem is that we need two invertibility conditions: the sample condition (on \\(\\bX'\\bX\\)) and the population one (on \\(\\E[\\bX_i\\bX_i']\\)). These conditions play different roles. Each can fail, while the other condition is true.\n\nThird subquestion: we can imagine a simple experiment in which the subjects have arrived to the lab in a random order, independently of their characteristics. However, there is only one real treatment, which is given to the first unit. Everyone else receives a placebo.\n\n\n\n\nLet \\(X_i\\) and \\(U_i\\) be scalar random variables. Suppose that \\(X_i\\) satisfies \\(X_i\\geq c&gt;0\\) for some \\(c\\) (strictly positive and bounded away from 0). Let \\(Y_i\\) be some outcome. Suppose that the following linear causal model holds: the potential outcome \\(Y_i^x\\) is determined as \\[\nY_i^x = \\beta x + U_i.\n\\tag{4}\\] The realized outcome \\(Y_i\\) is determined as \\(Y_i = Y_i^{X_i}\\).\nConsider the following estimator for \\(\\beta\\): \\[\n\\tilde{\\beta} = \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{Y_i}{X_i}.\n\\]\n\nPropose conditions under which \\(\\tilde{\\beta}\\) is consistent for \\(\\beta\\) and prove consistency.\nDerive the asymptotic distribution of \\(\\tilde{\\beta}\\). Describe any additional assumptions you make.\nNow suppose that the causal model allows heterogeneous effects: \\[\nY_i^x = \\beta_i x + U_i.\n\\tag{5}\\] Under which conditions does \\(\\tilde{\\beta}\\) consistently estimate \\(\\E[\\beta_i]\\)?\n\n\n\nClick to see the solution\n\nFirst subquestion: we again use the key technique — substituting the true model into the estimator. By Equation 4, the outcome \\(Y_i\\) satisfies \\[\nY_i = \\beta X_i + U_i.\n\\] We can substitute this expression into \\(\\tilde{\\beta}\\) to obtain \\[\n\\tilde{\\beta} = \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{Y_i}{X_i} = \\beta+ \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{U_i}{X_i}\n\\] The law of large number applies to the sum on the right hand side if\n\n\\((X_i, U_i)\\) are IID.\n\\(\\E[U_i/X_i]\\) exists.\n\nWe make these assumptions. Then by the law of large numbers: \\[\n\\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{U_i}{X_i}\\xrightarrow{p} \\E\\left[ \\dfrac{U_i}{X_i} \\right].\n\\] By the continuous mapping theorem: \\[\n\\tilde{\\beta} \\xrightarrow{p} \\beta + \\E\\left[ \\dfrac{U_i}{X_i} \\right].\n\\] \\(\\tilde{\\beta}\\) is consistent for \\(\\beta\\) if\n\n\\(\\E[U_i/X_i]=0\\) (note that it is sufficient that \\(\\E[U_i|X_i]=0\\) for this condition, why?).\n\nWe conclude that \\(\\tilde{\\beta}\\) is consistent for \\(\\beta\\) under assumptions (1)-(3).\n\nSecond subquestion: to study the asymptotic distribution, we keep the above conditions (1)-(3). Under these conditions (especially (3)) we note that \\[\n\\begin{aligned}\n\\tilde{\\beta} - \\beta & = \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{U_i}{X_i} \\\\\n& = \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{U_i}{X_i} - \\E\\left[\\dfrac{U_i}{X_i}\\right].\n\\end{aligned}\n\\tag{6}\\] The bottom line is in the form used in the central limit theorem: sample average minus population average. We can then apply the central limit theorem provided the following assumption holds:\n\nFinite second moments: \\(\\E[U_i^2/X_i^2]&lt;\\infty\\).\n\nThen by the central limit theorem it holds that \\[\n\\sqrt{N}\\left(  \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{U_i}{X_i} - \\E\\left[\\dfrac{U_i}{X_i}\\right] \\right)\\xrightarrow{d} N\\left(0, \\E\\left[\\dfrac{U_i^2}{X_i^2} \\right] \\right).\n\\]\nBy Equation 6 we conclude that, if assumptions (1)-(4) hold, then \\[\n\\sqrt{N}\\left(\\tilde{\\beta}- \\beta \\right)\\xrightarrow{d} N\\left(0, \\E\\left[\\dfrac{U_i^2}{X_i^2} \\right] \\right).\n\\]\n\nThird subquestion: we again start by substituting the causal model into the estimator. Under Equation 5 the outcome satisfies \\[\nY_i = \\beta_i X_i + U_i.\n\\] Then \\(\\tilde{\\beta}\\) can be written as \\[\n\\tilde{\\beta} = \\dfrac{1}{N}\\sum_{i=1}^N \\beta_i +  \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{U_i}{X_i}.\n\\tag{7}\\] The second sum in Equation 7 can be handled as in the first subquestion using assumptions (1)-(3). The first sum satisfies \\[\n\\dfrac{1}{N}\\sum_{i=1}^N \\beta_i  \\xrightarrow{p} \\E[\\beta_i]\n\\] by the law of large numbers provided\n\n\\(\\E[\\beta_i]\\) exists.\n\nWe conclude by the continuous mapping theorem (where do we apply it?) that \\(\\tilde{\\beta}\\) is consistent for \\(\\E[\\beta_i]\\) under conditions (1)-(3) and (5).\nNote that this consistency result does not restrict the dependence between \\(\\beta_i\\) and \\(X_i\\). This is in contrast to the behavior of the OLS estimator (see lectures).\n\n\n\n\nLet the outcome \\(Y_i\\), the covariates \\(\\bX_i\\), and an unobserved component \\(U_i\\) be linked through the linear causal model \\[\nY_i^{\\bx} = \\bx'\\bbeta + U_i\n\\] Suppose that we observe an IID sample of data on \\(Y_i, \\bX_i\\).\nDefine the ridge estimator \\(\\tilde{\\bbeta}\\) as \\[\n\\tilde{\\bbeta} = (\\bX'\\bX+ \\lambda_N \\bI_k)^{-1}\\bX'\\bY,\n\\] where \\(\\lambda_N\\) is some non-negative number, \\(\\bI_k\\) is the \\(k\\times k\\) identity matrix, and we assume that \\((\\bX'\\bX+ \\lambda_N \\bI_k)\\) is invertible.\n\nSuppose that \\(\\bX_i\\) is scalar. Show that \\(\\abs{\\tilde{\\bbeta}}\\leq \\abs{\\hat{\\bbeta}}\\), where \\(\\hat{\\bbeta}\\) is the OLS estimator (in words, the ridge estimator is always weakly closer to 0 than the OLS estimator — it is “shrunk” to zero).\nSuppose that \\(\\lambda_N = cN\\) for some fixed \\(c\\geq 0\\). Find the probability limit of \\(\\tilde{\\bbeta}\\). State explicitly any moment assumptions you make. When is \\(\\tilde{\\bbeta}\\) consistent for \\(\\bbeta\\)?\n(Optional): prove that ridge estimator satisfies \\[\n\\tilde{\\bbeta} = \\argmin_{\\bb} \\sum_{i=1}^N (Y_i - \\bX_i'\\bb)^{2} + \\lambda \\norm{\\bb}^2\n\\] Hint: use the same approach as we used to derive the OLS estimator.\n\n\n\nClick to see the solution\n\nFirst subquestion: in the scalar case we can write the two estimators as \\[\n\\begin{aligned}\n\\hat{\\bbeta} & = \\dfrac{\\sum_{i=1}^N X_i Y_i }{\\sum_{i=1}^N X_i^2},\\\\\n\\tilde{\\bbeta} & = \\dfrac{\\sum_{i=1}^N X_i Y_i}{\\sum_{i=1}^N X_i^2 + \\lambda_N}.\n\\end{aligned}\n\\] We can then divide the ridge estimator by the OLS estimator: \\[\n\\dfrac{\\tilde{\\bbeta}}{\\hat{\\bbeta}} = \\dfrac{\\sum_{i=1}^N X_i^2}{ \\sum_{i=1}^N X_i^2 + \\lambda_N } \\leq 1.\n\\] The desired inequality follows.\n\nSecond subquestion: for the second subquestion, we again start by substituting the model into the estimator: \\[\n\\begin{aligned}\n\\tilde{\\bbeta} & = (\\bX'\\bX+ \\lambda_N \\bI_k)^{-1}\\bX'\\bY\\\\\n& = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' + c\\bI_k\\right)^{-1} \\dfrac{1}{N}\\sum_{i=1}^N \\bX_iY_i\\\\\n& = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' + c\\bI_k\\right)^{-1} \\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'\\bbeta\\\\\n& \\quad + \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' + c\\bI_k\\right)^{-1} \\dfrac{1}{N}\\sum_{i=1}^N \\bX_i U_i\n\\end{aligned}\n\\tag{8}\\] We now only need to handle the individual averages in the above expression.\nFirst, we assume that\n\n\\(\\E[\\bX_i\\bX_i']\\) and \\(\\E[\\bX_i'U_i]\\) exist\n\nThen by the law of large number it holds that \\[\n\\begin{aligned}\n\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i \\bX_i' & \\xrightarrow{p} \\E[\\bX_i\\bX_i'], \\\\\n\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i U_i & \\xrightarrow{p} \\E[\\bX_iU_i] .\n\\end{aligned}\n\\] By the continuous mapping theorem it also follows that \\[\n\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i \\bX_i' + c\\bI_i \\xrightarrow{p} \\E[\\bX_i\\bX_i'] + c\\bI_k.\n\\tag{9}\\]\nSecond, to handle the leading terms in Equation 8, we also assume that\n\n\\(\\E[\\bX_i\\bX_i'] + c\\bI_k\\) is invertible\n\nThen by the continuous mapping theorem and Equation 9 it holds that \\[\n\\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i \\bX_i' + c\\bI_k\\right)^{-1} \\xrightarrow{p} \\left(\\E[\\bX_i\\bX_i'] + c\\bI_k\\right)^{-1}.\n\\]\nCombining the above arguments together and applying the continuous mapping theorem, we conclude that \\[\n\\begin{aligned}\n\\tilde{\\bbeta} & \\xrightarrow{p}  \\left(\\E[\\bX_i\\bX_i'] + c\\bI_k\\right)^{-1} \\E[\\bX_i\\bX_i']\\bbeta\n\\\\\n& \\quad +  \\left(\\E[\\bX_i\\bX_i'] + c\\bI_k\\right)^{-1} \\E[\\bX_iU_i].\n\\end{aligned}\n\\] We conclude that \\(\\tilde{\\bbeta}\\) is consistent for \\(\\bbeta\\) if \\(c=0\\) and \\(\\E[\\bX_iU_i]=0\\).\n\n\nWhy would one use \\(\\tilde{\\bbeta}\\)? Note that \\(\\bX'\\bX + c\\bI_k\\) is invertible if \\(c&gt;0\\), regardless of invertibility of \\(\\bX'\\bX\\). This means that \\(\\tilde{\\bbeta}\\) can be computed even if the OLS estimator cannot. A leading case is high-dimensional regression, where the number of regressors \\(k\\) exceeds the number \\(N\\) of data points. See section 6.2.1 in James et al. (2023) about the ridge estimator and regularization techniques in general. We will discuss some of these ideas later in the class.\n\n\n\nLet the outcome \\(Y_i\\), the covariates \\(\\bX_i\\), and an unobserved component \\(U_i\\) be linked through the linear causal model \\[\nY_i^{\\bx} = \\bx'\\bbeta + U_i\n\\] Suppose that our data is IID, that \\(\\E[\\bX_iU_i]=0\\), the second moments of the data are finite, and that \\(\\E[\\bX_i\\bX_i']\\) is invertible.\nSuppose that we do not observe the true \\(Y_i\\), but instead a mismeasured version \\(Y_i^*= Y_i + V_i\\), where the measurement error \\(V_i\\) is mean zero, independent of \\((X_i, U_i)\\), and has finite second moments.\n\nShow that the OLS estimator for the regression of \\(Y_i^*\\) on \\(\\bX_i\\) is consistent for \\(\\bbeta\\).\nDerive the asymptotic distribution of the above OLS estimator. Express the asymptotic variance in terms of moments involving \\(V_i\\) and \\(U_i\\). Interpret the result: how does the measurement error in \\(\\bX\\) affect the asymptotic variance of the OLS estimator (increase, decrease, unchanged, unclear)?\n\nNow suppose that we do observe \\(Y_i\\), but we do not observe \\(\\bX_i\\). Instead, we only see a mismeasured version \\(\\bX_i^*= \\bX_i + \\bV_i\\), where the measurement error \\(\\bV_i\\) is mean zero, independent of \\((\\bX_i, U_i)\\), and has finite second moments.\n\nCompute the limit of the OLS estimator in the regression of \\(Y_i\\) on \\(\\bX_i^*\\) in terms of \\((Y_i, \\bX_i, V_i, U_i)\\). Is this estimator consistent for \\(\\bbeta\\)? If so, under which conditions?\n\nCompare the two cases of measurement error.\n\n\nClick to see the solution\n\nFirst subquestion: we begin by writing out the OLS estimator, and substituting the causal and measurement models: \\[\n\\begin{aligned}\n\\hat{\\bbeta} & = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' \\right)^{-1} \\dfrac{1}{N} \\sum_{i=1}^N \\bX_iY_i^* \\\\\n& = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' \\right)^{-1} \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i(Y_i + V_i)\\\\\n& = \\bbeta +\\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' \\right)^{-1} \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i (U_i+V_i)\n\\end{aligned}\n\\tag{10}\\]\nWe handle the individual averages using the law of large numbers and combine the results using the continuous mapping theorem.\nBy the assumptions of the problem and the law of large numbers it holds that \\[\n\\begin{aligned}\n\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' & \\xrightarrow{p} \\E[\\bX_i\\bX_i'], \\\\\n\\dfrac{1}{N}\\sum_{i=1}^N \\bX_iU_i & \\xrightarrow{p} \\E[\\bX_iU_i] =0, \\\\\n\\dfrac{1}{N}\\sum_{i=1}^N \\bX_iV_i & \\xrightarrow{p} \\E[\\bX_iV_i] =\\E[\\bX_i]\\E[V_i]=0, \\\\\n\\end{aligned}\n\\] where we use the independence of \\(V_i\\) in the last line.\nBy the continuous mapping theorem and assumption that \\(\\E[\\bX_i\\bX_i']\\) is invertible it holds that \\[\n\\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'\\right)^{-1}  \\xrightarrow{p} \\left( \\E[\\bX_i\\bX_i']\\right)^{-1}.\n\\tag{11}\\]\nBy the continuous mapping theorem and the above results we conclude that \\[\n\\hat{\\bbeta} \\xrightarrow{p} \\bbeta.\n\\]\n\nSecond subquestion: to analyze the asymptotic distribution, we go to the last line in Equation 10. By the assumptions of the problem it holds that \\[\n\\E[X_i(U_i+V_i)] = 0.\n\\] Accordingly, \\[\n\\begin{aligned}\n& \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i (U_i+V_i) \\\\\n& = \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i (U_i+V_i) - \\E[\\bX_i(U_i+V_i)].\n\\end{aligned}\n\\] We have identified a term to which we can apply the central limit theorem. We can now proceed as with the usual OLS estimator without measurement error in \\(Y_i\\).\n\nBy the central limit theorem it holds that \\[\n\\begin{aligned}\n& \\dfrac{1}{\\sqrt{N}} \\sum_{i=1}^N \\bX_i (U_i+V_i) \\\\\n& \\sqrt{N}\\left( \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i (U_i+V_i) - \\E[\\bX_i(U_i+V_i)] \\right) \\\\\n& \\xrightarrow{d} N(0, \\E[(U_i+V_i)^2\\bX_i\\bX_i'])\n\\end{aligned}\n\\]\nBy Equation 11 it holds that \\[\n\\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'\\right)^{-1}  \\xrightarrow{p} \\left( \\E[\\bX_i\\bX_i']\\right)^{-1}.\n\\]\nBy (1)-(2), Slutsky’s theorem, and the properties of variance it follows that \\[\n\\begin{aligned}\n& \\sqrt{N}(\\hat{\\bbeta}-\\bbeta)\\\\\n& = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'\\right)^{-1} \\dfrac{1}{\\sqrt{N} } \\sum_{i=1}^N \\bX_i (U_i+V_i)   \\\\\n&  \\xrightarrow{d} N\\left(0,  \\left( \\E[\\bX_i\\bX_i']\\right)^{-1} \\E[(U_i+V_i)^2\\bX_i\\bX_i']\\left( \\E[\\bX_i\\bX_i']\\right)^{-1} \\right)\n\\end{aligned}\n\\]\n\nWe can further examine the asymptotic variance. First consider the middle component: \\[\n\\begin{aligned}\n&\\E[(U_i+V_i)^2\\bX_i\\bX_i'] \\\\\n& = \\E[U_i^2 \\bX_i\\bX_i'] + 2\\E[V_i]\\E[U_i\\bX_i\\bX_i'] + \\E[V_i^2]\\E[\\bX_i\\bX_i'] \\\\\n& = \\E[U_i^2 \\bX_i\\bX_i'] + \\E[V_i^2]\\E[\\bX_i\\bX_i'],\n\\end{aligned}\n\\] where we have used the properties of \\(V_i\\).\nSubstituting this expression back into the asymptotic variance expression gives us \\[\n\\begin{aligned}\n& \\left( \\E[\\bX_i\\bX_i']\\right)^{-1} \\E[(U_i+V_i)^2\\bX_i\\bX_i']\\left( \\E[\\bX_i\\bX_i']\\right)^{-1} \\\\\n& = \\left( \\E[\\bX_i\\bX_i']\\right)^{-1} \\E[U_i^2\\bX_i\\bX_i']\\left( \\E[\\bX_i\\bX_i']\\right)^{-1} + \\E[V_i^2](\\E[\\bX_i\\bX_i'])^{-1}.\n\\end{aligned}\n\\] The first term is the asymptotic variance of the OLS estimator without measurement error. The presence of independent measurement error adds an additional positive definite component — increases the asymptotic variance (check this at least in the scalar case!).\n\nThird subquestion: we again proceed by wriring down the estimator and then substituting the causal and measurement models. We will do those substitutions step-by-step to keep things cleaner: \\[\n\\begin{aligned}\n\\hat{\\bbeta} &  = \\left(\\sum_{i=1}^N \\bX_i^*\\bX_i^{*'}  \\right)^{-1}\\sum_{i=1}^N \\bX_i^*Y_i \\\\\n& = \\left(\\sum_{i=1}^N \\bX_i^*\\bX_i^{*'}  \\right)^{-1}\\sum_{i=1}^N \\bX_i^* (\\bX_i'\\bbeta + U_i)\\\\\n& = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i^*\\bX_i^{*'}  \\right)^{-1} \\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'\\bbeta\\\\\n& \\quad + \\left(\\dfrac{1}{N} \\sum_{i=1}^N \\bX_i^*\\bX_i^{*'}  \\right)^{-1} \\dfrac{1}{N}\\sum_{i=1}^N \\bV_i\\bX_i'\\bbeta \\\\\n& \\quad + \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i^*\\bX_i^{*'}  \\right)^{-1} \\dfrac{1}{N} \\sum_{i=1}^N \\bX_iU_i\\\\\n& \\quad + \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i^*\\bX_i^{*'}  \\right)^{-1}\\dfrac{1}{N} \\sum_{i=1}^N \\bV_i U_i.\n\\end{aligned}\n\\tag{12}\\] We now analyze the different averages in the above expressions. By the law of large numbers it holds that \\[\n\\begin{aligned}\n\\dfrac{1}{N}\\sum_{i=1}^N\\bV_i\\bX_i' & \\xrightarrow{p} \\E[\\bV_i\\bX_i'] = \\E[\\bV_i]\\E[\\bX_i'] =0, \\\\\n\\dfrac{1}{N}\\sum_{i=1}^N\\bX_i U_i & \\xrightarrow{p} \\E[\\bX_iU_i]  =0, \\\\\n\\dfrac{1}{N}\\sum_{i=1}^N \\bV_i U_i & \\xrightarrow{p} \\E[\\bV_iU_i] = \\E[\\bV_i]\\E[U_i] =0.\n\\end{aligned}\n\\] It also holds that \\[\n\\begin{aligned}\n\\dfrac{1}{N} \\sum_{i=1}^N \\bX_i^*\\bX_i^{*'} & =   \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i\\bX_i'   + \\dfrac{1}{N} \\sum_{i=1}^N \\bV_i\\bV_i'  \\\\\n& \\quad +\n\\dfrac{1}{N} \\sum_{i=1}^N \\bX_i\\bV_i' +\n\\dfrac{1}{N} \\sum_{i=1}^N \\bV_i\\bX_i' \\\\\n& \\xrightarrow{p} \\E[\\bX_i\\bX_i'] + \\E[\\bV_i\\bV_i']\n\\end{aligned}\n\\] Combining the above convergence results, the continuous mapping theorem and Equation 12 we get that \\[\n\\hat{\\bbeta} \\xrightarrow{p} \\left(\\E[\\bX_i\\bX_i']  + \\E[\\bV_i\\bV_i'] \\right)^{-1} \\E[\\bX_i\\bX_i']\\bbeta.\n\\] We see that in general the OLS estimator is not consistent for \\(\\bbeta\\) if there is measurement error in the covariates.\nConsistency holds if \\(\\E[\\bV_i\\bV_i']=0\\). Since \\(\\E[\\bV_i]=0\\), it holds that \\(\\E[\\bV_i\\bV_i'] = \\var(\\bV_i)\\). In other words, consistency requires that \\(\\var(\\bV_i)=0\\), which means there is no measurement error in \\(\\bX_i\\).\nTo summarize, we find notable differences between the two kinds of measurement errors.\n\nIndependent measurement error in the outcome variable does not break consistency or asymptotic normality. It only increases the asymptotic variances (reduces precision).\nIndependent measurement error in the covariates makes the OLS estimator inconsistent.\n\n\n\n\n\nLet \\(Y_i\\) be some outcome of interest. Let \\(\\bX_i\\) be an observed covariate vector; \\(\\E[\\bX_i\\bX_i']\\) is assumed to be invertible. Let \\(U_i\\) be an unobserved component that satisfies \\(\\E[\\bX_iU_i]=0\\). Let \\(\\bW_i\\) be another group of variables that affect \\(Y_i\\). Suppose that \\(Y_i\\) and \\((\\bX_i, \\bW_i)\\) are related through the potential outcomes model \\[\nY_i^{(\\bx, \\bw)} = \\bx'\\bbeta + \\bw'\\bdelta + U_i.\n\\]\nSuppose that \\(\\bW_i\\) is not observed, and we instead regress \\(Y_i\\) only on \\(\\bX_i\\). Find the probability limit of the corresponding OLS estimator. Make any necessary moment assumptions. When is that limit equal to \\(\\bbeta\\)?\n\n\nClick to see the solution\n\nYet again the approach is to substitute the true model into the estimator. The observed outcomes satisfy \\[\nY_i = \\bX_i'\\bbeta + \\bW_i'\\bdelta + U_i.\n\\]\nThe OLS estimator of \\(Y_i\\) on \\(\\bX_i\\) satisfies \\[\n\\begin{aligned}\n\\hat{\\bbeta} & = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'   \\right)^{-1}\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i Y_i\\\\\n& =  \\bbeta+  \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'   \\right)^{-1} \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i U_i \\\\\n& \\quad + \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'   \\right)^{-1} \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i\\bW_i'\\bdelta\n\\end{aligned}\n\\] We now handle the individual sums in the usual way. By the law of large numbers, continuous mapping theorem and our moment conditions it holds that \\[\n\\begin{aligned}\n\\dfrac{1}{N} \\sum_{i=1}^N \\bX_iU_i & \\xrightarrow{p} \\E[\\bX_iU_i] =0 ,\\\\\n\\dfrac{1}{N} \\sum_{i=1}^N \\bX_i\\bW_i' & \\xrightarrow{p} \\E[\\bX_i\\bW_i'] ,\\\\\n\\left( \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i\\bX_i'\\right)^{-1} & \\xrightarrow{p} \\left( \\E[\\bX_i\\bX_i'] \\right)^{-1}.\n\\end{aligned}\n\\] By the continuous mapping theorem we obtain the probability limit of \\(\\hat{\\bbeta}\\): \\[\n\\hat{\\bbeta} \\xrightarrow{p} \\bbeta + \\left(\\E[\\bX_i\\bX'] \\right)^{-1}\\E[\\bX_i\\bW_i']\\bdelta.\n\\] This limit is equal to \\(\\bbeta\\) if \\(\\E[\\bX_i\\bW_i']\\bdelta = 0\\). Standard sufficient conditions are that \\(\\bdelta = 0\\) (and so \\(\\bW_i\\) are not important to \\(Y_i\\)) or that \\(\\bX_i\\) and \\(\\bW_i\\) are orthogonal in the sense that \\(\\E[\\bX_i\\bW_i']=0\\).",
    "crumbs": [
      "Linear Regression II",
      "Exercises: Vector Linear Model and Asymptotics"
    ]
  },
  {
    "objectID": "exercises/exercises-linear-asymptotic.html#theoretical-exercises",
    "href": "exercises/exercises-linear-asymptotic.html#theoretical-exercises",
    "title": "Exercises: Vector Linear Model and Asymptotics",
    "section": "",
    "text": "Suppose that we observe data \\((Y_1, \\bX_1), \\dots (Y_N, \\bX_N)\\). Let \\(\\bY\\) be the \\(N\\times 1\\) vector of outcomes, \\(\\bX\\) be the data matrix, and suppose that \\(\\bX'\\bX\\) is invertible.\nLet the vector \\(\\hat{\\bY}\\) of fitted values and the vector \\(\\hat{\\be}\\) of errors be given by \\[\n\\begin{aligned}\n\\hat{\\bY} & = \\bX\\hat{\\bbeta},\\\\\n\\hat{\\be} & = \\bY- \\hat{\\bY},\n\\end{aligned}\n\\] where \\(\\hat{\\bbeta}\\) is the OLS estimator.\nFind the OLS coefficient vector from\n\nRegressing \\(\\hat{Y}_i\\) on \\(\\bX_i\\).\nRegressing \\(\\hat{e}_i\\) on \\(\\bX_i\\).\n\nIn both cases, express the OLS estimator in terms of \\(\\bY\\) and \\(\\bX\\). Then express it in terms of \\((Y_i, \\bX_i)\\), \\(i=1, \\dots, N\\).\n\n\nClick to see the solution\n\nFirst subquestion: First consider the question of regressing \\(\\hat{\\bY}\\) on \\(\\bX\\). Let \\(\\tilde{\\bbeta}\\) be the OLS estimator of regressing \\(\\hat{\\bY}\\) on \\(\\bX\\). We can use the usual formula for OLS, but replace \\(\\bY\\) with \\(\\hat{\\bY}\\). We can then use the definition of \\(\\hat{\\bY}\\) and \\(\\hat{\\bbeta}\\) to express the coefficients in terms of the original data: \\[\n\\begin{aligned}\n\\tilde{\\bbeta} & = (\\bX'\\bX)^{-1}\\bX'\\hat{\\bY} = (\\bX'\\bX)^{-1}\\bX'\\bX\\hat{\\bbeta} \\\\\n& = \\hat{\\bbeta} = (\\bX'\\bX)^{-1}\\bX\\bY = \\left(\\sum_{i=1}^N \\bX_i\\bX_i' \\right)^{-1}\\sum_{i=1}^N\\bX_iY_i,\n\\end{aligned}\n\\] One way to interpret the above result is that applying OLS more than once does not change anything. The first application already extracts all the information that can be linearly explained by \\(\\bX\\) (a property sometimes called idempotency).\n\nSecond subquestion: We can proceed similarly with \\(\\hat{\\be}\\). Let \\(\\check{\\bbeta}\\) be the OLS estimator for regressing \\(\\hat{\\bbeta}\\) on \\(\\bX\\). We can again use the general expression for the OLS estimator and substitute the definition of \\(\\hat{\\bbeta}\\): \\[\n\\begin{aligned}\n\\check{\\bbeta} & = (\\bX'\\bX)^{-1}\\bX'\\hat{\\be} = (\\bX'\\bX)^{-1}\\bX'(\\bY-\\hat{\\bY})\\\\\n& = \\hat{\\bbeta} -\\tilde{\\bbeta} = \\hat{\\bbeta}- \\hat{\\bbeta}\\\\\n& = 0,\n\\end{aligned}\n\\] where we have used the first result of the problem.\n\n\n\n\nLet \\(X_i\\) and \\(Y_i\\) be scalar variables. Let \\(X_i\\) satisfy \\[\nX_i = \\begin{cases}\n1, & i = 1, \\\\\n0, & i &gt; 1.\n\\end{cases}\n\\tag{1}\\]\n\nSuppose we have a sample of \\(N\\) units: \\((Y_1, X_1), \\dots, (Y_N, X_N)\\). Can we compute the OLS estimator for regressing \\(Y_i\\) on \\(X_i\\) (without a constant)? If yes, express the estimator in terms of \\((Y_i, X_i)\\), \\(i=1,\\dots, N\\). If not, explain why.\nSuppose that \\(X_i\\) and \\(Y_i\\) are linked through the linear causal model \\[\nY_i^x = \\beta x + U_i,\n\\tag{2}\\] where \\(Y_i^x\\) is a potential outcome, \\(U_i\\) is independent of \\(X_i\\) with \\(\\E[U_i]=0\\). Why does the OLS estimator of \\(\\beta\\) not converge to \\(\\beta\\) without stronger assumptions on \\(U_i\\)? Informally, which of the conditions of our consistency results fail?\nProvide an informal empirical interpretation of the above data-generating process for \\(X_i\\).\n\n\n\nClick to see the solution\n\nFirst subquestion: The computability of the OLS estimator is determined by one key question: is \\(\\bX'\\bX\\) invertible? If \\(\\bX'\\bX\\) is invertible, then the answer is positive.\nIn this case, there is only one scalar covariate. \\(\\bX'\\bX\\) is itself then scalar (\\(1\\times 1\\)) and given by \\[\n\\bX'\\bX = \\sum_{i=1}^N X_i^2\n\\] By Equation 1, we have that \\(\\sum_{i=1}^N X_i^2=1\\), which is invertible. It follows that we can compute \\(\\hat{\\beta}=(\\bX'\\bX)^{-1}\\bX'\\bY\\): \\[\n\\hat{\\beta} = (\\bX'\\bX)^{-1}\\bX'\\bY = \\dfrac{\\sum_{i=1}^N X_iY_i}{\\sum_{i=1}^N X_i^2} = Y_1.\n\\tag{3}\\]\n\nSecond subquestion: To answer this question formally, we use the same technique we use in the lectures — substituting the underlying model into the estimator. By Equation 2, the realized outcomes satisfy \\[\nY_i = \\beta X_i + U_i\n\\]\nWe substitute this expression for realized values into the OLS estimator (3) to obtain \\[\n\\hat{\\beta} = \\beta + U_1,\n\\] where we have used that \\(X_1 =1\\) by Equation 1.\nWe now see that the value of \\(\\hat{\\beta}\\) does not depend on sample size \\(N\\). The full value of \\(U_1\\) will always be present in \\(\\hat{\\beta}\\): as \\(N\\to\\infty\\) \\[\n\\hat{\\beta} \\xrightarrow{p} \\beta + U_1\n\\] The only case where \\(\\hat{\\beta}\\xrightarrow{p}\\beta\\) is when \\(U_1=0\\) — an additional stronger condition.\nWhich conditions of our consistency results fail? There are two conditions that hold:\n\nOrthogonality: \\(\\E[X_iU_i] =0\\) holds.\nIndependence of units\n\nThere are two conditions that do not hold:\n\nIdentical distributions: unit 1 is different to the rest.\nInvertibility of the limit of \\(N^{-1}\\sum_{i=1}^N\\bX_i\\bX_i'\\). By Equation 1, this sum is equal to \\(N^{-1}\\to 0\\).\n\nOf these two failing conditions, the first one is usually not a big issue. It concerns only a single point, and in general we have tools for handling non-identical distributions. It is the second condition that creates a problem in the limit.\nThe message of this problem is that we need two invertibility conditions: the sample condition (on \\(\\bX'\\bX\\)) and the population one (on \\(\\E[\\bX_i\\bX_i']\\)). These conditions play different roles. Each can fail, while the other condition is true.\n\nThird subquestion: we can imagine a simple experiment in which the subjects have arrived to the lab in a random order, independently of their characteristics. However, there is only one real treatment, which is given to the first unit. Everyone else receives a placebo.\n\n\n\n\nLet \\(X_i\\) and \\(U_i\\) be scalar random variables. Suppose that \\(X_i\\) satisfies \\(X_i\\geq c&gt;0\\) for some \\(c\\) (strictly positive and bounded away from 0). Let \\(Y_i\\) be some outcome. Suppose that the following linear causal model holds: the potential outcome \\(Y_i^x\\) is determined as \\[\nY_i^x = \\beta x + U_i.\n\\tag{4}\\] The realized outcome \\(Y_i\\) is determined as \\(Y_i = Y_i^{X_i}\\).\nConsider the following estimator for \\(\\beta\\): \\[\n\\tilde{\\beta} = \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{Y_i}{X_i}.\n\\]\n\nPropose conditions under which \\(\\tilde{\\beta}\\) is consistent for \\(\\beta\\) and prove consistency.\nDerive the asymptotic distribution of \\(\\tilde{\\beta}\\). Describe any additional assumptions you make.\nNow suppose that the causal model allows heterogeneous effects: \\[\nY_i^x = \\beta_i x + U_i.\n\\tag{5}\\] Under which conditions does \\(\\tilde{\\beta}\\) consistently estimate \\(\\E[\\beta_i]\\)?\n\n\n\nClick to see the solution\n\nFirst subquestion: we again use the key technique — substituting the true model into the estimator. By Equation 4, the outcome \\(Y_i\\) satisfies \\[\nY_i = \\beta X_i + U_i.\n\\] We can substitute this expression into \\(\\tilde{\\beta}\\) to obtain \\[\n\\tilde{\\beta} = \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{Y_i}{X_i} = \\beta+ \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{U_i}{X_i}\n\\] The law of large number applies to the sum on the right hand side if\n\n\\((X_i, U_i)\\) are IID.\n\\(\\E[U_i/X_i]\\) exists.\n\nWe make these assumptions. Then by the law of large numbers: \\[\n\\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{U_i}{X_i}\\xrightarrow{p} \\E\\left[ \\dfrac{U_i}{X_i} \\right].\n\\] By the continuous mapping theorem: \\[\n\\tilde{\\beta} \\xrightarrow{p} \\beta + \\E\\left[ \\dfrac{U_i}{X_i} \\right].\n\\] \\(\\tilde{\\beta}\\) is consistent for \\(\\beta\\) if\n\n\\(\\E[U_i/X_i]=0\\) (note that it is sufficient that \\(\\E[U_i|X_i]=0\\) for this condition, why?).\n\nWe conclude that \\(\\tilde{\\beta}\\) is consistent for \\(\\beta\\) under assumptions (1)-(3).\n\nSecond subquestion: to study the asymptotic distribution, we keep the above conditions (1)-(3). Under these conditions (especially (3)) we note that \\[\n\\begin{aligned}\n\\tilde{\\beta} - \\beta & = \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{U_i}{X_i} \\\\\n& = \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{U_i}{X_i} - \\E\\left[\\dfrac{U_i}{X_i}\\right].\n\\end{aligned}\n\\tag{6}\\] The bottom line is in the form used in the central limit theorem: sample average minus population average. We can then apply the central limit theorem provided the following assumption holds:\n\nFinite second moments: \\(\\E[U_i^2/X_i^2]&lt;\\infty\\).\n\nThen by the central limit theorem it holds that \\[\n\\sqrt{N}\\left(  \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{U_i}{X_i} - \\E\\left[\\dfrac{U_i}{X_i}\\right] \\right)\\xrightarrow{d} N\\left(0, \\E\\left[\\dfrac{U_i^2}{X_i^2} \\right] \\right).\n\\]\nBy Equation 6 we conclude that, if assumptions (1)-(4) hold, then \\[\n\\sqrt{N}\\left(\\tilde{\\beta}- \\beta \\right)\\xrightarrow{d} N\\left(0, \\E\\left[\\dfrac{U_i^2}{X_i^2} \\right] \\right).\n\\]\n\nThird subquestion: we again start by substituting the causal model into the estimator. Under Equation 5 the outcome satisfies \\[\nY_i = \\beta_i X_i + U_i.\n\\] Then \\(\\tilde{\\beta}\\) can be written as \\[\n\\tilde{\\beta} = \\dfrac{1}{N}\\sum_{i=1}^N \\beta_i +  \\dfrac{1}{N}\\sum_{i=1}^N \\dfrac{U_i}{X_i}.\n\\tag{7}\\] The second sum in Equation 7 can be handled as in the first subquestion using assumptions (1)-(3). The first sum satisfies \\[\n\\dfrac{1}{N}\\sum_{i=1}^N \\beta_i  \\xrightarrow{p} \\E[\\beta_i]\n\\] by the law of large numbers provided\n\n\\(\\E[\\beta_i]\\) exists.\n\nWe conclude by the continuous mapping theorem (where do we apply it?) that \\(\\tilde{\\beta}\\) is consistent for \\(\\E[\\beta_i]\\) under conditions (1)-(3) and (5).\nNote that this consistency result does not restrict the dependence between \\(\\beta_i\\) and \\(X_i\\). This is in contrast to the behavior of the OLS estimator (see lectures).\n\n\n\n\nLet the outcome \\(Y_i\\), the covariates \\(\\bX_i\\), and an unobserved component \\(U_i\\) be linked through the linear causal model \\[\nY_i^{\\bx} = \\bx'\\bbeta + U_i\n\\] Suppose that we observe an IID sample of data on \\(Y_i, \\bX_i\\).\nDefine the ridge estimator \\(\\tilde{\\bbeta}\\) as \\[\n\\tilde{\\bbeta} = (\\bX'\\bX+ \\lambda_N \\bI_k)^{-1}\\bX'\\bY,\n\\] where \\(\\lambda_N\\) is some non-negative number, \\(\\bI_k\\) is the \\(k\\times k\\) identity matrix, and we assume that \\((\\bX'\\bX+ \\lambda_N \\bI_k)\\) is invertible.\n\nSuppose that \\(\\bX_i\\) is scalar. Show that \\(\\abs{\\tilde{\\bbeta}}\\leq \\abs{\\hat{\\bbeta}}\\), where \\(\\hat{\\bbeta}\\) is the OLS estimator (in words, the ridge estimator is always weakly closer to 0 than the OLS estimator — it is “shrunk” to zero).\nSuppose that \\(\\lambda_N = cN\\) for some fixed \\(c\\geq 0\\). Find the probability limit of \\(\\tilde{\\bbeta}\\). State explicitly any moment assumptions you make. When is \\(\\tilde{\\bbeta}\\) consistent for \\(\\bbeta\\)?\n(Optional): prove that ridge estimator satisfies \\[\n\\tilde{\\bbeta} = \\argmin_{\\bb} \\sum_{i=1}^N (Y_i - \\bX_i'\\bb)^{2} + \\lambda \\norm{\\bb}^2\n\\] Hint: use the same approach as we used to derive the OLS estimator.\n\n\n\nClick to see the solution\n\nFirst subquestion: in the scalar case we can write the two estimators as \\[\n\\begin{aligned}\n\\hat{\\bbeta} & = \\dfrac{\\sum_{i=1}^N X_i Y_i }{\\sum_{i=1}^N X_i^2},\\\\\n\\tilde{\\bbeta} & = \\dfrac{\\sum_{i=1}^N X_i Y_i}{\\sum_{i=1}^N X_i^2 + \\lambda_N}.\n\\end{aligned}\n\\] We can then divide the ridge estimator by the OLS estimator: \\[\n\\dfrac{\\tilde{\\bbeta}}{\\hat{\\bbeta}} = \\dfrac{\\sum_{i=1}^N X_i^2}{ \\sum_{i=1}^N X_i^2 + \\lambda_N } \\leq 1.\n\\] The desired inequality follows.\n\nSecond subquestion: for the second subquestion, we again start by substituting the model into the estimator: \\[\n\\begin{aligned}\n\\tilde{\\bbeta} & = (\\bX'\\bX+ \\lambda_N \\bI_k)^{-1}\\bX'\\bY\\\\\n& = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' + c\\bI_k\\right)^{-1} \\dfrac{1}{N}\\sum_{i=1}^N \\bX_iY_i\\\\\n& = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' + c\\bI_k\\right)^{-1} \\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'\\bbeta\\\\\n& \\quad + \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' + c\\bI_k\\right)^{-1} \\dfrac{1}{N}\\sum_{i=1}^N \\bX_i U_i\n\\end{aligned}\n\\tag{8}\\] We now only need to handle the individual averages in the above expression.\nFirst, we assume that\n\n\\(\\E[\\bX_i\\bX_i']\\) and \\(\\E[\\bX_i'U_i]\\) exist\n\nThen by the law of large number it holds that \\[\n\\begin{aligned}\n\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i \\bX_i' & \\xrightarrow{p} \\E[\\bX_i\\bX_i'], \\\\\n\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i U_i & \\xrightarrow{p} \\E[\\bX_iU_i] .\n\\end{aligned}\n\\] By the continuous mapping theorem it also follows that \\[\n\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i \\bX_i' + c\\bI_i \\xrightarrow{p} \\E[\\bX_i\\bX_i'] + c\\bI_k.\n\\tag{9}\\]\nSecond, to handle the leading terms in Equation 8, we also assume that\n\n\\(\\E[\\bX_i\\bX_i'] + c\\bI_k\\) is invertible\n\nThen by the continuous mapping theorem and Equation 9 it holds that \\[\n\\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i \\bX_i' + c\\bI_k\\right)^{-1} \\xrightarrow{p} \\left(\\E[\\bX_i\\bX_i'] + c\\bI_k\\right)^{-1}.\n\\]\nCombining the above arguments together and applying the continuous mapping theorem, we conclude that \\[\n\\begin{aligned}\n\\tilde{\\bbeta} & \\xrightarrow{p}  \\left(\\E[\\bX_i\\bX_i'] + c\\bI_k\\right)^{-1} \\E[\\bX_i\\bX_i']\\bbeta\n\\\\\n& \\quad +  \\left(\\E[\\bX_i\\bX_i'] + c\\bI_k\\right)^{-1} \\E[\\bX_iU_i].\n\\end{aligned}\n\\] We conclude that \\(\\tilde{\\bbeta}\\) is consistent for \\(\\bbeta\\) if \\(c=0\\) and \\(\\E[\\bX_iU_i]=0\\).\n\n\nWhy would one use \\(\\tilde{\\bbeta}\\)? Note that \\(\\bX'\\bX + c\\bI_k\\) is invertible if \\(c&gt;0\\), regardless of invertibility of \\(\\bX'\\bX\\). This means that \\(\\tilde{\\bbeta}\\) can be computed even if the OLS estimator cannot. A leading case is high-dimensional regression, where the number of regressors \\(k\\) exceeds the number \\(N\\) of data points. See section 6.2.1 in James et al. (2023) about the ridge estimator and regularization techniques in general. We will discuss some of these ideas later in the class.\n\n\n\nLet the outcome \\(Y_i\\), the covariates \\(\\bX_i\\), and an unobserved component \\(U_i\\) be linked through the linear causal model \\[\nY_i^{\\bx} = \\bx'\\bbeta + U_i\n\\] Suppose that our data is IID, that \\(\\E[\\bX_iU_i]=0\\), the second moments of the data are finite, and that \\(\\E[\\bX_i\\bX_i']\\) is invertible.\nSuppose that we do not observe the true \\(Y_i\\), but instead a mismeasured version \\(Y_i^*= Y_i + V_i\\), where the measurement error \\(V_i\\) is mean zero, independent of \\((X_i, U_i)\\), and has finite second moments.\n\nShow that the OLS estimator for the regression of \\(Y_i^*\\) on \\(\\bX_i\\) is consistent for \\(\\bbeta\\).\nDerive the asymptotic distribution of the above OLS estimator. Express the asymptotic variance in terms of moments involving \\(V_i\\) and \\(U_i\\). Interpret the result: how does the measurement error in \\(\\bX\\) affect the asymptotic variance of the OLS estimator (increase, decrease, unchanged, unclear)?\n\nNow suppose that we do observe \\(Y_i\\), but we do not observe \\(\\bX_i\\). Instead, we only see a mismeasured version \\(\\bX_i^*= \\bX_i + \\bV_i\\), where the measurement error \\(\\bV_i\\) is mean zero, independent of \\((\\bX_i, U_i)\\), and has finite second moments.\n\nCompute the limit of the OLS estimator in the regression of \\(Y_i\\) on \\(\\bX_i^*\\) in terms of \\((Y_i, \\bX_i, V_i, U_i)\\). Is this estimator consistent for \\(\\bbeta\\)? If so, under which conditions?\n\nCompare the two cases of measurement error.\n\n\nClick to see the solution\n\nFirst subquestion: we begin by writing out the OLS estimator, and substituting the causal and measurement models: \\[\n\\begin{aligned}\n\\hat{\\bbeta} & = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' \\right)^{-1} \\dfrac{1}{N} \\sum_{i=1}^N \\bX_iY_i^* \\\\\n& = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' \\right)^{-1} \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i(Y_i + V_i)\\\\\n& = \\bbeta +\\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' \\right)^{-1} \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i (U_i+V_i)\n\\end{aligned}\n\\tag{10}\\]\nWe handle the individual averages using the law of large numbers and combine the results using the continuous mapping theorem.\nBy the assumptions of the problem and the law of large numbers it holds that \\[\n\\begin{aligned}\n\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i' & \\xrightarrow{p} \\E[\\bX_i\\bX_i'], \\\\\n\\dfrac{1}{N}\\sum_{i=1}^N \\bX_iU_i & \\xrightarrow{p} \\E[\\bX_iU_i] =0, \\\\\n\\dfrac{1}{N}\\sum_{i=1}^N \\bX_iV_i & \\xrightarrow{p} \\E[\\bX_iV_i] =\\E[\\bX_i]\\E[V_i]=0, \\\\\n\\end{aligned}\n\\] where we use the independence of \\(V_i\\) in the last line.\nBy the continuous mapping theorem and assumption that \\(\\E[\\bX_i\\bX_i']\\) is invertible it holds that \\[\n\\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'\\right)^{-1}  \\xrightarrow{p} \\left( \\E[\\bX_i\\bX_i']\\right)^{-1}.\n\\tag{11}\\]\nBy the continuous mapping theorem and the above results we conclude that \\[\n\\hat{\\bbeta} \\xrightarrow{p} \\bbeta.\n\\]\n\nSecond subquestion: to analyze the asymptotic distribution, we go to the last line in Equation 10. By the assumptions of the problem it holds that \\[\n\\E[X_i(U_i+V_i)] = 0.\n\\] Accordingly, \\[\n\\begin{aligned}\n& \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i (U_i+V_i) \\\\\n& = \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i (U_i+V_i) - \\E[\\bX_i(U_i+V_i)].\n\\end{aligned}\n\\] We have identified a term to which we can apply the central limit theorem. We can now proceed as with the usual OLS estimator without measurement error in \\(Y_i\\).\n\nBy the central limit theorem it holds that \\[\n\\begin{aligned}\n& \\dfrac{1}{\\sqrt{N}} \\sum_{i=1}^N \\bX_i (U_i+V_i) \\\\\n& \\sqrt{N}\\left( \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i (U_i+V_i) - \\E[\\bX_i(U_i+V_i)] \\right) \\\\\n& \\xrightarrow{d} N(0, \\E[(U_i+V_i)^2\\bX_i\\bX_i'])\n\\end{aligned}\n\\]\nBy Equation 11 it holds that \\[\n\\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'\\right)^{-1}  \\xrightarrow{p} \\left( \\E[\\bX_i\\bX_i']\\right)^{-1}.\n\\]\nBy (1)-(2), Slutsky’s theorem, and the properties of variance it follows that \\[\n\\begin{aligned}\n& \\sqrt{N}(\\hat{\\bbeta}-\\bbeta)\\\\\n& = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'\\right)^{-1} \\dfrac{1}{\\sqrt{N} } \\sum_{i=1}^N \\bX_i (U_i+V_i)   \\\\\n&  \\xrightarrow{d} N\\left(0,  \\left( \\E[\\bX_i\\bX_i']\\right)^{-1} \\E[(U_i+V_i)^2\\bX_i\\bX_i']\\left( \\E[\\bX_i\\bX_i']\\right)^{-1} \\right)\n\\end{aligned}\n\\]\n\nWe can further examine the asymptotic variance. First consider the middle component: \\[\n\\begin{aligned}\n&\\E[(U_i+V_i)^2\\bX_i\\bX_i'] \\\\\n& = \\E[U_i^2 \\bX_i\\bX_i'] + 2\\E[V_i]\\E[U_i\\bX_i\\bX_i'] + \\E[V_i^2]\\E[\\bX_i\\bX_i'] \\\\\n& = \\E[U_i^2 \\bX_i\\bX_i'] + \\E[V_i^2]\\E[\\bX_i\\bX_i'],\n\\end{aligned}\n\\] where we have used the properties of \\(V_i\\).\nSubstituting this expression back into the asymptotic variance expression gives us \\[\n\\begin{aligned}\n& \\left( \\E[\\bX_i\\bX_i']\\right)^{-1} \\E[(U_i+V_i)^2\\bX_i\\bX_i']\\left( \\E[\\bX_i\\bX_i']\\right)^{-1} \\\\\n& = \\left( \\E[\\bX_i\\bX_i']\\right)^{-1} \\E[U_i^2\\bX_i\\bX_i']\\left( \\E[\\bX_i\\bX_i']\\right)^{-1} + \\E[V_i^2](\\E[\\bX_i\\bX_i'])^{-1}.\n\\end{aligned}\n\\] The first term is the asymptotic variance of the OLS estimator without measurement error. The presence of independent measurement error adds an additional positive definite component — increases the asymptotic variance (check this at least in the scalar case!).\n\nThird subquestion: we again proceed by wriring down the estimator and then substituting the causal and measurement models. We will do those substitutions step-by-step to keep things cleaner: \\[\n\\begin{aligned}\n\\hat{\\bbeta} &  = \\left(\\sum_{i=1}^N \\bX_i^*\\bX_i^{*'}  \\right)^{-1}\\sum_{i=1}^N \\bX_i^*Y_i \\\\\n& = \\left(\\sum_{i=1}^N \\bX_i^*\\bX_i^{*'}  \\right)^{-1}\\sum_{i=1}^N \\bX_i^* (\\bX_i'\\bbeta + U_i)\\\\\n& = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i^*\\bX_i^{*'}  \\right)^{-1} \\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'\\bbeta\\\\\n& \\quad + \\left(\\dfrac{1}{N} \\sum_{i=1}^N \\bX_i^*\\bX_i^{*'}  \\right)^{-1} \\dfrac{1}{N}\\sum_{i=1}^N \\bV_i\\bX_i'\\bbeta \\\\\n& \\quad + \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i^*\\bX_i^{*'}  \\right)^{-1} \\dfrac{1}{N} \\sum_{i=1}^N \\bX_iU_i\\\\\n& \\quad + \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i^*\\bX_i^{*'}  \\right)^{-1}\\dfrac{1}{N} \\sum_{i=1}^N \\bV_i U_i.\n\\end{aligned}\n\\tag{12}\\] We now analyze the different averages in the above expressions. By the law of large numbers it holds that \\[\n\\begin{aligned}\n\\dfrac{1}{N}\\sum_{i=1}^N\\bV_i\\bX_i' & \\xrightarrow{p} \\E[\\bV_i\\bX_i'] = \\E[\\bV_i]\\E[\\bX_i'] =0, \\\\\n\\dfrac{1}{N}\\sum_{i=1}^N\\bX_i U_i & \\xrightarrow{p} \\E[\\bX_iU_i]  =0, \\\\\n\\dfrac{1}{N}\\sum_{i=1}^N \\bV_i U_i & \\xrightarrow{p} \\E[\\bV_iU_i] = \\E[\\bV_i]\\E[U_i] =0.\n\\end{aligned}\n\\] It also holds that \\[\n\\begin{aligned}\n\\dfrac{1}{N} \\sum_{i=1}^N \\bX_i^*\\bX_i^{*'} & =   \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i\\bX_i'   + \\dfrac{1}{N} \\sum_{i=1}^N \\bV_i\\bV_i'  \\\\\n& \\quad +\n\\dfrac{1}{N} \\sum_{i=1}^N \\bX_i\\bV_i' +\n\\dfrac{1}{N} \\sum_{i=1}^N \\bV_i\\bX_i' \\\\\n& \\xrightarrow{p} \\E[\\bX_i\\bX_i'] + \\E[\\bV_i\\bV_i']\n\\end{aligned}\n\\] Combining the above convergence results, the continuous mapping theorem and Equation 12 we get that \\[\n\\hat{\\bbeta} \\xrightarrow{p} \\left(\\E[\\bX_i\\bX_i']  + \\E[\\bV_i\\bV_i'] \\right)^{-1} \\E[\\bX_i\\bX_i']\\bbeta.\n\\] We see that in general the OLS estimator is not consistent for \\(\\bbeta\\) if there is measurement error in the covariates.\nConsistency holds if \\(\\E[\\bV_i\\bV_i']=0\\). Since \\(\\E[\\bV_i]=0\\), it holds that \\(\\E[\\bV_i\\bV_i'] = \\var(\\bV_i)\\). In other words, consistency requires that \\(\\var(\\bV_i)=0\\), which means there is no measurement error in \\(\\bX_i\\).\nTo summarize, we find notable differences between the two kinds of measurement errors.\n\nIndependent measurement error in the outcome variable does not break consistency or asymptotic normality. It only increases the asymptotic variances (reduces precision).\nIndependent measurement error in the covariates makes the OLS estimator inconsistent.\n\n\n\n\n\nLet \\(Y_i\\) be some outcome of interest. Let \\(\\bX_i\\) be an observed covariate vector; \\(\\E[\\bX_i\\bX_i']\\) is assumed to be invertible. Let \\(U_i\\) be an unobserved component that satisfies \\(\\E[\\bX_iU_i]=0\\). Let \\(\\bW_i\\) be another group of variables that affect \\(Y_i\\). Suppose that \\(Y_i\\) and \\((\\bX_i, \\bW_i)\\) are related through the potential outcomes model \\[\nY_i^{(\\bx, \\bw)} = \\bx'\\bbeta + \\bw'\\bdelta + U_i.\n\\]\nSuppose that \\(\\bW_i\\) is not observed, and we instead regress \\(Y_i\\) only on \\(\\bX_i\\). Find the probability limit of the corresponding OLS estimator. Make any necessary moment assumptions. When is that limit equal to \\(\\bbeta\\)?\n\n\nClick to see the solution\n\nYet again the approach is to substitute the true model into the estimator. The observed outcomes satisfy \\[\nY_i = \\bX_i'\\bbeta + \\bW_i'\\bdelta + U_i.\n\\]\nThe OLS estimator of \\(Y_i\\) on \\(\\bX_i\\) satisfies \\[\n\\begin{aligned}\n\\hat{\\bbeta} & = \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'   \\right)^{-1}\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i Y_i\\\\\n& =  \\bbeta+  \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'   \\right)^{-1} \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i U_i \\\\\n& \\quad + \\left(\\dfrac{1}{N}\\sum_{i=1}^N \\bX_i\\bX_i'   \\right)^{-1} \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i\\bW_i'\\bdelta\n\\end{aligned}\n\\] We now handle the individual sums in the usual way. By the law of large numbers, continuous mapping theorem and our moment conditions it holds that \\[\n\\begin{aligned}\n\\dfrac{1}{N} \\sum_{i=1}^N \\bX_iU_i & \\xrightarrow{p} \\E[\\bX_iU_i] =0 ,\\\\\n\\dfrac{1}{N} \\sum_{i=1}^N \\bX_i\\bW_i' & \\xrightarrow{p} \\E[\\bX_i\\bW_i'] ,\\\\\n\\left( \\dfrac{1}{N} \\sum_{i=1}^N \\bX_i\\bX_i'\\right)^{-1} & \\xrightarrow{p} \\left( \\E[\\bX_i\\bX_i'] \\right)^{-1}.\n\\end{aligned}\n\\] By the continuous mapping theorem we obtain the probability limit of \\(\\hat{\\bbeta}\\): \\[\n\\hat{\\bbeta} \\xrightarrow{p} \\bbeta + \\left(\\E[\\bX_i\\bX'] \\right)^{-1}\\E[\\bX_i\\bW_i']\\bdelta.\n\\] This limit is equal to \\(\\bbeta\\) if \\(\\E[\\bX_i\\bW_i']\\bdelta = 0\\). Standard sufficient conditions are that \\(\\bdelta = 0\\) (and so \\(\\bW_i\\) are not important to \\(Y_i\\)) or that \\(\\bX_i\\) and \\(\\bW_i\\) are orthogonal in the sense that \\(\\E[\\bX_i\\bW_i']=0\\).",
    "crumbs": [
      "Linear Regression II",
      "Exercises: Vector Linear Model and Asymptotics"
    ]
  },
  {
    "objectID": "exercises/exercises-linear-asymptotic.html#applied-exercises",
    "href": "exercises/exercises-linear-asymptotic.html#applied-exercises",
    "title": "Exercises: Vector Linear Model and Asymptotics",
    "section": "Applied Exercises",
    "text": "Applied Exercises\nApplied exercises in this list of exercises serve as reminders on how to apply multivariate regression:\n\nWooldridge (2020) Exercise C9 in chapter 3 (see C7 in chapter 2 for some more context).\nJames et al. (2023) Exercise 3.8 and 3.9.\n\nCheck out chapter 3 in Heiss and Brunner (2024) and section 3.6 in James et al. (2023).",
    "crumbs": [
      "Linear Regression II",
      "Exercises: Vector Linear Model and Asymptotics"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Econometrics (Econometrics II)",
    "section": "",
    "text": "Advanced Econometrics builds on the basic econometrics course in three directions. First, the course discusses a range of practical methods used in economics and business. Second, it addresses both causal inference and forecasting. Third, it offers a deeper discussion of the underlying theory.\n\n\nInstructor: Vladislav Morozov\nEmail: morozov [at] uni-bonn.de\nOffice Location: Adenauerallee 24-42, IFS, Statistics Section\nOffice Hours: Virtual, by appointment\nCourse Website: eCampus and this website\nLectures: 8:30-10:00; Wednesdays (Room 0.042), Fridays (Lecture Hall N)\nSchedule Changes and Holidays: See eCampus and BASIS\nDOI: \nLevel: Undergraduate\nPrerequisites: basic courses in statistics and econometrics\n\n\n    View slides in full screen\n       \n      \n    \n  \n\n\n\n\n\n\nThe course is structured as follows (subject to change):\n\nA deeper look at linear regression:\n\nA vector-matrix form approach to linear regression.\nBasics of identification analysis.\nAsymptotic theory for the OLS estimator.\n\nAsymptotic inference:\n\nRefresher: key definitions and intuition of hypothesis testing.\nTests for linear hypotheses: \\(t\\)- and Wald tests.\nThe delta method and nonlinear Wald tests.\n\nPanel data in causal settings:\n\nEvent studies.\nDifferences-in-differences.\nTwo-way fixed effect approaches with multivalued treatment.\nMean group estimation.\n\nIntroduction to forecasting:\n\nCausal inference vs. forecasting I.\nNotions of forecast optimality.\nForecasting in cross-sections.\n\nParametric nonlinear models:\n\nBeyond linearity: nonlinear regression and nonlinear least squares.\nDiscrete outcomes in causal settings.\nElements of asymptotic theory for nonlinear models.\nClassification as forecasting with discrete outcomes.\n\n\nIf time allows, we will further discuss:\n\nGeneralized method of moments.\n\nLinear generalized method of moments (GMM).\nIV estimation of dynamic panel data models.\nFundamentals of nonlinear GMM.\n\nTime series:\n\nTime series as probabilistic objects and their properties.\nUnivariate models: ARIMA(X).\nMultivariate time series: VARIMA(X).\nElements of causal inference with time series.\nForecasting with time series vs. forecasting with panel data\n\n\nEven further topics such as quantile regression, experimentation under interference, and high-dimensional data may be introduced as time allows.\n\n\n\n\n\nTextbooks: the course draws on several textbooks:\n\nBrockwell, P. J., & Davis, R. A. (2016). Introduction to Time Series and Forecasting. Springer International Publishing.\nCunningham, S. (2021). Causal Inference: The Mixtape. Yale University Press.\nHuntington-Klein, N. (2025). The Effect: An Introduction to Research Design and Causality. Chapman and Hall/CRC.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. E. (2023). An Introduction to Statistical Learning: With Applications in Python. Springer.\nWooldridge, J. M. (2020). Introductory Econometrics: A Modern Approach (Seventh edition). Cengage.\n\nAll books minus Wooldridge are available online (openly or through our university network). The Wooldridge book is available in our library. For specific chapters, please refer to each specific set of slides.\n\n\n\n\n\n\nThe final grade for this course is based on a 90 minute closed-book written exam. The date of the exam will be announced separately by the Examination Office.\n\n\n\nAttendance and Participation:\nRegular attendance and active participation are strongly encouraged.\nAcademic Integrity:\nStudents must adhere to the university’s policies on academic integrity and plagiarism. Any violations will be subject to disciplinary action.\nAccommodations:\nIf you require any accommodations due to a disability or other circumstances, please contact the relevant office as soon as possible."
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "Advanced Econometrics (Econometrics II)",
    "section": "",
    "text": "Instructor: Vladislav Morozov\nEmail: morozov [at] uni-bonn.de\nOffice Location: Adenauerallee 24-42, IFS, Statistics Section\nOffice Hours: Virtual, by appointment\nCourse Website: eCampus and this website\nLectures: 8:30-10:00; Wednesdays (Room 0.042), Fridays (Lecture Hall N)\nSchedule Changes and Holidays: See eCampus and BASIS\nDOI: \nLevel: Undergraduate\nPrerequisites: basic courses in statistics and econometrics\n\n\n    View slides in full screen"
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Advanced Econometrics (Econometrics II)",
    "section": "",
    "text": "The course is structured as follows (subject to change):\n\nA deeper look at linear regression:\n\nA vector-matrix form approach to linear regression.\nBasics of identification analysis.\nAsymptotic theory for the OLS estimator.\n\nAsymptotic inference:\n\nRefresher: key definitions and intuition of hypothesis testing.\nTests for linear hypotheses: \\(t\\)- and Wald tests.\nThe delta method and nonlinear Wald tests.\n\nPanel data in causal settings:\n\nEvent studies.\nDifferences-in-differences.\nTwo-way fixed effect approaches with multivalued treatment.\nMean group estimation.\n\nIntroduction to forecasting:\n\nCausal inference vs. forecasting I.\nNotions of forecast optimality.\nForecasting in cross-sections.\n\nParametric nonlinear models:\n\nBeyond linearity: nonlinear regression and nonlinear least squares.\nDiscrete outcomes in causal settings.\nElements of asymptotic theory for nonlinear models.\nClassification as forecasting with discrete outcomes.\n\n\nIf time allows, we will further discuss:\n\nGeneralized method of moments.\n\nLinear generalized method of moments (GMM).\nIV estimation of dynamic panel data models.\nFundamentals of nonlinear GMM.\n\nTime series:\n\nTime series as probabilistic objects and their properties.\nUnivariate models: ARIMA(X).\nMultivariate time series: VARIMA(X).\nElements of causal inference with time series.\nForecasting with time series vs. forecasting with panel data\n\n\nEven further topics such as quantile regression, experimentation under interference, and high-dimensional data may be introduced as time allows."
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Advanced Econometrics (Econometrics II)",
    "section": "",
    "text": "Textbooks: the course draws on several textbooks:\n\nBrockwell, P. J., & Davis, R. A. (2016). Introduction to Time Series and Forecasting. Springer International Publishing.\nCunningham, S. (2021). Causal Inference: The Mixtape. Yale University Press.\nHuntington-Klein, N. (2025). The Effect: An Introduction to Research Design and Causality. Chapman and Hall/CRC.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. E. (2023). An Introduction to Statistical Learning: With Applications in Python. Springer.\nWooldridge, J. M. (2020). Introductory Econometrics: A Modern Approach (Seventh edition). Cengage.\n\nAll books minus Wooldridge are available online (openly or through our university network). The Wooldridge book is available in our library. For specific chapters, please refer to each specific set of slides."
  },
  {
    "objectID": "index.html#course-policies",
    "href": "index.html#course-policies",
    "title": "Advanced Econometrics (Econometrics II)",
    "section": "",
    "text": "The final grade for this course is based on a 90 minute closed-book written exam. The date of the exam will be announced separately by the Examination Office.\n\n\n\nAttendance and Participation:\nRegular attendance and active participation are strongly encouraged.\nAcademic Integrity:\nStudents must adhere to the university’s policies on academic integrity and plagiarism. Any violations will be subject to disciplinary action.\nAccommodations:\nIf you require any accommodations due to a disability or other circumstances, please contact the relevant office as soon as possible."
  },
  {
    "objectID": "slides/panel/data-types.html#introduction",
    "href": "slides/panel/data-types.html#introduction",
    "title": "Data Types",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Panel Data",
      "Data Types"
    ]
  },
  {
    "objectID": "slides/panel/data-types.html#classifying-data",
    "href": "slides/panel/data-types.html#classifying-data",
    "title": "Data Types",
    "section": "Classifying Data",
    "text": "Classifying Data",
    "crumbs": [
      "Panel Data",
      "Data Types"
    ]
  },
  {
    "objectID": "slides/panel/data-types.html#types-of-data-by-observation",
    "href": "slides/panel/data-types.html#types-of-data-by-observation",
    "title": "Data Types",
    "section": "Types of Data by Observation",
    "text": "Types of Data by Observation",
    "crumbs": [
      "Panel Data",
      "Data Types"
    ]
  },
  {
    "objectID": "slides/panel/data-types.html#recap-and-conclusions",
    "href": "slides/panel/data-types.html#recap-and-conclusions",
    "title": "Data Types",
    "section": "Recap and Conclusions",
    "text": "Recap and Conclusions",
    "crumbs": [
      "Panel Data",
      "Data Types"
    ]
  },
  {
    "objectID": "slides/panel/event-studies.html#introduction",
    "href": "slides/panel/event-studies.html#introduction",
    "title": "Event Studies",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Panel Data",
      "Event Studies"
    ]
  },
  {
    "objectID": "slides/panel/event-studies.html#two-periods",
    "href": "slides/panel/event-studies.html#two-periods",
    "title": "Event Studies",
    "section": "Two Periods",
    "text": "Two Periods",
    "crumbs": [
      "Panel Data",
      "Event Studies"
    ]
  },
  {
    "objectID": "slides/panel/event-studies.html#multiple-periods",
    "href": "slides/panel/event-studies.html#multiple-periods",
    "title": "Event Studies",
    "section": "Multiple Periods",
    "text": "Multiple Periods",
    "crumbs": [
      "Panel Data",
      "Event Studies"
    ]
  },
  {
    "objectID": "slides/panel/event-studies.html#empirical-application",
    "href": "slides/panel/event-studies.html#empirical-application",
    "title": "Event Studies",
    "section": "Empirical Application",
    "text": "Empirical Application",
    "crumbs": [
      "Panel Data",
      "Event Studies"
    ]
  },
  {
    "objectID": "slides/panel/event-studies.html#recap-and-conclusions",
    "href": "slides/panel/event-studies.html#recap-and-conclusions",
    "title": "Event Studies",
    "section": "Recap and Conclusions",
    "text": "Recap and Conclusions",
    "crumbs": [
      "Panel Data",
      "Event Studies"
    ]
  },
  {
    "objectID": "slides/panel/mean-group.html#introduction",
    "href": "slides/panel/mean-group.html#introduction",
    "title": "Mean Group Estimation",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Panel Data",
      "Mean Group Estimation"
    ]
  },
  {
    "objectID": "slides/panel/mean-group.html#theory",
    "href": "slides/panel/mean-group.html#theory",
    "title": "Mean Group Estimation",
    "section": "Theory",
    "text": "Theory",
    "crumbs": [
      "Panel Data",
      "Mean Group Estimation"
    ]
  },
  {
    "objectID": "slides/panel/mean-group.html#empirical-illustration",
    "href": "slides/panel/mean-group.html#empirical-illustration",
    "title": "Mean Group Estimation",
    "section": "Empirical Illustration",
    "text": "Empirical Illustration",
    "crumbs": [
      "Panel Data",
      "Mean Group Estimation"
    ]
  },
  {
    "objectID": "slides/panel/mean-group.html#recap-and-conclusions",
    "href": "slides/panel/mean-group.html#recap-and-conclusions",
    "title": "Mean Group Estimation",
    "section": "Recap and Conclusions",
    "text": "Recap and Conclusions",
    "crumbs": [
      "Panel Data",
      "Mean Group Estimation"
    ]
  },
  {
    "objectID": "slides/vector/ols-consistency.html#introduction",
    "href": "slides/vector/ols-consistency.html#introduction",
    "title": "Consistency of the OLS Estimator",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Linear Regression II",
      "Consistency of the OLS Estimator"
    ]
  },
  {
    "objectID": "slides/vector/ols-consistency.html#probability-background",
    "href": "slides/vector/ols-consistency.html#probability-background",
    "title": "Consistency of the OLS Estimator",
    "section": "Probability Background",
    "text": "Probability Background",
    "crumbs": [
      "Linear Regression II",
      "Consistency of the OLS Estimator"
    ]
  },
  {
    "objectID": "slides/vector/ols-consistency.html#consistency-of-the-ols-estimator",
    "href": "slides/vector/ols-consistency.html#consistency-of-the-ols-estimator",
    "title": "Consistency of the OLS Estimator",
    "section": "Consistency of the OLS Estimator",
    "text": "Consistency of the OLS Estimator",
    "crumbs": [
      "Linear Regression II",
      "Consistency of the OLS Estimator"
    ]
  },
  {
    "objectID": "slides/vector/ols-consistency.html#recap-and-conclusions",
    "href": "slides/vector/ols-consistency.html#recap-and-conclusions",
    "title": "Consistency of the OLS Estimator",
    "section": "Recap and Conclusions",
    "text": "Recap and Conclusions",
    "crumbs": [
      "Linear Regression II",
      "Consistency of the OLS Estimator"
    ]
  },
  {
    "objectID": "slides/vector/ols-inference.html#introduction",
    "href": "slides/vector/ols-inference.html#introduction",
    "title": "Inference I: Linear Hypotheses",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Asymptotic Inference",
      "Inference I: Linear Hypotheses"
    ]
  },
  {
    "objectID": "slides/vector/ols-inference.html#background-and-definitions-for-testing",
    "href": "slides/vector/ols-inference.html#background-and-definitions-for-testing",
    "title": "Inference I: Linear Hypotheses",
    "section": "Background and Definitions for Testing",
    "text": "Background and Definitions for Testing",
    "crumbs": [
      "Asymptotic Inference",
      "Inference I: Linear Hypotheses"
    ]
  },
  {
    "objectID": "slides/vector/ols-inference.html#one-linear-hypothesis",
    "href": "slides/vector/ols-inference.html#one-linear-hypothesis",
    "title": "Inference I: Linear Hypotheses",
    "section": "One Linear Hypothesis",
    "text": "One Linear Hypothesis",
    "crumbs": [
      "Asymptotic Inference",
      "Inference I: Linear Hypotheses"
    ]
  },
  {
    "objectID": "slides/vector/ols-inference.html#general-linear-hypotheses",
    "href": "slides/vector/ols-inference.html#general-linear-hypotheses",
    "title": "Inference I: Linear Hypotheses",
    "section": "General Linear Hypotheses",
    "text": "General Linear Hypotheses",
    "crumbs": [
      "Asymptotic Inference",
      "Inference I: Linear Hypotheses"
    ]
  },
  {
    "objectID": "slides/vector/ols-inference.html#confidence-intervals-and-sets",
    "href": "slides/vector/ols-inference.html#confidence-intervals-and-sets",
    "title": "Inference I: Linear Hypotheses",
    "section": "Confidence Intervals and Sets",
    "text": "Confidence Intervals and Sets",
    "crumbs": [
      "Asymptotic Inference",
      "Inference I: Linear Hypotheses"
    ]
  },
  {
    "objectID": "slides/vector/ols-inference.html#recap-and-conclusions",
    "href": "slides/vector/ols-inference.html#recap-and-conclusions",
    "title": "Inference I: Linear Hypotheses",
    "section": "Recap and Conclusions",
    "text": "Recap and Conclusions",
    "crumbs": [
      "Asymptotic Inference",
      "Inference I: Linear Hypotheses"
    ]
  },
  {
    "objectID": "slides/vector/vector-ols.html#introduction",
    "href": "slides/vector/vector-ols.html#introduction",
    "title": "Linear Regression in Vector-Matrix Form",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Linear Regression II",
      "Linear Regression in Vector-Matrix Form"
    ]
  },
  {
    "objectID": "slides/vector/vector-ols.html#motivation",
    "href": "slides/vector/vector-ols.html#motivation",
    "title": "Linear Regression in Vector-Matrix Form",
    "section": "Motivation",
    "text": "Motivation",
    "crumbs": [
      "Linear Regression II",
      "Linear Regression in Vector-Matrix Form"
    ]
  },
  {
    "objectID": "slides/vector/vector-ols.html#vector-and-matrix-forms-of-regression",
    "href": "slides/vector/vector-ols.html#vector-and-matrix-forms-of-regression",
    "title": "Linear Regression in Vector-Matrix Form",
    "section": "Vector and Matrix Forms of Regression",
    "text": "Vector and Matrix Forms of Regression",
    "crumbs": [
      "Linear Regression II",
      "Linear Regression in Vector-Matrix Form"
    ]
  },
  {
    "objectID": "slides/vector/vector-ols.html#the-ols-estimator",
    "href": "slides/vector/vector-ols.html#the-ols-estimator",
    "title": "Linear Regression in Vector-Matrix Form",
    "section": "The OLS Estimator",
    "text": "The OLS Estimator",
    "crumbs": [
      "Linear Regression II",
      "Linear Regression in Vector-Matrix Form"
    ]
  },
  {
    "objectID": "slides/vector/vector-ols.html#recap-and-conclusions",
    "href": "slides/vector/vector-ols.html#recap-and-conclusions",
    "title": "Linear Regression in Vector-Matrix Form",
    "section": "Recap and Conclusions",
    "text": "Recap and Conclusions",
    "crumbs": [
      "Linear Regression II",
      "Linear Regression in Vector-Matrix Form"
    ]
  }
]