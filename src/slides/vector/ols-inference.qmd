---
title: "Inference I: Linear Hypotheses"
subtitle: "Testing Linear Hypotheses. Confidence Intervals"
author: Vladislav Morozov   
format:
  revealjs:
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "A Deeper Look at Linear Regression: Inference"
    footer-logo-link: "https://vladislav-morozov.github.io/econometrics-2/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#045D5D"
    data-footer: " "
filters:
  - reveal-header  
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
---



## Introduction {background="#00100F"}
  
### Lecture Info {background="#43464B" visibility="uncounted"}


#### Learning Outcomes

This lecture is about  

<br>

By the end, you should be able to

- Do

#### References
 

::: {.nonincremental}

 

- 8-2 and E4+E4a in @Wooldridge2020IntroductoryEconometricsModern (careful with the specialized formulas in 8-2, they are more confusing than the general case in the lecture and those in E4)
- Or 7.11-7.13, 7.16, 9.1-9.9 in @Hansen2022Econometrics
- (*Curious background reading*): @Wooldridge2023WhatStandardError on what "standard error" might mean


::: 

 

### Reminder on the Empirical Example {background="#43464B" visibility="uncounted"}


#### Reminder: Empirical Model 

Studying link between wages and (education, experience)
$$
\begin{aligned}[]
& [\ln(\text{wage}_i)]^{\text{(education, experience)}} \\
&  =   \beta_1 + \beta_2 \times \text{education} \\
& \quad  + \beta_3 \times  \text{experience} + \beta_4 \times  \dfrac{\text{experience}^2}{100} + U_i
\end{aligned}
$$ {#eq-vector-inference-emp-model}

. . . 
 
Data: married white women from March 2009 CPS


```{python}
#| echo: true 
#| code-fold: true
#| code-summary: "Expand for full data preparation code"
import numpy as np
import pandas as pd
import statsmodels.api as sm

from statsmodels.regression.linear_model import OLS

# Read in the data
data_path = ("https://github.com/pegeorge/Econ521_Datasets/"
             "raw/refs/heads/main/cps09mar.csv")
cps_data = pd.read_csv(data_path)

# Generate variables
cps_data["experience"] = cps_data["age"] - cps_data["education"] - 6
cps_data["experience_sq_div"] = cps_data["experience"]**2/100
cps_data["wage"] = cps_data["earnings"]/(cps_data["week"]*cps_data["hours"] )
cps_data["log_wage"] = np.log(cps_data['wage'])

# Retain only married women white with present spouses
select_data = cps_data.loc[
    (cps_data["marital"] <= 2) & (cps_data["race"] == 1) & (cps_data["female"] == 1), :
]

# Construct X and y for regression 
exog = select_data.loc[:, ['education', 'experience', 'experience_sq_div']]
exog = sm.add_constant(exog)
endog = select_data.loc[:, "log_wage"]
```

::: footer

:::




#### Reminder: Estimation Results 

```{.python code-line-numbers="0-1"}
results = OLS(endog, exog).fit(cov_type='HC0') # Robust covariance matrix estimator
print(results.summary())
```

```{python} 
results = OLS(endog, exog).fit(cov_type='HC0')
print(results.summary())
```

#### Reminder: Parameters of Interest and Estimators

 
<br>

Our parameters of interest: 

1. $100\beta_2$. Estimate: $11.14$
2. $100\beta_3 + 20 \beta_4$. Estimate: $1.59$
3. $-50\beta_3/\beta_4$. Estimate: $36.67$


. . . 


<br>

<div class="rounded-box">

What is the interpretation of those parameters?

</div>



#### Reminder: Empirical Questions


<br> 

1. Does education matter at all? (up to our statistical confidence)
2. Does experience matter at all? (up to our statistical confidence)
3. Is the best amount of experience to have equal to 25 years? (up to our statistical confidence)
4. How certain are we of our estimates of target parameters?


## Background and Definitions for Testing {background="#00100F"}
   

#### Basic Setup: Hypotheses

Suppose that we have a model with some parameters $\theta$ (of whatever nature)

Two competing *hypotheses* (statements about parameters $\theta$)
$$
H_0: \theta\in \Theta_0 \text{  vs.  } H_1: \theta \in \Theta_1 
$$
for some non-intersecting $\Theta_0$ and $\Theta_1$

. . .

Example

- $H_0: \beta_2=0$ (education does not affect wages)
- $H_1: \beta_2\neq 0$ (education affects wages)
 

#### Definition of a Test

A test is a *decision rule*: you see the sample and then you decide in favor of $H_0$ or $H_1$


. . . 

<br>

Formally:

<div class="rounded-box">

::: {#def-vector-inference-test}

A test $T$ is a function of the sample $(X_1, \dots, X_N)$ to the space $\curl{\text{Reject} H_0,  \text{Do not reject }H_0}$

:::

</div>




#### Power

<br>

<div class="rounded-box">

::: {#def-vector-inference-power}

The power function $\text{Power}_T(\theta)$ of the test $T$ is the probability that $T$ rejects if $\theta$ is the true parameter value:
$$
\text{Power}_T(\theta) = P(T(X_1, \dots, X_N)=\text{Reject }H_0|\theta)
$$

:::

</div>

 

#### Test Size

Maximal power under the null has a special name 
<div class="rounded-box">

::: {#def-vector-inference-size}

The <span class="highlight"> size </span> $\alpha$ of the test $T$ is 
$$
\alpha = \max_{\theta\in\Theta_0} \text{Power}_T(\theta)
$$

:::

</div>

In other words, the probability of falsely rejecting the null (type I error)


#### What Defines a Good Test? 

The best possible test has perfect detection:

- Never rejects under $H_0$
- Always reject under $H_1$

. . . 

<br>

Usually impossible in practice. Instead we ask

- Not too much false rejection under $H_0$ (e.g. $\leq 5\%$ of the time)
- As much rejection as possible under $H_1$ 

#### Test Consistency

- In finite samples, usually cannot compute $\text{Power}_T(\theta)$
- Instead ask that you detect $H_1$ <span class="highlight"> asymptotically </span>  
 
. . .

<div class="rounded-box">

::: {#def-vector-inference-consistency}

$T$ is *consistent* if for any $\theta\in \Theta_1$ 
$$ \small
 \lim_{N\to\infty} P(T(X_1, \dots, X_N)=\text{Reject }H_0|\theta) = 1
$$

:::

</div>

::: {.callout-important appearance="minimal"}

As with estimators, we say "test" when we mean a sequence of tests, one for each sample size.

:::

::: footer

:::


#### Asymptotic Size

- In finite samples, usually cannot control size exactly
- But can require it asymptotically


<div class="rounded-box">

::: {#def-vector-inference-asy-significane}

The asymptotic size $\alpha$ of the test $T$ is 
$$
\alpha = \lim_{N\to\infty} \max_{\theta\in\Theta_0}  P(T(X_1, \dots, X_N)=\text{Reject }H_0|\theta) 
$$

:::

</div>


## One Linear Hypothesis {background="#00100F"}

### Example and $t$-Statistic {background="#43464B" visibility="uncounted"}

#### Single Example Hypothesis

Let's start with our first empirical question:

<div class="rounded-box">

Does education affect wages?

</div>

. . .

<br>

In the framework of @eq-vector-inference-emp-model can translate to 
$$
H_0: \beta_2 = 0, \quad H_1: \beta_2\neq 0
$$
What are the $\Theta_0$ and $\Theta_1$ here if $\theta=\bbeta$?

<!-- Here $\Theta_1 = \curl{0}$ and $\Theta_2 = \R - \curl{0}$ -->



#### How Testing Works in General

*How do we construct a test/decision rule?*

<br>

. . .

The basic approach to testing is surprisingly simple

1. Pick a "statistic" (=some known function of the data) that behaves "differently" under $H_0$ and $H_1$
2. Is the observed value of the statistic compatible with $H_0$? 
   - No $\Rightarrow$ reject $H_0$ in favor or $H_1$
   - Yes $\Rightarrow$ do not reject $H_0$


#### Picking a Statistic

- In principle, can pick any statistic. Some are more "standard"
- For testing hypotheses about coefficients, there are three main classes:
  - Wald statistics: need only *unrestricted* estimates 
  - Lagrange multiplier (LM): need *restricted* estimates 
  - Likelihood ratio (LR): need both

. . .
 

::: {.callout-note appearance="minimal"}

Wald tests easiest to work with in linear models, but others have their uses in different contexts 
:::


#### Convergence of $\hat{\beta}_2$

Recall asymptotic distribution result for OLS estimator
$$\small
\sqrt{N}\left( \hat{\bbeta}- \bbeta \right) \xrightarrow{d} N(0, \avar(\hat{\bbeta}))
$$

. . . 

It implies (why?) that
$$ \small
\dfrac{\hat{\beta}_2 - \beta_2}{\sqrt{ \avar(\hat{\beta}_2)/N }  } \xrightarrow{d} N\left(0, 1\right)
$$
where $\avar(\hat{\beta}_2)$ is the (2, 2) element of $\avar(\hat{\bbeta})$


#### $t$-statistic

- Let $\widehat{\avar}(\hat{\bbeta})$ be a consistent estimator of $\avar(\hat{\bbeta})$
- Let $H_0: \beta_2 = 0$ be true

. . .

By Slutsky's theorem (why?) it holds that
$$ \small
t = \dfrac{\hat{\beta}_2}{\sqrt{ \widehat{\avar}(\hat{\beta}_2)/N }  } \xrightarrow{d} N\left(0, 1\right)
$$
$\sqrt{ \widehat{\avar}(\hat{\beta}_2)/N }$ â€” <span class="highlight">standard error</span> of $\hat{\beta}_2$

::: footer

:::

#### Decision Rule: Test  {#sec-ols-inference-t-test}

We call the following the <span class="highlight">asymptotic size $\alpha$ $t$-test</span>:


<br>

<div class="rounded-box">

Let $z_{1-\alpha/2} = \Phi^{-1}(1-\alpha/2)$. Then

- Reject $H_0$ is $\abs{t}>z_{1-\alpha/2}$
- Do not reject $H_0$ is $\abs{t}\leq z_{1-\alpha/2}$


</div>

### Illustration {background="#43464B" visibility="uncounted"}


#### Illustration: Extracting Standard Errors {.scrollable}

Can get $\widehat{\avar}(\hat{\bbeta})$ from the `results` object:
```{python}
#| echo: true
(results.nobs)*results.cov_params()
```
`cov_params()` extracts *standard errors*

::: footer

:::

#### Illustration: Doing the Test by Hand

Compute $t$-statistic as
```{python}
#| echo: true
t = (results.params.iloc[1])/np.sqrt(results.cov_params().iloc[1, 1])
print(t)
```

- Can compare `t` statistic to suitable quantile of the normal
- Set $\alpha=0.05$ if want to reject at most 5% under $H_0$ in the limit

. . .

```{python}
#| echo: true
from scipy.stats import norm
np.abs(t) > norm.ppf(1-0.05/2)
```
Reject $H_0$ in favor $H_1: \beta_2\neq 0$ at 5\% asymptotic level 

::: footer

:::

#### Illustration: Using `t_test()`

Can also use 
```{python}
#| echo: true
results.t_test(np.array([0, 1, 0, 0]), use_t=False)
```

. . .

- Reports <span class="highlight">asymptotic</span> $p$-values
- Decision by comparing with the  $p$-value
- Can reject with high confidence (very small $p$-value)

#### Illustration: $t$ Test From the Regression Results

Regression results also print out results for $t$-tests of hypotheses $H_0:\beta_k=0$
```{python} 
results = OLS(endog, exog).fit(cov_type='HC0')
print(results.summary())
```

### Properties {background="#43464B" visibility="uncounted"}


#### $t$-Test under $H_0$: Size

What is the (asymptotic) probability of rejecting under $H_0$?

. . .

$$
\begin{aligned}
& P\left(\text{Reject} H_0|H_0 \right) = P\left(\abs{t}>z_{1-\alpha/2} |H_0\right) \\
& = P\left( \abs{ \dfrac{\hat{\beta}_2}{\sqrt{ \widehat{\avar}(\hat{\beta}_2)/N }  }}> z_{1-\alpha/2}\Bigg|H_0 \right)\\
& \to \Phi(z_{\alpha/2}) + (1- \Phi(z_{1-\alpha/2})) = \alpha 
\end{aligned}
$$

. . .

The test has asymptotic size $\alpha$

#### $t$-Test under $H_1$: Consistency


What happens to $t$ under $H_1$? Suppose that $\beta_2\neq 0$ is the true value. Can write
$$ \small 
t = \dfrac{\hat{\beta}_2}{\sqrt{ \widehat{\avar}(\hat{\beta}_2)/N }  }=  \underbrace{\dfrac{\hat{\beta}_2 - \beta_2}{\sqrt{ \widehat{\avar}(\hat{\beta}_2)/N }  }}_{\scriptsize \xrightarrow{d} N(0, 1)} +  \underbrace{\dfrac{\beta_2}{\sqrt{ \widehat{\avar}(\hat{\beta}_2)/N }  }}_{\scriptsize \xrightarrow{p} \pm \infty }
$$


. . .

It follows (why?) that the $t$-test is consistent
$$ \small
P(\text{Reject } H_0|H_1) \xrightarrow{N\to\infty} 1
$$ 



#### $t$-Test for $H_0:\beta_2 = c$

More generally, can test
$$
H_0: \beta_k = c \text{ vs } H_1: \beta_k \neq c
$$

. . . 

$t$-statistic
$$
t = \dfrac{\hat{\beta}_2 - c}{\sqrt{ \widehat{\avar}(\hat{\beta}_k)/N }  }
$$ {#eq-vector-inference-t-general}

Same decision rule: compare $t$ to $z_{1-\alpha/2}$

#### Intuition: What the Test Does

- Under $H_0$, the $t$ statistic should be "well-behaved": normal and centered at 0. Big values of $t$-statistic unlikely
- So if we see a big $t$-statistic, such a value is unlikely under $H_0$ â€” evidence against $H_0$
- If value is large enough, we think the evidence is strong enough to be reasonably incompatible with $H_0$ â€” rejection


#### Combined Result: $t$-statistics


<div class="rounded-box">

::: {#prp-vector-inference-t}

Let the assumptions for asymptotic normality of the OLS estimator hold. Let $t$ be defined as in @eq-vector-inference-t-general. Then

1. If $H_0: \beta_k=c$ holds, then $t\xrightarrow{d} N(0, 1)$ The associated test has asymptotic size $\alpha$
2. If $H_0: \beta_k=c$ does not hold, then $t\xrightarrow{p}\pm\infty$. The associated test is consistent



:::

</div>


#### Estimating $\avar(\hat{\bbeta})$

One remaining issue: how to estimate 
$$
\avar(\hat{\bbeta}) =  \left( \E[\bX_i\bX_i']\right)^{-1} \E[U_i^2\bX_i\bX_i']\left( \E[\bX_i\bX_i']\right)^{-1} 
$$

. . . 


Can estimate using <span class="highlight">sample analogs</span>

1. $\left( \E[\bX_i\bX_i']\right)^{-1}$ with $\left( N^{-1}\sum_{i=1}^N  \bX_i\bX_i'\right)^{-1}$
2. $\E[U_i^2\bX_i\bX_i']$ with $N^{-1}\sum_{i=1}^N \hat{U}_i \bX_i\bX_i'$ for $\hat{U}_i = Y_i-\bX_i'\hat{\bbeta}$


#### Estimating $\avar(\hat{\bbeta})$: Robust Standard Errors

Resulting $\widehat{\avar}(\hat{\bbeta})$ is consistent:
$$
\widehat{\avar}(\hat{\bbeta}) \xrightarrow{p} {\avar}(\hat{\bbeta})
$$

- Catn use in test statistics
- $N^{-1}\widehat{\avar}(\hat{\bbeta})$ is called  *robust* (or *heteroskedasticity robust*) standard errors â€” specifically,  <span class="highlight">HC0</span>
- In `statsmodels`, we used them by called `OLS(endog, exog).fit(cov_type='HC0')`





## General Linear Hypotheses {background="#00100F"}



### Example and Wald Statistic {background="#43464B" visibility="uncounted"}

#### Motivation: Need Tests for Multiple Restrictions

- $t$-tests allowed us to test if education mattered ($\beta_2=0$)
- But next question is whether experience affects wages: 
$$
H_0: \begin{cases}
\beta_3 =0\\
\beta_4 = 0
\end{cases} \quad 
\text{ vs }  \quad H_1: \beta_3 \neq 0\text{ or } \beta_4 \neq 0
$$
-  $H_0$ has two constraints as the same time! 

. . .

::: {.callout-note appearance="minimal"}

The "or" in $H_1$ allows the possibility that <span class="highlight">both</span> $\beta_3\neq 0$ and $\beta_4\neq 0$


:::

#### Matrix Representation of the Null

Can write our $H_0$ as
$$
\bR\bbeta = \bq
$$
for 
$$
\bR = \begin{pmatrix}
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}, \quad \bq =\begin{pmatrix}
0 \\
0 
\end{pmatrix}
$$

 


#### Linear Hypotheses
 

More generally, can consider <span class="highlight">linear hypotheses</span> of the form
$$
\bR\bbeta = \bq
$$

Here 

- $\bbeta$ is $p\times 1$
- $\bR$ is $k\times p$ with maximum rank â€” $k$ constraints
- $k\geq 1$ and $k\leq p$ 


. . . 

::: {.callout-note appearance="minimal"}

Covers both example $H_0$ we have seen so far

:::


#### Towards a Statistic

Intuitive way to construct a statistic

<div class="rounded-box"> 
Check the distance between $\bR\hat{\bbeta}$ and $\bq$

</div>

. . .

- Reject if distance is large, do not reject if not
- How to pick "large" to ensure correct test size? 
- $\Rightarrow$ need to combine asymptotic distribution of $\bR\hat{\bbeta}$ and distance

#### Asymptotic Distribution of $\bR\hat{\bbeta}$

Recall that 
$$
\sqrt{N}(\hat{\bbeta}-\bbeta)\xrightarrow{d} N(0, \avar(\bbeta))
$$

. . .

By the CMT
$$
\sqrt{N}(\bR\hat{\bbeta}-\bR\bbeta) \xrightarrow{d} N(0, \bR\avar(\bbeta)\bR')
$$


#### $\chi^2$ Random Variables

A special useful distribution

<div class="rounded-box"> 

::: {#eq-vector-inference-chi2}

Let $Z_1, \dots, Z_k$ be independent $N(0, 1)$ variables. The distribution of $\sum_{j=1}^k Z_j^2$ is called the chi-squared distribution with $k$ degrees of freedom (written $\chi^2_k$)

:::


</div>


. . . 

- If $\bZ=(Z_1, \dots, Z_k)$, then $\bZ\sim N(0, \bI_k)$ and $\norm{\bZ}^2 = \bZ'\bZ \sim \chi^2_k$
- Has two ingredients we need: normality and distance


#### Wald Statistic


<br> 


The following statistic is called a <span class="highlight">Wald</span> statistic: 
$$
W = N\left(  \bR\hat{\bbeta}-\bq   \right)'\left(\bR\widehat{\avar}(\bbeta)\bR'\right)^{-1}\left(  \bR\hat{\bbeta}-\bq   \right)
$$ {#eq-vector-inference-wald}
 
- Interpretation: weighted distance
- Weighted by the inverse of the variance-covariance matrix

#### Decision Rule: Wald Test

We call the following the <span class="highlight">asymptotic size $\alpha$ Wald-test</span>:


<br>

<div class="rounded-box">

Let $c_{1-\alpha}$ solve $P(\chi^2\leq c_{1-\alpha})=1-\alpha$. Then

- Reject $H_0$ is $W>c_{1-\alpha}$
- Do not reject $H_0$ is $W\leq c_{1-\alpha}$


</div>

#### Plot: PDF of $\chi^2_k$ and Rejection Region (Shaded)

```{python}
import matplotlib.pyplot as plt
from scipy.stats import chi2
BG_COLOR = "whitesmoke"

# Parameters for the chi-squared distribution
df = 5

# Generate the x values for the PDF
x_vals = np.linspace(0, 20, 1000)

# Compute the PDF values
pdf_vals = chi2.pdf(x_vals, df)

# Compute the 95th quantile
quantile_95 = chi2.ppf(0.95, df)

# Set up the figure and the axes
fig, ax = plt.subplots(figsize=(15, 6))
fig.patch.set_facecolor(BG_COLOR)
fig.patch.set_edgecolor("teal")
fig.patch.set_linewidth(5)

# Plot the PDF
ax.plot(x_vals, pdf_vals, label=f'PDF of $\\chi^2_k$ distribution', color='teal')

# Fill the area under the PDF to the right of the 95th quantile
ax.fill_between(x_vals, pdf_vals, where=(x_vals >= quantile_95), color='darkorange', alpha=0.3)

# Mark the 95th quantile
ax.axvline(quantile_95, color='red', linestyle='--', label="$c_{1-\\alpha}$")

# Set the labels and title 
ax.set_ylabel('Density') 
ax.legend(loc='upper right')
ax.set_ylim(0, 0.17)
ax.set_xlim(0, 20)
ax.set_xticklabels([])
ax.set_yticklabels([])
ax.set_facecolor(BG_COLOR)

# Show the plot
plt.show()

```

#### Wald and $t$ Statistics
 
What is $k=1$ â€” only one constraint? 

. . .

<br> 

- Wald statistic is the square of the corresponding $t$-statistic (show this)
- You lose nothing by doing a Wald test even if you have a single hypothesis
 



### Illustration {background="#43464B" visibility="uncounted"}

#### Effect of Experience: Expressing $\bR$ and $\bq$

The results class in `statsmodels` has a `wald_test()` method 

. . . 
 
First need to define the $\bR$ and $\bq$ matrices: 
```{python}
#| echo: true
constraint_matrix = np.array( 
    [[0, 0, 1, 0],
     [0, 0, 0, 1]]
)

rhs_vector = np.array(
    [0, 0]
)

print(constraint_matrix, '\n', rhs_vector)
```

#### Effect of Experience: Using `wald_test()`

Then can supply $\bR$ and $\bq$ to `wald_test()` as a tuple:
```{python}
#| echo: true
wald_results = results.wald_test(
  (constraint_matrix, rhs_vector), 
  use_f=False, 
  scalar=True
)
print(wald_results)
```

. . .

<br>

- Strong evidence against $H_0: \beta_3=\beta_4=0$
- Experience matters for earnings


#### Second Example: Effect of Experience for 10 Years

Recall other parameter of interest: $100\beta_3 + 20\beta_4$. Can ask:
$$ \small
H_0: 100\beta_3 + 20\beta_4 =5 \text{ vs } H_0: 100\beta_3 + 20\beta_4 \neq 1.4
$$

. . . 
 

Can also do Wald test:
```{python}
#| echo: true
constraint_matrix = np.array( 
    [0, 0, 100, 20], 
)
rhs_vector = np.array(
    [1.4]
)
# Perform test
wald_results = results.wald_test((constraint_matrix, rhs_vector), use_f=False, scalar=True)
print(wald_results)
```

::: footer


:::

### Properties {background="#43464B" visibility="uncounted"}

 
#### Why Normalize by $( \bR\avar(\bbeta)\bR')^{-1}$

- Under $H_0: \R\bbeta=\bq$ it holds that 
$$
\sqrt{N}(\bR\hat{\bbeta}-\bq) \xrightarrow{d} N(0, \bR\avar(\bbeta)\bR')
$$
- $\norm{\bR\hat{\bbeta}-\bq}^2$ is <span class="highlight"> not </span> $\chi^2$ unless $\bR\avar(\bbeta)\bR'= \bI_k$


. . . 

<br>

$\Rightarrow$ In general need to something transform $\bR\hat{\bbeta}-\bq$ to get $\bI_k$ asymptotic variance for $\chi^2_k$ limit

#### Matrix Square Root

- Recall: if $W\sim N(0, \sigma^2)$, then $W/\sigma \sim N(0, 1)$
- Similar result can be used for vectors $\bW\sim N(0, \bSigma)$

. . . 

Just need the equivalent of standard deviation:
<div class="rounded-box"> 

::: {#prp-vector-inference-sqrt}

Let $\bSigma$ be a positive definite matrix. Then there is a unique positive definite matrix $\bSigma^{1/2}$ such that 
$$
\bSigma^{1/2} \bSigma^{1/2} =  \bSigma
$$
:::


</div>

::: footer

See [Wikipedia](https://en.wikipedia.org/wiki/Definite_matrix#Square_root) on matrix square roots

:::

#### Standardizing $\bR\hat{\bbeta}$

- So if $\bW\sim N(0, \bSigma)$ with full rank $\bSigma$
$$\small
(\bSigma^{1/2})^{-1}\bW \sim N(0, (\bSigma^{1/2})^{-1}\bSigma(\bSigma^{1/2})^{-1}) = N(0, \bI_k)
$$
- Or 
$$ \small
((\bSigma^{1/2})^{-1}\bW )'(\bSigma^{1/2})^{-1}\bW ) = \bW' \bSigma^{-1}\bW\sim \chi^2_k
$$

. . . 

We just take $\bW=\sqrt{N}(\hat{\bbeta}-\bbeta)$ and apply the argument asymptotically and with an estimator of $\avar(\hat{\bbeta})$

::: footer

:::

#### Wald Test under $H_0$: Size

Under the null $H_0: \bR\bbeta=\bq$

. . .

By Slutsky's theorem and the above it follows
$$
W = N\left(  \bR\hat{\bbeta}-\bq   \right)'\left(\bR\widehat{\avar}(\bbeta)\bR'\right)^{-1}\left(  \bR\hat{\bbeta}-\bq   \right) \xrightarrow{d} \chi^2_k
$$

. . .

<br>

Hence the test is asymptotically size $\alpha$
$$ \small
P(\text{Reject } H_0|H_0) \xrightarrow{N\to\infty} \alpha
$$ 


#### Wald Test under $H_1$: Consistency

Suppose that $H_1$ holds: $\bR\bbeta\neq \bq$. Then

- $\bR\hat{\bbeta}-\bq$ converges to something $\neq 0$
- $\left(\bR\widehat{\avar}(\bbeta)\bR'\right)^{-1}\xrightarrow{p} \left(\bR{\avar}(\bbeta)\bR'\right)^{-1}$ by the CMT
- $\left(\bR{\avar}(\bbeta)\bR'\right)^{-1}$ is positive definite (why?)
  

. . .

It follows (why?) that $W\to+\infty$ and  
$$ \small
P(\text{Reject } H_0|H_1) \xrightarrow{N\to\infty} 1
$$ 




#### Combined Result: Wald Statistic and Test


<div class="rounded-box">

::: {#prp-vector-inference-wald}

Let the assumptions for asymptotic normality of the OLS estimator hold. Let $W$ be defined as in @eq-vector-inference-wald. Then

1. If $H_0: \bR\bbeta=\bq$ holds, then $W\xrightarrow{d} \chi^2_k$ The associated test has asymptotic size $\alpha$
2. If $H_0: \bR\bbeta=\bq$ does not hold, then $W\xrightarrow{p}\pm +\infty$. The associated test is consistent



:::

</div>


<!-- ## Nonlinear Hypotheses and the Delta Method  {background="#00100F"}

### The Delta Method {background="#43464B" visibility="uncounted"}

####

### Nonlinear Wald Tests  {background="#43464B" visibility="uncounted"}

#### 

A -->

## Confidence Intervals and Sets {background="#00100F"}

#### Point vs. Interval Estimators


- Our $\hat{\bbeta}$ is a <span class="highlight">point</span> estimator for $\bbeta$ â€” returns a single value in $\R^p$ for each sample
- Can also consider <span class="highlight">set estimator</span> â€” returns a whole set of values in $\R^q$ as a collection of guesses for $\bbeta$

<br>

. . .

- Anything can be a set estimator, but we want "sensible" ones
- Leading example: confidence intervals/sets

#### Confidence Sets: Definition


<div class="rounded-box">

::: {#def-vector-inference-conf-set}

1. A <span class="highlight"> size $\alpha$ confidence set for $\theta$ </span> ($\theta\in\R^p$) is a random set $S(X_1, \dots, X_N)\subseteq \R^p$  
$$ \scriptsize
P(\theta \in S(X_1, \dots, X_N)) = 1-\alpha
$$

2. $S(\cdot, \cdots)$ is an <span class="highlight">asymptotic size $\alpha$ confidence set for $\theta$ </span>  if $\lim_{N\to\infty} P(\theta \in S(X_1, \dots, X_N)) = 1-\alpha$

3. $P(\theta \in S(X_1, \dots, X_N))$ is the <span class="highlight">coverage</span> of $S$

:::


</div>


::: footer

:::

#### Example: Confidence Intervals for $\beta_k$

- Can construct confidence sets based on asymptotic distribution of $\hat{\bbeta}$
- Example: try to construct a symmetric interval based on $\hat{\beta}_k$ (since limit distribution is symmetric around $\beta_k$)

. . . 
 

Such an interval takes form
$$
[\hat{\beta}_k - \hat{c}, \hat{\beta}_k + \hat{c}]
$$
Here $\hat{c}_N\geq 0$ can depend on the sample and  $N$

#### Picking $\hat{c}$

$$\scriptsize
\begin{aligned}
& P\left(\beta_k\in[\hat{\beta}_k - \hat{c}_N, \hat{\beta}_k + \hat{c}]   \right)\\
&   = P\left( - \frac{\hat{c}_N}{\sqrt{\widehat{\avar}(\hat{\beta}_k/N}}\leq \sqrt{N} \frac{\hat{\beta}_k - \beta_k}{\widehat{\avar}(\hat{\beta}_k }\leq   \frac{\hat{c}_N}{\sqrt{\widehat{\avar}(\hat{\beta}_k/N}}  \right)\\
& \approx \Phi\left(       \frac{\hat{c}_N}{\sqrt{\widehat{\avar}(\hat{\beta}_k/N}}\right) - \Phi\left(  -\dfrac{\hat{c}_N}{\sqrt{\widehat{\avar}(\hat{\beta}_k/N}} \right)
\end{aligned}
$$

. . . 

If we want $(1-\alpha)\times 100\%$ asymptotic coverage, pick $\hat{c}_N = z_{1-\alpha/2} \sqrt{ \frac{\widehat{\avar}(\hat{\beta}_k)}{N} }$

#### Result and Interpretation

<div class="rounded-box">

::: {#prp-vector-inference-ci}

The confidence interval
$$ \small \hspace{-1.6cm}
S = \left[  \hat{\beta}_k -  z_{1-\alpha/2} \sqrt{ \frac{\widehat{\avar}(\hat{\beta}_k)}{N} }, \hat{\beta}_k+  z_{1-\alpha/2} \sqrt{ \frac{\widehat{\avar}(\hat{\beta}_k)}{N} }  \right]
$$ {#eq-vector-inference-basic-ci}
has asymptotic coverage $(1-\alpha)\times 100\%$

:::

</div>

. . . 

What is the interpretation of this interval?


#### Connection to $t$-Tests: Test Inversion

There is an equivalent way to construct the confidence interval ([-@eq-vector-inference-basic-ci])

- Recall $t$-statistic ([-@eq-vector-inference-t-general]) for $H_0: \beta_k = c$
- $S$ is the set of all $c$ for which the test does not reject

. . . 

<br>

An example of <span class="highlight">test inversion</span> and equivalence between testing and confidence intervals

#### Multivariate Confidence Sets

- Can also construct <span class="highlight">joint</span>  confidence sets for multiple parts of $\bbeta$
- Example approach: inverting the Wald test for $H_0: \bbeta = \bc$
- A bit more advanced â€” can read in 7.18 in @Hansen2022Econometrics

. . . 

<br>

Theree are other ways of constructing confidence sets 

## Recap and Conclusions {background="#00100F"}
  
#### Recap

In this lecture we

1. Did
   
#### Next Questions

<br>

How 

#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::