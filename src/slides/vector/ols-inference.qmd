---
title: "Inference and the Delta Method"
subtitle: "Hypothesis Testing and Confidence Intervals"
author: Vladislav Morozov  
execution:
  eval: false
format:
  revealjs:
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "A Deeper Look at Linear Regression: Inference"
    footer-logo-link: "https://vladislav-morozov.github.io/econometrics-2/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#045D5D"
    data-footer: " "
filters:
  - reveal-header  
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
---



## Introduction {background="#00100F"}
  
### Lecture Info {background="#43464B" visibility="uncounted"}


#### Learning Outcomes

This lecture is about  

<br>

By the end, you should be able to

- Do

#### References
 

::: {.nonincremental}

 

- 8-2 and E4+E4a in @Wooldridge2020IntroductoryEconometricsModern (careful with the specialized formulas in 8-2, they are more confusing than the general case in the lecture and those in E4)
- Or 7.11-7.13, 7.16, 7.18* in @Hansen2022Econometrics
- (*Curious background reading*): @Wooldridge2023WhatStandardError on what "standard error" might mean


::: 

 

### Reminder on the Empirical Example {background="#43464B" visibility="uncounted"}


#### Reminder: Empirical Model 

Studying link between wages and (education, experience)
$$
\begin{aligned}[]
& [\ln(\text{wage}_i)]^{\text{(education, experience)}} \\
&  =   \beta_1 + \beta_2 \times \text{education} \\
& \quad  + \beta_3 \times  \text{experience} + \beta_4 \times  \dfrac{\text{experience}^2}{100} + U_i
\end{aligned}
$$ {#eq-vector-inference-emp-model}

. . . 
 
Data: married white women from March 2009 CPS


```{python}
#| echo: true 
#| code-fold: true
#| code-summary: "Expand for full data preparation code"
import numpy as np
import pandas as pd
import statsmodels.api as sm

from statsmodels.regression.linear_model import OLS

# Read in the data
data_path = ("https://github.com/pegeorge/Econ521_Datasets/"
             "raw/refs/heads/main/cps09mar.csv")
cps_data = pd.read_csv(data_path)

# Generate variables
cps_data["experience"] = cps_data["age"] - cps_data["education"] - 6
cps_data["experience_sq_div"] = cps_data["experience"]**2/100
cps_data["wage"] = cps_data["earnings"]/(cps_data["week"]*cps_data["hours"] )
cps_data["log_wage"] = np.log(cps_data['wage'])

# Retain only married women white with present spouses
select_data = cps_data.loc[
    (cps_data["marital"] <= 2) & (cps_data["race"] == 1) & (cps_data["female"] == 1), :
]

# Construct X and y for regression 
exog = select_data.loc[:, ['education', 'experience', 'experience_sq_div']]
exog = sm.add_constant(exog)
endog = select_data.loc[:, "log_wage"]
```

::: footer

:::




#### Reminder: Estimation Results 

```{.python code-line-numbers="0-1"}
results = OLS(endog, exog).fit(cov_type='HC0') # Robust covariance matrix estimator
print(results.summary())
```

```{python} 
results = OLS(endog, exog).fit(cov_type='HC0')
print(results.summary())
```

#### Reminder: Parameters of Interest and Estimators

 
<br>

Our parameters of interest: 

1. $100\beta_2$. Estimate: $11.14$
2. $100\beta_3 + 20 \beta_4$. Estimate: $2.22$
3. $-50\beta_3/\beta_4$. Estimate: $36.67$


. . . 


<br>

<div class="rounded-box">

What is the interpretation of those parameters?

</div>



#### Reminder: Empirical Questions


<br> 

1. Does education matter at all? (up to our statistical confidence)
2. Does experience matter at all? (up to our statistical confidence)
3. Is the best amount of experience to have equal to 25 years? (up to our statistical confidence)
4. How certain are we of our estimates of target parameters?


## Background and Definitions for Testing {background="#00100F"}
   

#### Basic Setup: Hypotheses

Suppose that we have a model with some parameters $\theta$ (of whatever nature):

Two competing *hypotheses* (statements about parameters $\theta$)
$$
H_0: \theta\in \Theta_0 \text{  vs.  } H_1: \theta \in \Theta_1 
$$
for some nonintersecting $\Theta_0$ and $\Theta_1$

. . .

Example

- $H_0: \beta_2=0$ (education does not affect wages)
- $H_1: \beta_2\neq 0$ (education affects wages)
 

#### Definition of a Test

Informally, a test is a *decision rule*: you see the sample and then you decide in favor of $H_0$ or $H_1$


. . . 

<br>

Formally:

<div class="rounded-box">

::: {#def-vector-inference-test}

A test $T$ is a function of the sample $(X_1, \dots, X_N)$ to the space $\curl{\text{Reject} H_0,  \text{Do not reject }H_0}$

:::

</div>




#### Power

<br>

<div class="rounded-box">

::: {#def-vector-inference-power}

The power function $\text{Power}_T(\theta)$ of the test $T$ is the probability that $T$ rejects if $\theta$ is the true parameter value:
$$
\text{Power}_T(\theta) = P(T(X_1, \dots, X_N)=\text{Reject }H_0|\theta)
$$

:::

</div>

 

#### Significance Level

Maximal power under the null has a special name 
<div class="rounded-box">

::: {#def-vector-inference-significane}

The significance level $\alpha$ of the test $T$ is 
$$
\alpha = \max_{\theta\in\Theta_0} \text{Power}_T(\theta)
$$

:::

</div>

In other words, the probability of falsely rejecting the null (type I error)


#### What Defines a Good Test? 

The best possible test has perfect detection:

- Never rejects under $H_0$
- Always reject under $H_1$

. . . 

<br>

Usually impossible in practice. Instead we ask

- Not too much false rejection under $H_0$ (e.g. $\leq 5\%$ of the time)
- As much rejection as possible under $H_1$ 

#### Test Consistency

- For given sample sizes, usually cannot power function
- Substitute requirement that asymptoticall you detect any failure of $H_0$:
 
. . .

<div class="rounded-box">

::: {#def-vector-inference-significance}

$T$ is *consistent* if for any $\theta\in \Theta_1$ 
$$ \small
 \lim_{N\to\infty} P(T(X_1, \dots, X_N)=\text{Reject }H_0|\theta) = 1
$$

:::

</div>

::: {.callout-important appearance="minimal"}

As with estimators, we say "test" when we mean a sequence of tests, one for each sample size.

:::

::: footer

:::


#### Asymptotic Significance Level

- Also, can't necessarily control rejection under $H_0$ in finite
- But can require it asymptotically


<div class="rounded-box">

::: {#def-vector-inference-asy-significane}

The asymptotic significance level $\alpha$ of the test $T$ is 
$$
\alpha = \lim_{N\to\infty} \max_{\theta\in\Theta_0}  P(T(X_1, \dots, X_N)=\text{Reject }H_0|\theta) 
$$

:::

</div>


## Linear Hypotheses {background="#00100F"}

### A Single Linear Hypothesis {background="#43464B" visibility="uncounted"}

#### Single Example Hypothesis

Let's start with our first empirical question:

<div class="rounded-box">

Does education affect wages?

</div>

. . .

<br>

In the framework of @eq-vector-inference-emp-model can translate to 
$$
H_0: \beta_2 = 0, \quad H_1: \beta_2\neq 0
$$

. . .

Here $\Theta_1 = \curl{0}$ and $\Theta_2 = \R - \curl{0}$



#### How Testing Works in General

*How do we construct a test/decision rule?*

<br>

. . .

The basic approach to testing is surprisingly simple

1. Pick a "statistic" (=some known function of the data) that behaves "differently" under $H_0$ and $H_1$
2. Is the observed value of the statistic compatible with $H_0$? If not, reject $H_0$ in favor or $H_1$. If yes, do not reject $H_0$


#### Picking a Statistic

- In principle, can pick any statistic. Some are more "standard"
- For testing hypotheses about coefficients, there are three main classes
  - Wald statistics: need only *unrestricted* estimates 
  - Lagrange multiplier (LM): need *restricted* estimates 
  - Likelihood ratio (LR): need both

. . .
 

::: {.callout-note appearance="minimal"}

Wald tests easiest to work with in linear models, but others have their uses in different contexts 
:::


#### Convergence of $\hat{\beta}_2$

Recall asymptotic distribution result for OLS estimator
$$\small
\sqrt{N}\left( \hat{\bbeta}- \bbeta \right) \xrightarrow{d} N(0, \avar(\hat{\bbeta}))
$$

. . . 

It implies (why?) that
$$ \small
\dfrac{\hat{\beta}_2 - \beta_2}{\sqrt{ \avar(\hat{\beta}_2)/N }  } \xrightarrow{d} N\left(0, 1\right)
$$
where $\avar(\hat{\beta}_2)$ is the (2, 2) element of $\avar(\hat{\bbeta})$


#### $t$-statistic

Suppose that we have a consistent estimator of $\avar(\hat{\bbeta})$:
$$ \small
\widehat{\avar}(\hat{\bbeta}) \xrightarrow{p} \avar(\hat{\bbeta})
$$

. . .

Then if $H_0: \beta_2 = 0$, by Slutsky's theorem (why?) it holds that
$$ \small
t = \dfrac{\hat{\beta}_2 - \beta_2}{\sqrt{ \widehat{\avar}(\hat{\beta}_2)/N }  } \xrightarrow{d} N\left(0, 1\right)
$$
$t$ is called a $t$-statistic; $\sqrt{ \widehat{\avar}(\hat{\beta}_2)/N }$ â€” standard error of $\hat{\beta}_2$

::: footer

:::

#### Decision Rule: Test  {#sec-ols-inference-t-test}

We call the following the "asymptotic level $\alpha$ $t$-test":


<br>

<div class="rounded-box">

Let $z_{1-\alpha/2} = \Phi^{-1}(1-\alpha/2)$. Then

- Reject $H_0$ is $\abs{t}>z_{1-\alpha/2}$
- Do not reject $H_0$ is $\abs{t}\leq z_{1-\alpha/2}$


</div>


#### $t$-Statistic under $H_0$




#### $t$-Statistics under $H_1$


#### $t$-Test for $H_0:\beta_2 = c$

More generally, can test
$$
H_0: \beta_k = c \text{ vs } H_1: \beta_k \neq c
$$

. . . 

$t$-statistic
$$
t = \dfrac{\hat{\beta}_2 - c}{\sqrt{ \widehat{\avar}(\hat{\beta}_k)/N }  }
$$

Same decision rule as on @sec-ols-inference-t-test


#### Estimating $\avar(\hat{\bbeta})$

One remaining issue: how to estimate 
$$
\avar(\hat{\bbeta}) =  \left( \E[\bX_i\bX_i']\right)^{-1} \E[U_i^2\bX_i\bX_i']\left( \E[\bX_i\bX_i']\right)^{-1} 
$$

. . . 


Turns out that 




#### Practical Implementation


#### Summary


### Multiple Linear Hypothesis {background="#43464B" visibility="uncounted"}

#### Potentially Multiple

#### 
 

Special case

Wald statistics is the square of the $t$-statistic

- You lose nothing by doing a Wald test even if you have a single hypothesis
 


## Nonlinear Hypotheses and the Delta Method  {background="#00100F"}

### The Delta Method {background="#43464B" visibility="uncounted"}

####

### Nonlinear Wald Tests  {background="#43464B" visibility="uncounted"}

#### 

A

## Confidence Intervals and Sets {background="#00100F"}

####

## Recap and Conclusions {background="#00100F"}
  
#### Recap

In this lecture we

1. Did
   
#### Next Questions

<br>

How 

#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::