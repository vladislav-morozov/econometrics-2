---
title: "Limit Distribution of the OLS Estimator"
subtitle: "Normality as an Asymptotic Approximation"
author: Vladislav Morozov  
format:
  revealjs:
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "A Deeper Look at Linear Regression: Asymptotic Normality"
    footer-logo-link: "https://vladislav-morozov.github.io/econometrics-2/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#045D5D"
    data-footer: " "
filters:
  - reveal-header  
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
---



## Introduction {background="#00100F"}
  
### Lecture Info {background="#43464B" visibility="uncounted"}


#### Learning Outcomes

This lecture is about a 

<br>

By the end, you should be able to

- R

#### Textbook References
 

::: {.nonincremental}

 
- Refresher on probability: 
    - Your favorite probability textbook (e.g. chapter 5 in @Bertsekas2008IntroductionProbability)
    - Sections B-C in @Wooldridge2020IntroductoryEconometricsModern
- Asymptotic theory for the OLS estimator
    - 5.2 and E4 in @Wooldridge2020IntroductoryEconometricsModern
    - Or 7.3 in @Hansen2022Econometrics

  
::: 



## Motivation {background="#00100F"}

### Motivating Empirical Example {background="#43464B" visibility="uncounted"}


#### Setting: Linear Causal Model

<br> 

We'll continue to work in the linear causal model with potential outcomes:
$$
Y_i^\bx = \bx'\bbeta + U_i
$$
 
#### Motivating Empirical Example: Variables

- $Y_i$ — hourly log wage
- $\bx$ — education and job experience in years
- $U_i$ — unobserved characteristics (skill, health, etc.), assumed to satisfy $\E[U_i|\bX_i]=0$
- Sample: some suitably homogeneous group (e.g. white married women)

#### Motivating Empirical Example: Potential Outcomes
 
$$
\begin{aligned}[]
& [\ln(\text{wage}_i)]^{\text{(education, experience)}} \\
&  =   \beta_1 + \beta_2 \times \text{education} \\
& \quad  + \beta_3 \times  \text{experience} + \beta_4 \times  \dfrac{\text{experience}^2}{100} + U_i
\end{aligned}
$$

. . . 
 
- Can write model in terms of realized variables, but above emphasizes causal assumption
- Divide experience$^2$ by 100 for numerical reasons

#### Motivating Empirical Example: Parameters of Interest

 
<br>

Our parameters of interest: 

1. $100\beta_2$ — (more or less) average effect of additional year of education in percent
2. $100\beta_3 + 20 \beta_4$ — average effect of increasing education for individuals with 10 years of experience
3. $-50\beta_3/\beta_4$ — experience level which maximizes expected log wage

#### Motivating Empirical Example: Data {.scrollable}


- `cps09mar` — a selection from the March 2009 US Current Population Survey: 
- Can be obtained from the [website](https://users.ssc.wisc.edu/~bhansen/econometrics/) for @Hansen2022Econometrics
- Sample: married white women with present spouses

<br> 

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Expand for full data preparation code"
import numpy as np
import pandas as pd
import statsmodels.api as sm

from statsmodels.regression.linear_model import OLS

# Read in the data
data_path = ("https://github.com/pegeorge/Econ521_Datasets/"
             "raw/refs/heads/main/cps09mar.csv")
cps_data = pd.read_csv(data_path)

# Generate variables
cps_data["experience"] = cps_data["age"] - cps_data["education"] - 6
cps_data["experience_sq_div"] = cps_data["experience"]**2/100
cps_data["wage"] = cps_data["earnings"]/(cps_data["week"]*cps_data["hours"] )
cps_data["log_wage"] = np.log(cps_data['wage'])

# Retain only married women white with present spouses
select_data = cps_data.loc[
    (cps_data["marital"] <= 2) & (cps_data["race"] == 1) & (cps_data["female"] == 1), :
]

# Construct X and y for regression 
exog = select_data.loc[:, ['education', 'experience', 'experience_sq_div']]
exog = sm.add_constant(exog)
endog = select_data.loc[:, "log_wage"]
```

::: footer

:::

#### Motivating Empirical Example: Estimation Results 

```{.python code-line-numbers="0-1"}
results = OLS(endog, exog).fit(cov_type='HC0') # Robust covariance matrix estimator
print(results.summary())
```

```{python} 
results = OLS(endog, exog).fit(cov_type='HC0')
print(results.summary())
```


#### Empirical Questions


<br> 

1. How certain are we of our estimates of target parameters?
2. Does education matter at all? (up to our statistical confidence)
3. Is the best amount of experience to have equal to 15 years? (up to our statistical confidence)


### Translating to Theory {background="#43464B" visibility="uncounted"}

#### Goal: Inference

<br> 

Recall: 

<div class="rounded-box">

Inference is about answering questions about the population based on the finite sample

</div>

<br>

All of our questions — examples of inference

#### Challenge: Randomness

Key challenge:

<div class="rounded-box">

We only see a random sample instead of the full population

</div>
In other words: our estimated values are also random and do not perfectly reflect the underlying population values

. . . 

 
- How to quantify whether we are close or far to the true parameters? ($\Rightarrow$ confidence intervals)
- Are the values we obtained compatible with our hypotheses? ($\Rightarrow$ hypothesis testing)

#### Necessary Object: Distribution of Estimator


<br>

<div class="rounded-box">

To answer these questions, we need the distribution of the estimator for given sample size

</div>

. . . 

<br> 

How do you get this distribution if you only have one sample? 


#### Possible Approach: Distributional Assumptions

- It is possible to impose exact distributional assumptions on the data (i.e., $(\bX_i, \bU_i))$
- Example: assuming $U_i|\bX_i\sim N(\cdot, \cdot)$ (e.g. chapter 4 in @Wooldridge2020IntroductoryEconometricsModern)

. . . 

Such approaches usually problematic:

- How do you justify such assumptions?
- If these distributions have unknown parameters, how do you estimate them and quantify uncertainty about those parameters?  
 

#### Other Approaches
 

- Nonasymptotic/finite-sample analysis based on "high-probability bounds":
  - Usually require making assumptions about tails of the data (e.g. that $U_i$ has bounded support),
  - See examples in @Mohri2018FoundationsMachineLearning and  @Wainwright2019HighDimensionalStatisticsNonAsymptotic for some examples
- Large-sample approximations using tools like the central limit theorem(s) --- topic of this lecture


 


## Probability Background {background="#00100F"}

### Definitions {background="#43464B" visibility="uncounted"}

#### Convergence in Distribution

<div class="rounded-box">


::: {#def-vector-distribution-convergence-distribution}

Let $\bX_1, \bX_2, \dots$ and $\bX$ be random vectors in $\R^q$. Let $\bX_N$ have CDF $F_N(\cdot)$ and $\bX$ have CDF $F(\cdot)$. $\curl{\bX_N}$ converges *in distribution* (converges *weakly*) to $\bX$ if 

$$
\lim_{N\to\infty} F_N(\bx) = F(\bx)
$$
for every $x\in\R^q$ such that $F(\cdot)$ is continuous at $\bx$

:::

</div>

Convergence in distribution is labeled $\bX_N\Rightarrow \bX$ or $\bX_N\xrightarrow{d} \bX$

#### Convergence in Distribution vs. in Probability

<div class="rounded-box">


::: {#prp-vector-distribution-convergence-contrast}

1. If $\bX_N\xrightarrow{p} \bX$, then $\bX_N\xrightarrow{p} \bX$
2. If $\bX_N\xrightarrow{d} \bX$ and $\bX$ is not random (=constant), then $\bX_N\xrightarrow{p} \bX$

:::

</div>

Convergence in probability is generally *stronger* than convergence in distribution

### Tools for Working with Convergence in Distribution {background="#43464B" visibility="uncounted"}



#### Working with Vectors: Mean and Variance

In this class we work with random vectors (such as $\bX_iU_i$)

. . .

<br>

Some notation:

- Mean of random vector $\bX$ is the vector $\bmu = \E[\bX]$: coordinates of $\bmu$ — means of coordinates of $\E[\bX]$
- Variance-covariance matrix of $\bX$ is the matrix  
$$
\var(\bX) = \E[(\bX-\bmu)(\bX-\bmu)']
$$



#### Key Tool: (Multivariate) CLT

<div class="rounded-box">


::: {#prp-vector-distribution-clt}

Let $\bX_1, \bX_2, \dots$ be a sequence of random vectors such that

1. $\bX_i$ are independently and identically distributed (IID)
2. $\E[\norm{\bX_i}^2]<\infty$ and $\bmu=\E[\bX_i]$

Then 
$$ \small
 \sqrt{N}\left(  \dfrac{1}{N}\sum_{i=1}^N \bX_i - \bmu  \right) \xrightarrow{d} N(0, \var(\bX) )
$$ 

:::

</div>


#### Visualizing The CLT

 
```{python}
#| eval: false 

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from scipy.stats import beta, norm, nct
from scipy.ndimage import uniform_filter1d
from statsmodels.distributions.empirical_distribution import ECDF
from pathlib import Path


plt.rcParams["font.family"] = "sans-serif"
plt.rcParams["font.sans-serif"] = ["Arial"]

BG_COLOR = "whitesmoke"

# Function to generate samples from a given distribution
def generate_samples(distribution, params, size):
    return distribution.rvs(*params, size=size)

# Parameters for the Beta distribution
beta_params = (0.5, 0.5)

# Parameters for the noncentral t distribution
nct_params = (3, 10, 0, 1)  # df=3, nc=10, loc=0, scale=1

# Choose the distribution (uncomment the desired distribution)
# distribution = beta
# params = beta_params
distribution = nct
params = nct_params

# Generate sample sizes
sample_sizes = np.concatenate(
    [
        np.arange(5, 101),   
        np.arange(100, 201, 4),  
        np.arange(200, 501, 5),   
        np.arange(500, 4001, 13),  
    ]
).astype(int)

# Initialize the history of standardized sample means for smoothing
standardized_means_history = []

# Set up the figure and the axes
fig, ax = plt.subplots(figsize=(12, 6))
fig.patch.set_facecolor(BG_COLOR)
fig.patch.set_edgecolor("teal")
fig.patch.set_linewidth(5)


overall_samples = generate_samples(distribution, params, size=(80000, sample_sizes.max()))
population_mean = np.mean(overall_samples)
population_std = np.std(overall_samples)

# Animation function: this is called sequentially
def animate(i):
    ax.clear()

    # Get the current sample size
    n = sample_sizes[i]

    # Draw samples from the chosen distribution
    samples = overall_samples[:, :n]

    # Compute the sample means
    sample_means = np.mean(samples, axis=1)

    # Standardize the sample means
    standardized_means = (sample_means - population_mean) / population_std * np.sqrt(n)

    # Compute the empirical CDF
    ecdf = ECDF( standardized_means)
    x_vals = np.linspace(-4, 4, 1000)
    y_vals = ecdf(x_vals)

    # Plot the empirical CDF
    ax.plot(x_vals, y_vals, label=f'Sample Size = {n}', color='darkorange')

    # Plot the standard normal CDF for reference
    ax.plot(x_vals, norm.cdf(x_vals), label='Standard Normal', color='teal', linestyle='--')

    ax.set_xlim(-4, 4)
    ax.set_ylim(-0.03, 1.03) 
    ax.set_title('Convergence of CDF of Sample Mean to Standard Normal CDF',
                 loc='left')
    ax.legend(loc='lower right')
    ax.set_facecolor(BG_COLOR)

# Call the animator
WriterMP4 = animation.writers["ffmpeg"]
writer_mp4 = WriterMP4(fps=50, metadata=dict(artist="g_ww"), bitrate=1800)
ani = animation.FuncAnimation(
    fig,
    animate,
    frames=len(sample_sizes),
    repeat=True,
)
ani.save(Path("images").resolve() / "clt_cdf.mp4", writer=writer_mp4)

plt.close()

```

<video width="99%" style="display: block; margin: 0 auto;" controls muted autoplay loop>
  <source src="images/clt_cdf.mp4 " type="video/webm">
</video> 
 
 
#### Practical Interpetation: Scalar Case

The CLT (@prp-vector-distribution-clt) for scalar data states that for sufficiently large $N$ 
$$
P\left(\dfrac{N^{-1}\sum_{i=1}^N X_i - \E[X_i]  }{\sqrt{ \var(X_i)/N  }}\leq x \right) \approx \Phi(x)
$$
where $\Phi(\cdot)$ is the standard normal. In other words,
$$
\bar{X}\sim N\left( \E[X_i], \dfrac{\var(X_i)}{N} \right)
$$

::: footer

Vector version in the next lecture

:::
 

#### Tool: Continuous Mapping Theorem



<div class="rounded-box">

::: {#prp-vector-distribution-cmt}

Let $\bX_N\xrightarrow{d}\bX$, and let $f(\cdot)$ be continuous is some neighborhood of all the possible values of  $\bX$.

Then  
$$
f(\bX_N) \xrightarrow{d} f(\bX)
$$
:::

</div>
In words: convergence in distribution is also preserved under continuous transformations


#### CMT: Scalar Example

Consider scalar case with $\sqrt{N}(\bar{X}-\mu)\Rightarrow Z$ for $Z\sim N(0, \var(X_i))$. Then
$$
N(\bar{X}-\mu)^2 \xrightarrow{d} Z^2
$$

. . .


::: {.callout-important appearance="minimal"}


Note: the above is not the same as looking at 
$$
\sqrt{N}([\bar{X}]^2 - \mu^2)
$$ {#eq-vector-distribution-predelta}
The function $f$ is applied to the elements of the convergent sequence

:::

::: footer

But next lecture will discuss how to talk about @eq-vector-distribution-predelta using the CLT

:::

#### CMT: Vector Example

In the vector case, label

- $\bZ_N= \sqrt{N}\left(  \dfrac{1}{N}\sum_{i=1}^N \bX_i - \E[\bX_i]  \right)$ with $q$ coordinates
- $\bZ\sim N(0, \var(\bX) )$
- $\bA$ — some $q\times q$ matrix

. . .

Then 

$$
\bZ_N'\bA\bZ_N\xrightarrow{d} \bZ'\bA\bZ
$$

#### Tool: Slutsky's Theorem

We w

#### Warning: $c$ Must Be Constant



## Asymptotic Normality of the OLS Estimator {background="#00100F"}
  
### Model {background="#43464B"}

####

Model 


#### Discussion

- Can also prove the result directly with 
- `nonrobust` — in practice there is never a use case 


#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::