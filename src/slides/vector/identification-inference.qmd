---
title: "Identification and Inference"
subtitle: "The Two Halves of Statistics"
author: Vladislav Morozov  
format:
  revealjs:
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "Identification and Inference"
    footer-logo-link: "https://vladislav-morozov.github.io/econometrics-2/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#045D5D"
    data-footer: " " 
embed-resources: true
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
---




## Introduction {background="#00100F"}
  
### Lecture Info {background="#43464B"}


#### Learning Outcomes

This lecture is about a 

<br>

By the end, you should be able to

- R

```{python}
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np


plt.rcParams["font.family"] = "sans-serif"
plt.rcParams["font.sans-serif"] = ["Arial"]

FIG_HEIGHT = 7
FIG_RATIO = 2.3
```


#### References
 

::: {.nonincremental}

 
- Textbooks:
    - Section 2.32 in @Hansen2022Econometrics: brief definitions
    - [Chapter 5](https://theeffectbook.net/ch-Identification.html){preview-link="true"} in @Huntington-Klein2025EffectIntroductionResearch: more examples and philosophical points
    - Sections 15.1-15.2 @Wooldridge2020IntroductoryEconometricsModern: IV example
    
- (*Advanced*): a *very* expansive dictionary article by @Lewbel2019IdentificationZooMeanings

  
::: 


### Identification and Inference {background="#43464B"}

#### Overall Goal

Goal of all of statistics:

<br> 

<div style="border: 2px solid #ccc; padding: 9px; border-radius: 15px; margin-bottom: 10px;">

"Say something" about a "parameter" of interest "based on data"

</div>

<br>

Which "parameter"? What is "something"?  How much "data"?


#### Parameters of Interest

Which parameter you want depends on the context: 

- Causal settings: 
  - Treatment effects: averages, variances, ... 
  - Features of structural economic models: elasticities, multipliers, ...
- Prediction: 
  - Forecast of GDP, ...
  - Whether the patient has a disease ...
  
#### Kind of Questions

What about the "something"? 

. . . 

Three example questions:

1. Can the parameter be learned at all? 
2. If we have an estimate, how sure are we about it?
3. Does the parameter satisfy some constraints? (equal to 0, positive, etc.)


#### Identification and Inference

All possible questions can be split into two classes of work: 

1. <span class="highlight">Identification</span>: what could we learn if we had an infinite amount of data? 
2. <span class="highlight">Estimation and inference</span>: how to "learn" from limited data? 

. . .

<br>

Both equally important in causal settings. Identification less/not important in prediction


## Identification {background="#00100F"}
  

### General Idea {background="#43464B"}


#### Parameter Label

Focus first on identification

<br>

. . .

Let $\theta$ be the "parameter of interest" — something unknown you care about, e.g.

- Average treatment effect
- Coefficient vector
- Even an unknown function

#### Population Distribution $F_{X, Y}$

Suppose that our data — observations on $(X, Y)$

. . .

<br>

How to express the idea of having "infinite data"? 

. . . 

<br>

For us:
<div style="border: 2px solid #ccc; padding: 9px; border-radius: 15px; margin-bottom: 10px;">

Infinite data = knowing the joint distribution function $F_{X, Y}$


</div> 
 

<!-- #### Observed Data

<br>

::: {#fig-actual}
``` tikz  
%%| fig-cap: Observed data arises from some underlying data generating process 
%%| fig-align: center
\begin{tikzpicture}
 \tikzset{line width=1.5pt, outer sep=0pt,
 ell/.style={draw,fill=white, inner sep=2pt,
 line width=1.5pt},
 swig vsplit={gap=5pt,
 inner line width right=0.5pt}};

 
 
\node[name=activity, ell, shape=ellipse, align=left]{True data \\ generation};

\node[name=observed, ell, shape=ellipse, align=left, right = 5mm of activity]{Actual \\ $F_{X, Y}(x, y)$}; 
 
\draw[->](activity) to (observed);  
\end{tikzpicture}
``` 
:::


<br>

Identification: assume that we see the full $F_{X, Y}(x, y)$ -->

#### Models and Parameters Imply Distributions of Data

::: {#fig-model}
``` tikz  
%%| fig-cap: Path from parameter $\theta$ to the *implied* data distribution
%%| fig-align: center
\begin{tikzpicture}
 \tikzset{line width=1.5pt, outer sep=0pt,
 ell/.style={draw,fill=white, inner sep=2pt,
 line width=1.5pt},
 swig vsplit={gap=5pt,
 inner line width right=0.5pt}};

 

\node[name=theta, ell, shape=ellipse]{Model($\theta$)};
 
\node[name=observed, ell, shape=ellipse, align=left, right = 5mm of theta]{Implications$(\theta)$ \\ for  $F_{X, Y}$}; 

\draw[->](theta) to (observed);  
\end{tikzpicture}
``` 
:::

The "model" specifies all or parts of a generating mechanism: 

- Parts of $F_{X, Y}$ might be unknown even if you know $\theta$
- Example: linear model with exogeneity say that $\E[Y_i|\bX_i]$ is linear in $\bX_i$, but doesn't say much about the distribution of $\bX_i$ or $Y_i$ beyond that

#### Definition of Identification

Identification basically asks: 
 
<div style="border: 2px solid #ccc; padding: 9px; border-radius: 15px; margin-bottom: 10px;">

Given the 

1. The joint distribution $F_{X, Y}$ of the data
2. Assumptions that the model is true for some $\theta_0$
3. "Implications"$(\theta_0)$ of the model,

can $\theta_0$ be uniquely determined?

</div> 
 

### General Idea {background="#43464B"}


#### Fully Parametric Case: Intro

May sound a bit vague

. . .

<br>

To make idea simpler, a special <span class="highlight">parametric</span> case 

- Model fully determines the distribution of the data up $\theta$  
- If you know $\theta$, you know distribution of the data

#### Example

Consider simple example: 

- $Y_i\sim N(\theta_0, 1)$, no $X_i$ (model)
- Parameter of interest is <span class="highlight">$\theta$</span>

. . . 

<br> 

Implication of the model:

- $F_{Y}$ is a normal distribution with mean $\theta_0$ and variance 1
- Known up to $\theta$, can label the distribution as $F_Y(y|\theta_0)$ 
 
#### Identification of $\theta$
 

Let's try our definition of identification:

1. Distribution of the data tells us $\E[Y_i]$
2. The model tells us that $\E[Y_i]$ must be $\theta_0$

. . . 

Therefore, it must be that 
$$
\theta_0 = \E[Y_i]
$$
$\theta_0$ uniquely determined as the above function of the distribution of the data


#### Another View of Identification

 

<span class="highlight">Equivalent</span> way to state definition of identification
<div style="border: 2px solid #ccc; padding: 9px; border-radius: 15px; margin-bottom: 10px;">

$\theta_0$ is identified if for any $\theta\neq\theta_0$ it holds that
$$
F_Y(y|\theta) \neq F_Y(y|\theta_0)
$$

</div> 


<br> 

In words: different $\theta$ give different distributions of observed data


#### Visual Example: Difference in Distributions

```{python}
from scipy.stats import norm
fig, ax = plt.subplots(1, 1, figsize=(FIG_HEIGHT * FIG_RATIO, FIG_HEIGHT))
BG_COLOR = "whitesmoke"
fig.patch.set_facecolor(BG_COLOR)
fig.patch.set_edgecolor("teal")
fig.patch.set_linewidth(5)
x = np.linspace(norm.ppf(0.0001), norm.ppf(0.9999), 100)
ax.plot(
    x,
    norm.cdf(x),
    color = "teal",
    lw=4,
    alpha=0.6,
    label="CDF under $\\theta$",
)
ax.plot(
    x,
    norm.cdf(x, loc=1),
    color = "darkorange",
    linestyle = "--",
    lw=4,
    alpha=0.6,
    label="CDF under $\\theta'$",
)
ax.set_xlabel("$y$")
ax.legend()
ax.set_facecolor(BG_COLOR)
plt.show()
```
 


#### Example of Non-Identification

Second definition useful for showing <span class="highlight">non-identification</span>

. . . 

<br>

An example: suppose that $Y_i \sim N(\abs{\theta_0}, 1)$:

- If $\theta = 1$, then $Y_i$ should be $N(1, 1)$
- If $\theta = -1$, then $Y_i$ should be $N(1, 1)$

. . . 


<div style="border: 2px solid #ccc; padding: 9px; border-radius: 15px; margin-bottom: 10px;">

Different $\theta$ give the same distribution = $\theta_0$ not identified if $\theta_0\neq 0$

</div>

#### Visual Illustration: Same Distribution

```{python}
from scipy.stats import norm
fig, ax = plt.subplots(1, 1,  figsize=(FIG_HEIGHT * FIG_RATIO, FIG_HEIGHT)) 
fig.patch.set_facecolor(BG_COLOR)
fig.patch.set_edgecolor("teal")
fig.patch.set_linewidth(5)
x = np.linspace(norm.ppf(0.0001), norm.ppf(0.9999), 100)
ax.plot(
    x,
    norm.cdf(x, loc=1),
    color = "teal",
    lw=4,
    linestyle = "-.",
    alpha=0.6,
    label="CDF under $\\theta=-1$",
)
ax.plot(
    x,
    norm.cdf(x, loc=1),
    color = "darkorange",
    linestyle = "--",
    lw=4,
    alpha=0.6,
    label="CDF under $\\theta=1$",
)
ax.set_xlabel("$y$")
ax.legend()
ax.set_facecolor(BG_COLOR)
plt.show()
```
  
### Identification in Linear Model with Exogeneity {background="#43464B"}

#### Towards a More Complex Example

#### Setting: Potential Outcomes

$$
Y^{\bx}_i = \bx'\bbeta + U_i
$$

What we see: realized values of $(Y_i, \bX_i)$. The realized outcomes are determined as 
$$
Y_i = Y^{\bX_i}_i
$$

####


#### Motivation 

"Identification strategy" refers to imposing some assumptions

It is a fundamentally theoretical exercise

#### Discussion

Identification is the property of the model

Something you do with "sample" arguments

Sometimes you do it with 

@Lewbel2019IdentificationZooMeanings




@Wooldridge2020IntroductoryEconometricsModern defines identification directly as being able to estimate it consistently

- Good definition for practice - if we cannot, then what use is the parameter? 


#### 

Linear model: if not unique, the other models give the same 





## Estimation and Inference {background="#00100F"}
  

#### Broad

fif

Goal of next Lectures


#### Goal of Estimation 

In prediction, also care: how uncertain are we about our prediction? 

In prediction, there are two  sources of uncertainty 

## Recap and Conclusions {background="#00100F"}
  
#### Recap

#### Next Questions


#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::
