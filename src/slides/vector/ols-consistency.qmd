---
title: "Consistency of the OLS Estimator"
subtitle: "Convergence as Sample Size Grows"
author: Vladislav Morozov  
format:
  revealjs:
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "A Deeper Look at Linear Regression: Consistency"
    footer-logo-link: "https://vladislav-morozov.github.io/econometrics-2/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#045D5D"
    data-footer: " "
filters:
  - reveal-header  
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
---



## Introduction {background="#00100F"}
  
### Lecture Info {background="#43464B"}


#### Learning Outcomes

This lecture is about a 

<br>

By the end, you should be able to

- R

#### Textbook References
 

::: {.nonincremental}

 
- Refresher on probability: 
    - Your favorite probability textbook (e.g. 5 in @Bertsekas2008IntroductionProbability)
    - Sections B-C in @Wooldridge2020IntroductoryEconometricsModern
- Consistency of the OLS estimator
    - 7.1-7.2 in @Hansen2022Econometrics
    - *Or* 5.1 and E4 in @Wooldridge2020IntroductoryEconometricsModern
    

  
::: 

#### Consistency as Basic Requirement
 
Want estimators with good properties 

. . . 

<br>

<span class =  "highlight">Consistency</span> is a minimal required property for a "sensible" estimator

. . .

<br>

Informally:
<div class="rounded-box">
  An estimation procedure is consistent if it get the target parameter right as sample size grows infinite large
</div>


## Probability Background {background="#00100F" visibility="uncounted"}

### Definitions {background="#43464B" visibility="uncounted"}


#### Reminder: Convergence of a Deterministic Sequence


Recall: 

<div class="rounded-box">

::: {#def-vector-consistency-deterministic-convergence}

Let $\bx_1, \bx_2, \dots$ be a sequence of vectors in $\R^p$. Then $\bx_n$ converges to some $\bx\in \R^p$  if for any $\varepsilon>0$ there exists an $N_0$ such that for all $N\geq N_0$
$$
\norm{ \bx_N - \bx } < \varepsilon
$$

:::

</div>
 
Here $\norm{\cdot}$ is the Euclidean norm on $\R^p$: if $\by = (y_1, \dots, y_p)$, then $\norm{\by} = \sqrt{ \sum_{i=1}^p y_i^p   }$
 


#### Towards Formalizing Convergence

- Let $\theta\in \R^p$
- Sample of size $N$ is $(X_1, \dots, X_N)$
- Recall: estimators $\curl{\hat{\theta}_k}_{k=\min N}^{\infty}$ are a sequence of functions. $N$th estimator maps $(X_1, \dots, X_N)$ to $\R^p$ to produce *estimates*

. . .

<br> 

Sample is random $\Rightarrow$ each $\hat{\theta}_N(X_1, \dots, X_N)$ is random

. . .

$\Rightarrow$ How to formalize convergence?

::: footer
See [Wikipedia](https://en.wikipedia.org/wiki/Convergence_of_random_variables)
:::
 

#### Convergence in Probability: Definition

<br> 

<div class="rounded-box">

::: {#def-vector-consistency-convergence-ip}

Let $\bX_1, \bX_2, \dots$ be a sequence of random vectors in $\R^p$. Then $\bX_N$ converges to some $\bX\in \R^p$ <span class="highlight">in probability</span> if for any $\varepsilon>0$  it holds that 
$$
\lim_{N\to\infty} P(\norm{\bX_N - \bX}>\varepsilon) = 0
$$

:::

</div>

#### Convergence in Probability: Discussion

<br> 

- The limit $\bX$ can be random or deterministic 
- Convergence in probability written $\bX_n\xrightarrow{p}\bX$
- $\bX_N\xrightarrow{p} \bX$ is the same as $\bX_N-\bX\xrightarrow{p} 0$
- Convergence of vector/matrix is equivalent to convergence of every element
- Sometimes terminology "with probability approaching 1" (w.p.a. 1) is used

#### Definition of Consistency

<div class="rounded-box">

::: {#def-vector-consistency-consistency}

The estimator (sequence) $\hat{\theta}_N$ is consistent for $\theta$ if as $N\to\infty$ 
$$
\hat{\theta}_N(X_1, \dots, X_N) \xrightarrow{p} \theta
$$

:::

</div>

Note: we usually use the word "estimator" to refer to the whole sequence $\curl{\hat{\theta}_N}$

### Tools for Showing Consistency {background="#43464B" visibility="uncounted"}

#### Two Approaches To Showing Consistency

1. Qualitative: just that convergence happens
  - Relies on laws of large numbers (LLNs) and related results
  - Approach in this course
2. Quantitive: shows that convergence happens and answers *how fast*
  - Usually based on <span class="highlight">concentration inequalities</span>
  - Check out chapter 2 in @Wainwright2019HighDimensionalStatisticsNonAsymptotic


::: footer
Wikipedia gives a [list](https://en.wikipedia.org/wiki/Concentration_inequality) of some concentration inequalities. Check 
:::

#### Tool: (Weak) Law of Large Numbers
 

<div class="rounded-box">

::: {#prp-vector-consistency-lln}

Let $\bX_1, \bX_2, \dots$ be a sequence of random vectors such that

1. $\bX_i$ are independently and identically distributed (IID)
2. $\E[\norm{\bX_i}]<\infty$

Then 
$$ \small
\dfrac{1}{N} \sum_{i=1}^N \bX_i \xrightarrow{p} \E[\bX_i]
$$
:::

</div>
 

#### Visualization  

#### Tool: Continuous Mapping Theorem



<div class="rounded-box">

::: {#prp-vector-consistency-lln}

Let $\bX_N\xrightarrow{p}\bX$, and let $f(\cdot)$ be continuous.

Then  
$$
f(\bX_N) \xrightarrow{p} f(\bX)
$$
:::

</div>
In words: convergence in probability is preserved under continuous transformations


#### CMT Examples

Simple examples

1. If $X_N$ is scalar and $X_n\xrightarrow{p}X$, then 
   - $X_N^2 \xrightarrow{p} X^2$
   - $\max\curl{0, X_N}\xrightarrow \max\curl{0, X}$
2. If $\bX_N\to \bX$, $\bX_N\in \R^p$ and $\bA_N\xrightarrow{p}\bA$, $\bA_N\in\R^{k\times p}$, then 
   $$
   \bA_N\bX_N\xrightarrow{p} \bA\bX
   $$




## Consistency of the OLS Estimator {background="#00100F" visibility="uncounted"}
   

### Model-Free Convergence  {background="#43464B" visibility="uncounted"}

#### OLS

Converges to $\E[\bX_i\bX_i']^{-1}\E[\bX_i\bX_i]$

No "model", no "potential outcomes" --- just correlations



### Convergence under Exogeneity  {background="#43464B" visibility="uncounted"}



#### 

Can always say! 


$\bbeta + \E[\bX_i\bX_i']\E[\bX_i\bU_i(\bbeta)]$



Model-free in teh sense that by itself writing $Y_i = \bX_i'\bbeta + \bU_i(\bbeta)$ does not say anything (a bit like writing $5 = X + (5-X)$)

#### Potential Outcomes Framework

If we want $\bbeta$ to have any causal meaning, we need a casual framework 

So let's make the usual assumption

####

In this class we will maintain SUTVA --- no general equilbrium, "only your known treatment matters"

- Not always true, think about policies which apply to everyone
- Insert tutoring example

#### Consistency of the OLS Estimator

Combining the steps together
$$
\hat{\bbeta} \xrightarrow{p} \bbeta
$$

#### Discussion

- Work with "sampling" properties of realised values
- Use the *assumed* structure to connect to causally interpretable paramtesr

#### Discussion of Assumptions

- iid bad?

#### Orthogonality

Let's think about the proof again

We don't actually need $\E[U_i|\bX_i]=0$

Sufficient to have 
$$
\E[\bX_iU_i] = 0
$$
This is $k$ conditions now --- one per component of $\bX_i$

#### Consistency Under Orthogonality

Things go through



#### What Do We Lose Without Strict Exogeneity?

Still can specify the potential outcomes frameworok

But we lose mean interpretation: 
$$
\E[Y_i|\bX_i] = \bX_i'\bbeta + \E[U_{it}|\bX_i]
 $$
 Now maybe $ \E[U_{it}|\bX_i]$ is not zero. 


In finite samples you may have bias!
Consistency result shows that in the limit you are still estimating the correct thing

 
#### Visualization



#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::