---
title: "Consistency of the OLS Estimator"
subtitle: "Convergence as Sample Size Grows"
author: Vladislav Morozov  
format:
  revealjs:
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "A Deeper Look at Linear Regression: Consistency"
    footer-logo-link: "https://vladislav-morozov.github.io/econometrics-2/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#045D5D"
    data-footer: " "
filters:
  - reveal-header  
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
---



## Introduction {background="#00100F"}
  
### Lecture Info {background="#43464B"}


#### Learning Outcomes

This lecture is about a 

<br>

By the end, you should be able to

- Provide definitions of consistency and convergence in probability
- Discuss 
- Handle the case of non-invertible $\bX'\bX$
- 

#### Textbook References
 

::: {.nonincremental}

 
- Refresher on probability: 
    - Your favorite probability textbook (e.g. 5 in @Bertsekas2008IntroductionProbability)
    - Sections B-C in @Wooldridge2020IntroductoryEconometricsModern
- Consistency of the OLS estimator
    - 7.1-7.2 in @Hansen2022Econometrics
    - *Or* 5.1 and E4 in @Wooldridge2020IntroductoryEconometricsModern
    

  
::: 

#### Consistency as Basic Requirement
 
Want estimators with good properties 

. . . 

<br>

<span class =  "highlight">Consistency</span> is a minimal required property for a "sensible" estimator

. . .

<br>

Informally:
<div class="rounded-box">
  An estimation procedure is consistent if it get the target parameter right as sample size grows infinite large
</div>


## Probability Background {background="#00100F" visibility="uncounted"}

### Definitions {background="#43464B" visibility="uncounted"}


#### Reminder: Convergence of a Deterministic Sequence


Recall: 

<div class="rounded-box">

::: {#def-vector-consistency-deterministic-convergence}

Let $\bx_1, \bx_2, \dots$ be a sequence of vectors in $\R^p$. Then $\bx_n$ converges to some $\bx\in \R^p$  if for any $\varepsilon>0$ there exists an $N_0$ such that for all $N\geq N_0$
$$
\norm{ \bx_N - \bx } < \varepsilon
$$

:::

</div>
 
Here $\norm{\cdot}$ is the Euclidean norm on $\R^p$: if $\by = (y_1, \dots, y_p)$, then $\norm{\by} = \sqrt{ \sum_{i=1}^p y_i^p   }$
 


#### Towards Formalizing Convergence

- Let $\theta\in \R^p$
- Sample of size $N$ is $(X_1, \dots, X_N)$
- Recall: estimators $\curl{\hat{\theta}_k}_{k=\min N}^{\infty}$ are a sequence of functions. $N$th estimator maps $(X_1, \dots, X_N)$ to $\R^p$ to produce *estimates*

. . .

<br> 

Sample is random $\Rightarrow$ each $\hat{\theta}_N(X_1, \dots, X_N)$ is random

. . .

$\Rightarrow$ How to formalize convergence?

::: footer
See [Wikipedia](https://en.wikipedia.org/wiki/Convergence_of_random_variables)
:::
 

#### Convergence in Probability: Definition

<br> 

<div class="rounded-box">

::: {#def-vector-consistency-convergence-ip}

Let $\bX_1, \bX_2, \dots$ be a sequence of random vectors in $\R^p$. Then $\bX_N$ converges to some $\bX\in \R^p$ <span class="highlight">in probability</span> if for any $\varepsilon>0$  it holds that 
$$
\lim_{N\to\infty} P(\norm{\bX_N - \bX}>\varepsilon) = 0
$$

:::

</div>

#### Convergence in Probability: Discussion

<br>


- The limit $\bX$ can be random or deterministic 
- Convergence in probability written $\bX_n\xrightarrow{p}\bX$
- $\bX_N\xrightarrow{p} \bX$ is the same as $\bX_N-\bX\xrightarrow{p} 0$

#### Two Important Characterizations 


<div class="rounded-box">

::: {#prp-vector-consistency-characterizations-vector}

Let $\bA_N, \bA$ be a $m \times n$ matrices with $(i, j)$th element $a_{ij, N}$ and $a_{ij}$. Then
$$
\bA_N\xrightarrow{p}\bA   \Leftrightarrow a_{ij, N} \xrightarrow a_{ij}
$$
:::

</div>

. . . 

<br>

<div class="rounded-box">

::: {#prp-vector-consistency-characterizations-open}

If $\bX_N\xrightarrow{p}\bX$, then $P(\bX_N\in U)\to 1$ for any open neighborhood $U$ of $\bX$
:::

</div>

 



#### Definition of Consistency

<div class="rounded-box">

::: {#def-vector-consistency-consistency}

The estimator (sequence) $\hat{\theta}_N$ is consistent for $\theta$ if as $N\to\infty$ 
$$
\hat{\theta}_N(X_1, \dots, X_N) \xrightarrow{p} \theta
$$

:::

</div>

Note: we usually use the word "estimator" to refer to the whole sequence $\curl{\hat{\theta}_N}$

### Tools for Showing Consistency {background="#43464B" visibility="uncounted"}

#### Two Approaches To Showing Consistency

1. Qualitative: just that convergence happens
  - Relies on laws of large numbers (LLNs) and related results
  - Approach in this course
2. Quantitive: shows that convergence happens and answers *how fast*
  - Usually based on <span class="highlight">concentration inequalities</span>
  - Check out chapter 2 in @Wainwright2019HighDimensionalStatisticsNonAsymptotic


::: footer
Wikipedia gives a [list](https://en.wikipedia.org/wiki/Concentration_inequality) of some concentration inequalities 
:::

#### Tool: (Weak) Law of Large Numbers
 

<div class="rounded-box">

::: {#prp-vector-consistency-lln}

Let $\bX_1, \bX_2, \dots$ be a sequence of random vectors such that

1. $\bX_i$ are independently and identically distributed (IID)
2. $\E[\norm{\bX_i}]<\infty$

Then 
$$ \small
\dfrac{1}{N} \sum_{i=1}^N \bX_i \xrightarrow{p} \E[\bX_i]
$$
:::

</div>
 

#### Visualization  

#### Tool: Continuous Mapping Theorem



<div class="rounded-box">

::: {#prp-vector-consistency-lln}

Let $\bX_N\xrightarrow{p}\bX$, and let $f(\cdot)$ be continuous is some neighborhood of all the possible values of  $\bX$.

Then  
$$
f(\bX_N) \xrightarrow{p} f(\bX)
$$
:::

</div>
In words: convergence in probability is preserved under continuous transformations


#### CMT Examples

Simple examples

1. If $X_N$ is scalar and $X_n\xrightarrow{p}X$, then 
   - $X_N^2 \xrightarrow{p} X^2$
   - $\max\curl{0, X_N}\xrightarrow \max\curl{0, X}$
2. If $\bX_N\to \bX$, $\bX_N\in \R^p$ and $\bA_N\xrightarrow{p}\bA$, $\bA_N\in\R^{k\times p}$, then 
   $$
   \bA_N\bX_N\xrightarrow{p} \bA\bX
   $$




## Consistency of the OLS Estimator {background="#00100F" visibility="uncounted"}
   

#### Returning to the OLS Estimator

Let's go back to the OLS estimator based on sample $(\bX_1, Y_1), \dots, (\bX_N, Y_N)$

. . . 

<br>

<div class="rounded-box">

Is the OLS estimator consistent? 

</div>

<br>

Consistent for what?


### Model-Free Convergence  {background="#43464B" visibility="uncounted"}



#### Invertibility of $\bX'\bX$

First an unpleasant technical issue

<div class="rounded-box">

How to handle non-invertible $\bX'\bX$? 

</div>

<br>

Some fall-back known value $\bc$ 
$$
\hat{\bbeta} = \begin{cases}
(\bX'\bX)^{-1}\bX'\bY, & \bX'\bX \text{ invertible}\\
\bc, & \bX'\bX \text{ not invertible}
\end{cases}
$$ 
Now $\hat{\bbeta}$ is always defined

#### Representation in Terms of Averages

First lecture shows that 
$$
\bX'\bX = \sum_{i=1}^N \bX_i\bX_i', \quad \bX'\bY = \sum_{i=1}^N \bX_i Y_i
$$

Then we can write (under invertibility)
$$
(\bX'\bX)\bX'\bY = \left(\textcolor{teal}{\dfrac{1}{N}} \sum_{i=1}^N \bX_i\bX_i'\right)^{-1} \left( \textcolor{teal}{\dfrac{1}{N}} \sum_{i=1}^N \bX_i Y_i\right)
$$

#### Assumption

Make the following assumptions:

1. $(\bX)


#### Handling


The inverse function $\bA\to \bA^{-1}$ is continuous on the space of invertible matrices

@prp-vector-consistency-characterizations-open


::: footer
See [this StackExchange discussion](https://math.stackexchange.com/questions/84392/why-do-the-n-times-n-non-singular-matrices-form-an-open-set) for more details
:::

#### 

With probability approaching 1 
$$
1
$$
Accordingly, we only need to study what happens to $(\bX'\bX)^{-1}\bX'\bY$

There is a technicality 

#### 

In the future you can always write directly 
$$
\hat{\bbeta}  = (\bX'\bX)^{-1}\bX'\bY
$$
if the assumption that $\E[\bX_i\bX_i']$ is invertlbe holds

#### Limit 



Converges to $\E[\bX_i\bX_i']^{-1}\E[\bX_i\bX_i]$
No "model", no "potential outcomes" --- just correlations

- Called the "population project" because it is, just that projections defined in a more general sense that you might be used to

::: footer

See [Wikipedia](https://en.wikipedia.org/wiki/Inner_product_space#Random_variables) on inner products of random variables

:::

### Convergence under Exogeneity  {background="#43464B" visibility="uncounted"}


 

#### Potential Outcomes Framework

Let's go back to our causal framework

#### Sampling Error Representation

$$
\hat{\bbeta} = 
$$
we retain the same invertibility 

#### Consistency of the OLS Estimator

Combining the steps together
$$
\hat{\bbeta} \xrightarrow{p} \bbeta
$$

#### Discussion

- Work with "sampling" properties of realised values
- Use the *assumed* structure to connect to causally interpretable paramtesr

#### Discussion of Assumptions

- iid bad?

#### Orthogonality

Let's think about the proof again

We don't actually need $\E[U_i|\bX_i]=0$

Sufficient to have 
$$
\E[\bX_iU_i] = 0
$$
This is $k$ conditions now --- one per component of $\bX_i$

#### Consistency Under Orthogonality

Things go through



#### What Do We Lose Without Strict Exogeneity?

Still can specify the potential outcomes frameworok

But we lose mean interpretation: 
$$
\E[Y_i|\bX_i] = \bX_i'\bbeta + \E[U_{it}|\bX_i]
 $$
 Now maybe $ \E[U_{it}|\bX_i]$ is not zero. 


In finite samples you may have bias!
Consistency result shows that in the limit you are still estimating the correct thing

 
#### Visualization


#### Identification

This convergence result is another way to prove the identification result for $\bbeta$ 

The probability limit of $\hat{\bbeta}$ can be computed from the joint distribution $F_{\bX, Y}$. At the same time equal to $\bbeta$. Fits our definition


## Recap and Conclusions {background="#00100F" visibility="uncounted"}
  
#### Recap

In this lecture we

1. Discussed the difference between identification, estimation, and inference
2. Saw definitions of identification
3. Reviewed potential outcomes
4. Discussed identification in the linear model under exogeneity



#### Next Questions

- Linear model: inference on $\bbeta$ based on the OLS estimator
  - Quantifying uncertainty
  - Hypothesis testing
- Identification in various settings: IV, panel, nonlinear, etc.



#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::