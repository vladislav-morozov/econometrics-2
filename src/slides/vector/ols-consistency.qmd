---
title: "Consistency of the OLS Estimator"
subtitle: "Convergence as Sample Size Grows"
author: Vladislav Morozov  
format:
  revealjs:
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "A Deeper Look at Linear Regression: Consistency"
    footer-logo-link: "https://vladislav-morozov.github.io/econometrics-2/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#045D5D"
    data-footer: " "
filters:
  - reveal-header  
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
---



## Introduction {background="#00100F"}
  
### Lecture Info {background="#43464B" visibility="uncounted"}


#### Learning Outcomes

This lecture is about a 

<br>

By the end, you should be able to

- Provide definitions of consistency and convergence in probability
- Discuss 
- Handle the case of non-invertible $\bX'\bX$
- 

#### Textbook References
 

::: {.nonincremental}

 
- Refresher on probability: 
    - Your favorite probability textbook (e.g. 5 in @Bertsekas2008IntroductionProbability)
    - Sections B-C in @Wooldridge2020IntroductoryEconometricsModern
- Consistency of the OLS estimator
    - 7.1-7.2 in @Hansen2022Econometrics
    - *Or* 5.1 and E4 in @Wooldridge2020IntroductoryEconometricsModern
    

  
::: 

#### Consistency as Basic Requirement
 
Want estimators with good properties 

. . . 

<br>

<span class =  "highlight">Consistency</span> is a minimal required property for a "sensible" estimator

. . .

<br>

Informally:
<div class="rounded-box">
  An estimation procedure is consistent if it get the target parameter right as sample size grows infinite large
</div>


## Probability Background {background="#00100F"}

### Definitions {background="#43464B" visibility="uncounted"}


#### Reminder: Convergence of a Deterministic Sequence


Recall: 

<div class="rounded-box">

::: {#def-vector-consistency-deterministic-convergence}

Let $\bx_1, \bx_2, \dots$ be a sequence of vectors in $\R^p$. Then $\bx_n$ converges to some $\bx\in \R^p$  if for any $\varepsilon>0$ there exists an $N_0$ such that for all $N\geq N_0$
$$
\norm{ \bx_N - \bx } < \varepsilon
$$

:::

</div>
 
Here $\norm{\cdot}$ is the Euclidean norm on $\R^p$: if $\by = (y_1, \dots, y_p)$, then $\norm{\by} = \sqrt{ \sum_{i=1}^p y_i^p   }$
 


#### Towards Formalizing Convergence

- Let $\theta\in \R^p$
- Sample of size $N$ is $(X_1, \dots, X_N)$
- Recall: estimators $\curl{\hat{\theta}_k}_{k=\min N}^{\infty}$ are a sequence of functions. $N$th estimator maps $(X_1, \dots, X_N)$ to $\R^p$ to produce *estimates*

. . .

<br> 

Sample is random $\Rightarrow$ each $\hat{\theta}_N(X_1, \dots, X_N)$ is random

. . .

$\Rightarrow$ How to formalize convergence?

::: footer
See [Wikipedia](https://en.wikipedia.org/wiki/Convergence_of_random_variables)
:::
 

#### Convergence in Probability: Definition

<br> 

<div class="rounded-box">

::: {#def-vector-consistency-convergence-ip}

Let $\bX_1, \bX_2, \dots$ be a sequence of random vectors in $\R^p$. Then $\bX_N$ converges to some $\bX\in \R^p$ <span class="highlight">in probability</span> if for any $\varepsilon>0$  it holds that 
$$
\lim_{N\to\infty} P(\norm{\bX_N - \bX}>\varepsilon) = 0
$$

:::

</div>

#### Convergence in Probability: Discussion

<br>


- The limit $\bX$ can be random or deterministic 
- Convergence in probability written $\bX_n\xrightarrow{p}\bX$
- $\bX_N\xrightarrow{p} \bX$ is the same as $\bX_N-\bX\xrightarrow{p} 0$

#### Two Important Characterizations 


<div class="rounded-box">

::: {#prp-vector-consistency-characterizations-vector}

Let $\bA_N, \bA$ be a $m \times n$ matrices with $(i, j)$th element $a_{ij, N}$ and $a_{ij}$. Then
$$
\bA_N\xrightarrow{p}\bA   \Leftrightarrow a_{ij, N} \xrightarrow a_{ij}
$$
:::

</div>

. . . 

<br>

<div class="rounded-box">

::: {#prp-vector-consistency-characterizations-open}

$\bX_N\xrightarrow{p}\bX$ if and only if $P(\bX_N\in U)\to 1$ for any open set $U$ that contains $\bX$
:::

</div>

 



#### Definition of Consistency

<div class="rounded-box">

::: {#def-vector-consistency-consistency}

The estimator (sequence) $\hat{\theta}_N$ is consistent for $\theta$ if as $N\to\infty$ 
$$
\hat{\theta}_N(X_1, \dots, X_N) \xrightarrow{p} \theta
$$

:::

</div>

Note: we usually use the word "estimator" to refer to the whole sequence $\curl{\hat{\theta}_N}$

### Tools for Showing Consistency {background="#43464B" visibility="uncounted"}

#### Two Approaches To Showing Consistency

1. Qualitative: just that convergence happens
  - Relies on laws of large numbers (LLNs) and related results
  - Approach in this course
2. Quantitive: shows that convergence happens and answers *how fast*
  - Usually based on <span class="highlight">concentration inequalities</span>
  - Check out chapter 2 in @Wainwright2019HighDimensionalStatisticsNonAsymptotic


::: footer
Wikipedia gives a [list](https://en.wikipedia.org/wiki/Concentration_inequality) of some concentration inequalities 
:::

#### Tool: (Weak) Law of Large Numbers
 

<div class="rounded-box">

::: {#prp-vector-consistency-lln}

Let $\bX_1, \bX_2, \dots$ be a sequence of random vectors such that

1. $\bX_i$ are independently and identically distributed (IID)
2. $\E[\norm{\bX_i}]<\infty$

Then 
$$ \small
\dfrac{1}{N} \sum_{i=1}^N \bX_i \xrightarrow{p} \E[\bX_i]
$$
:::

</div>
 

#### Visualization  

#### Tool: Continuous Mapping Theorem



<div class="rounded-box">

::: {#prp-vector-consistency-cmt}

Let $\bX_N\xrightarrow{p}\bX$, and let $f(\cdot)$ be continuous is some neighborhood of all the possible values of  $\bX$.

Then  
$$
f(\bX_N) \xrightarrow{p} f(\bX)
$$
:::

</div>
In words: convergence in probability is preserved under continuous transformations


#### CMT Examples

Simple examples

1. If $X_N$ is scalar and $X_n\xrightarrow{p}X$, then 
   - $X_N^2 \xrightarrow{p} X^2$
   - $\max\curl{0, X_N}\xrightarrow \max\curl{0, X}$
2. If $\bX_N\to \bX$, $\bX_N\in \R^p$ and $\bA_N\xrightarrow{p}\bA$, $\bA_N\in\R^{k\times p}$, then 
   $$
   \bA_N\bX_N\xrightarrow{p} \bA\bX
   $$




## Consistency of the OLS Estimator {background="#00100F"}
   

#### Returning to the OLS Estimator

Let's go back to the OLS estimator based on <span class="highlight"> IID</span>  sample $(\bX_1, Y_1), \dots, (\bX_N, Y_N)$

. . . 

<br>

<div class="rounded-box">

Is the OLS estimator consistent? 

</div>

<br>

Consistent for what?


### Convergence Without a Causal Model {background="#43464B" visibility="uncounted"}



#### Invertibility of $\bX'\bX$

First an unpleasant technical issue

<div class="rounded-box">

How to handle non-invertible $\bX'\bX$? 

</div>

<br>

Some fall-back known value $\bc$ 
$$
\hat{\bbeta} = \begin{cases}
(\bX'\bX)^{-1}\bX'\bY, & \bX'\bX \text{ invertible}\\
\bc, & \bX'\bX \text{ not invertible}
\end{cases}
$$ 
Now $\hat{\bbeta}$ is always defined, but does $\bc$ matter?

#### Representation in Terms of Averages

First lecture shows that 
$$
\bX'\bX = \sum_{i=1}^N \bX_i\bX_i', \quad \bX'\bY = \sum_{i=1}^N \bX_i Y_i
$$

Then we can write (under invertibility)
$$
(\bX'\bX)\bX'\bY = \left(\textcolor{teal}{\dfrac{1}{N}} \sum_{i=1}^N \bX_i\bX_i'\right)^{-1} \left( \textcolor{teal}{\dfrac{1}{N}} \sum_{i=1}^N \bX_i Y_i\right)
$$

#### Limits of Averages

Can handle averages. If 

- $\E[\norm{\bX_i\bX_i'}]<\infty$
- $\E[\norm{\bX_iY_i}]<\infty$

then by the WLLN  (@prp-vector-consistency-lln)
$$
\begin{aligned}
\dfrac{1}{N} \sum_{i=1}^N \bX_i\bX_i' \xrightarrow{p} \E[\bX_i\bX_i'], \quad 
\dfrac{1}{N} \sum_{i=1}^N \bX_iY_i \xrightarrow{p} \E[\bX_i Y_i]
\end{aligned}
$$

 

#### Handling the Inverse  of $\bX'\bX$

Two facts: 

- The inverse function $\bA\to \bA^{-1}$ is continuous on the space of invertible matrices
- The set of invertible matrices is open
  
. . . 

So if $\E[\bX_i\bX_i']$ is invertible, then by @prp-vector-consistency-characterizations-open  and the CMT (@prp-vector-consistency-cmt)

- $P(\frac{1}{N} \sum_{i=1}^N \bX_i\bX_i' \text{ is invertible})\to 1$
- $(\sum_{i=1}^N \bX_i\bX_i')^{-1}  \xrightarrow{p} \left(\E[\bX_i\bX_i']\right)^{-1}$




  


::: footer
See [this StackExchange discussion](https://math.stackexchange.com/questions/84392/why-do-the-n-times-n-non-singular-matrices-form-an-open-set) for more details
:::

#### $\bc$ Does Not Matter

Since $\frac{1}{N} \sum_{i=1}^N \bX_i\bX_i'$ is invertible with probability approaching 1 (w.p.a. 1), then w.p.a.1 it holds
$$
\hat{\bbeta} =  (\bX'\bX)^{-1}\bX'\bY =  \left( \dfrac{1}{N} \sum_{i=1}^N \bX_i\bX_i'\right)^{-1} \left( \dfrac{1}{N} \sum_{i=1}^N \bX_i Y_i\right)
$$

. . . 

It follows that if $\bc\neq \E[\bX_i\bX_i']^{-1}\E[\bX_iY_i]$, then
$$
P(\hat{\bbeta}= \bc) \to 0
$$

#### Combining Together

<div class="rounded-box">

::: {#prp-vector-consistency-ols-model-free}

Let

1. $(\bX_i, Y_i)$ be IID
2. $\E[\norm{\bX_i\bX_i'}]<\infty$, $\E[\norm{\bX_iY_i}]<\infty$
3. $\E[\bX_i\bX_i']$ be invertible

Then
$$
\hat{\bbeta} \xrightarrow{p} \left( \E[\bX_i\bX_i'] \right)^{-1} \E[\bX_iY_i]
$$


:::

</div>

#### Quicker Way of Writing
 
It is common and acceptable (also on the exam) to directly write
$$
\hat{\bbeta}  = (\bX'\bX)^{-1}\bX'\bY
$$
<span class = "highlight"> provided that you </span>

1. Are talking about asymptotic properties (consistency, asymptotic distributions, asymptotic confidence intervals)
2. Make the assumption that $\E[\bX_i\bX_i']$ is invertible and say that $\bX'\bX$ is invertible w.p.a.1

#### Discussion
 

<br>

 
- @prp-vector-consistency-ols-model-free: no causal framework
- OLS just measures covariances in general
- Limit $\left( \E[\bX_i\bX_i'] \right)^{-1} \E[\bX_iY_i]$ is called "population projection of $Y_i$ on $\bX_i$"

::: footer

See [Wikipedia](https://en.wikipedia.org/wiki/Inner_product_space#Random_variables) on inner products of random variables

:::

### Convergence under Causal Model with Exogeneity  {background="#43464B" visibility="uncounted"}


 

#### Potential Outcomes Framework

Let's go back to our causal framework

#### Sampling Error Representation

$$
\hat{\bbeta} = 
$$
we retain the same invertibility 

#### Consistency of the OLS Estimator

Combining the steps together
$$
\hat{\bbeta} \xrightarrow{p} \bbeta
$$

#### Discussion

- Work with "sampling" properties of realised values
- Use the *assumed* structure to connect to causally interpretable paramtesr

#### Discussion of Assumptions

We made may assumptions 
- IID
- Causal framework + SUTVA
- Invertibility and existence of moments
- Exogeneity

#### Orthogonality

Let's think about the proof again

We don't actually need $\E[U_i|\bX_i]=0$

Sufficient to have 
$$
\E[\bX_iU_i] = 0
$$
This is $k$ conditions now --- one per component of $\bX_i$

#### Consistency Under Orthogonality

Things go through



#### What Do We Lose Without Strict Exogeneity?

Still can specify the potential outcomes frameworok

But we lose mean interpretation: 
$$
\E[Y_i|\bX_i] = \bX_i'\bbeta + \E[U_{it}|\bX_i]
 $$
 Now maybe $ \E[U_{it}|\bX_i]$ is not zero. 


In finite samples you may have bias!
Consistency result shows that in the limit you are still estimating the correct thing

 
#### Visualization


#### Identification

This convergence result is another way to prove the identification result for $\bbeta$ 

The probability limit of $\hat{\bbeta}$ can be computed from the joint distribution $F_{\bX, Y}$. At the same time equal to $\bbeta$. Fits our definition


## Recap and Conclusions {background="#00100F"}
  
#### Recap

In this lecture we

1. Discussed the difference between identification, estimation, and inference
2. Saw definitions of identification
3. Reviewed potential outcomes
4. Discussed identification in the linear model under exogeneity



#### Next Questions

- Linear model: inference on $\bbeta$ based on the OLS estimator
  - Quantifying uncertainty
  - Hypothesis testing
- Identification in various settings: IV, panel, nonlinear, etc.



#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::