---
title: "Exercises: Vector Linear Model and Asymptotics"
format:
  html:
    toc: true
---



## Theoretical Exercises 

### Testing Scalar Restrictions

Let the outcome $Y_i$, the covariates $\bX_i$, and an unobserved component $U_i$ be linked through the linear causal model
$$
Y_i^{\bx} = \bx'\bbeta + U_i.
$$
Suppose that we observe an IID sample of data on $Y_i, \bX_i$, that $\E[U_i|\bX_i]=0$, that $\E[\bX_i\bX_i']$ is invertible, and that $\E[U_i^2\bX_i\bX_i']$ has maximal rank.

1. Consider the hypotheses $H_0: \beta_k = c$ and $H_1: \beta_k\neq c$, where $\beta_k$ is the $k$th coordinate of the $\bbeta$ vector. Propose a consistent test for $H_0$ vs $H_1$ that has asymptotic size $\alpha$.
2. Now let $\ba\neq 0$ be some known constant vector of the same dimension as $\bbeta$. Consider the hypotheses $H_0: \ba'\bbeta = c$ and $H_1: \ba'\bbeta\neq c$. Propose a consistent $t$-test for $H_0$ vs $H_1$ that has asymptotic size $\alpha$.
3. Why do we require that $\ba\neq 0$ in the previous question?

In both cases remember to show that your test is consistent and has the desired asymptotic size. 

<details>
  <summary>Click to see the solution</summary>

*First subquestion*: to choose our test statistic, we observe two facts:

- We are dealing with a scalar hypothesis, 
- The OLS estimator is consistent and asymptotically normal (why?).

Accordingly, we can use the t-test. The t-statistic is given by
$$
t = \dfrac{\hat{\bbeta}_k - c}{ \sqrt{ \widehat{\avar}(\hat{\bbeta})/N  } },
$$
where $\hat{\bbeta}$ is the OLS estimator,  $\widehat{\avar}(\hat{\bbeta})$ is some consistent estimator of $\avar(\bbeta)$ (e.g. the HC0 estimator from the lectures) 
 
Our test is based on the following decision rule. Let $z_{1-\alpha/2}$ be the $(1-\alpha/2)$th quantile of the standard normal distribution. Then:

- If $\abs{t}>z_{1-\alpha/2}$, we reject $H_0$.
- If $\abs{t}\leq z_{1-\alpha/2}$, we do not reject $H_0$.

We now need to show that this test is consistent and has the desired asymptotic size.

- **Consistency**: We need to show that the probability of rejecting $H_0$ converges to 1 when $H_0$ is false. Let $\beta_k$ be the true value of the coefficient of interest, and write
$$
t = \dfrac{\hat{\bbeta}_k - \beta_k}{ \sqrt{ \widehat{\avar}(\hat{\bbeta})/N  } } + \dfrac{ \beta_k- c}{ \sqrt{ \widehat{\avar}(\hat{\bbeta})/N  } }.
$$
By our asymptotic normality results, the first term converges in distribution to a $N(0, 1)$ random variable. By our assumptions, $\widehat{\avar}(\hat{\bbeta})\xrightarrow{p} \avar(\hat{\bbeta})\neq 0$.  Under the alternative, $\beta_k\neq c$, and so the second term diverges to $\pm \infty$. It then follows that with probability approaching one $\abs{t}> z_{1-\alpha/2}$ for any $\beta_k\neq c$. In other words, consistency holds. 

- **Asymptotic size**: We need to show that the probability of rejecting $H_0$ converges to $\alpha$ when $H_0$ is true. Under $H_0$ it holds that $\beta_k=c$, and thus our asymptotic results and Slutsky's theorem imply that
$$
t = \dfrac{\hat{\bbeta}_k - \beta_k}{ \sqrt{ \widehat{\avar}(\hat{\bbeta})/N  } } \xrightarrow{d} N(0, 1).
$$ 
By definition of convergence of probability, definition of $z_{1-\alpha/2}$ and the fact that $z_{1-\alpha/2} = -z_{\alpha/2}$, it holds that
$$
\begin{aligned}
& P\left(\text{Reject} H_0|H_0 \right) = P\left(\abs{t}>z_{1-\alpha/2} |H_0\right) \\
& = P\left( \abs{ \dfrac{\hat{\beta}_k-c}{\sqrt{ \widehat{\avar}(\hat{\beta}_k)/N }  }}> z_{1-\alpha/2}\Bigg|H_0 \right)\\
& \to \Phi(z_{\alpha/2}) + (1- \Phi(z_{1-\alpha/2})) = \alpha 
\end{aligned}.
$$
The test has asymptotic size $\alpha$.

<br>

*Second subquestion*: the question explicitly asks for a $t$-test, and so we use the following $t$-statistic as the basis for our test:
$$
t = \dfrac{\ba'\hat{\bbeta} - c}{ \sqrt{ \widehat{\avar}(\ba'\hat{\bbeta})/N  } },
$$ {#eq-exercises-inference-t-single-linear}
The key question is how to construct a suitable estimator $\widehat{\avar}(\ba'\bbeta)$ for $\avar(\ba'\bbeta)$. 

By the continuous mapping theorem it holds that
$$
\sqrt{N}(\ba'\hat{\bbeta}- \ba'\bbeta) \xrightarrow{d} N(0, \ba'\avar(\bbeta)\ba).
$$
By the continuous mapping theorem again:
$$
\ba'\widehat{\avar}(\hat{\bbeta})\ba \xrightarrow{p}\ba'{\avar}(\hat{\bbeta})\ba  = \avar(\ba'\hat{\bbeta})
$$
Hence, we can use $\ba'\widehat{\avar}(\hat{\bbeta})\ba$ as $\widehat{\avar}(\ba'\bbeta)$ in @eq-exercises-inference-t-single-linear. With this choice, it follows by Slutsky's theorem that 
$$
\dfrac{\ba'\hat{\bbeta} - \ba'\bbeta}{ \sqrt{ \widehat{\avar}(\ba'\hat{\bbeta})/N  } } \xrightarrow{d} N(0, 1).
$$ {#eq-exercises-inference-t-single-linear-limit}

Our decision rule is analogous to the above one:

- If $\abs{t}>z_{1-\alpha/2}$, we reject $H_0$.
- If $\abs{t}\leq z_{1-\alpha/2}$, we do not reject $H_0$.

Consistency and asymptotic size can be shown entirely analogously to the above case by using @eq-exercises-inference-t-single-linear-limit (show them regardless to practice!).

<br>

*Third subquestion*: if $\ba=0$, then the null hypothesis is trivially true and reduces to $H_0: 0=c$. It is either trivially true or trivially false, depending on $c$. 


<br> 
 


</details>
 
### Testing Several Linear Restrictions

Let the outcome $Y_i$, the covariates $\bX_i$, and an unobserved component $U_i$ be linked through the linear causal model
$$
Y_i^{\bx} = \bx'\bbeta + U_i.
$$
Suppose that we observe an IID sample of data on $Y_i, \bX_i$, that $\E[U_i|\bX_i]=0$, that $\E[\bX_i\bX_i']$ is invertible, and that $\E[U_i^2\bX_i\bX_i']$ has maximal rank. Let $\bbeta = (\beta_1, \beta_2, \dots, \beta_p)$ with $p\geq 4$.

Consider the following two hypotheses on $\bbeta$:
$$
H_0: \begin{cases}
\beta_1 = 0, \\
\beta_2 - \beta_3 = 1, \\
\beta_2 = 4\beta_4 + 5,
\end{cases} \quad H_1: \text{at least one equality in $H_0$ fails}
$$
Propose a consistent test for $H_0$ vs. $H_1$ with asymptotic size $\alpha$. Show that the test possesses these properties. 

<details>
  <summary>Click to see the solution</summary>

First, we write the null hypothesis in matrix form. We can do this by stacking the three equations in $H_0$ into a single vector equation:
$$
\begin{aligned}
H_0: & \bR\bbeta = \bq, \\
\bR & = \begin{pmatrix}
1 & 0 & 0 & 0 & \cdots\\
0 & 1 & -1 & 0 & \cdots\\
0 & 1 & 0 & -4 & \cdots
\end{pmatrix}, \quad \bq = \begin{pmatrix}
0\\
1\\
5
\end{pmatrix},
\end{aligned}
$$
where $\bR$ has zero columns starting from the fifth column. 

We can construct a Wald test for $H_0$ vs. $H_1$. The Wald statistic is defined as
$$
W = N\left(\bR\hat{\bbeta} - \bq \right)' \left(\bR\widehat{\avar}(\hat{\bbeta})\bR' \right)^{-1} \left( \bR\hat{\bbeta} - \bq  \right)
$$
 
We propose the following test. Let $c_{1-\alpha}$ be the $(1-\alpha)$th quantile of the $\chi^2_{3}$ distribution (3 is the number of constraints in $H_0$). Then

- If $W>c_{1-\alpha}$, we reject $H_0$.
- If $W\leq c_{1-\alpha}$, we do not reject $H_0$.



We now need to show that this test is consistent and has the desired asymptotic size.


- **Asymptotic size**: Under $H_0$ it holds that  $\bR\bbeta=\bq$, and so by our asymptotic results for the OLS estimator and the continuous mapping theorem under $H_0$
$$
\sqrt{N}(\bR\hat{\bbeta}-\bq) \xrightarrow{d} N(0, \bR\avar(\hat{\bbeta})\bR').
$$ 
By Slutsky's theorem and the definition of $\chi^2_{\cdot}$ random variables, it holds under $H_0$ that
$$
W \xrightarrow{d} \chi^2_3.
$$
By definition of $c_{1-\alpha}$ and the definition of convergence in distribution
$$
\begin{aligned}
& P\left(\text{Reject} H_0|H_0 \right) = P\left(W>c_{1-\alpha} |H_0\right) \\
& \to P(\chi^2_3> c_{1-\alpha}) = \alpha .
\end{aligned}
$$
The test has asymptotic size $\alpha$.


- **Consistency**: Under $H_1$ we have that $\bR\hat{\bbeta}\xrightarrow{p} \bR\bbeta\neq \bq$. In words, the outside terms in $W$ converge to something $\neq 0$. At the same time $\left(\bR\widehat{\avar}(\hat{\bbeta})\bR' \right)^{-1} \xrightarrow{p} \left(\bR\avar(\hat{\bbeta})\bR'\right)^{-1}$ â€” a positive definite matrix. We conclude that 
$$
\begin{aligned}
& \left(\bR\hat{\bbeta} - \bq \right)' \left(\bR\widehat{\avar}(\hat{\bbeta})\bR' \right)^{-1} \left( \bR\hat{\bbeta} - \bq  \right) \\
& \xrightarrow{p} \left(\bR\bbeta -\bq\right)' \left(\bR\avar(\hat{\bbeta})\bR'\right)^{-1}(\bR\bbeta-\bq) \\
& >0,
\end{aligned}
$$
where we use the definition of positive definitiness. 
Finally, recall that there is also an $N$ term in $W$. We conclude that overall
$$
W\xrightarrow{p} \infty
$$
It follows that the probability of rejecting tends to 1 for any $\bbeta$ in $H_1$. In other words, consistency holds.



</details>

### Inference on a Nonlinear Function of Parameters

1. Hypothesis test
2. Confidence

<details>
  <summary>Click to see the solution</summary>

If the transformation were vector-valued, we would only be able to use the Wald test (as in the lectures)

</details>

### Consistency of the HC0 Asymptotic Variance Estimator



## Applied Exercises

Applied exercises in this list  

- @Wooldridge2020IntroductoryEconometricsModern   

Check out chapter 3 in @Heiss2024UsingPythonIntroductory 