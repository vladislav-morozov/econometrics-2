---
format:
  html:
    toc: true
---

# Exercises: Vector Linear Model and Asymptotics



## Regressing Fitted Values

Suppose that we observe data $(Y_1, \bX_1), \dots (Y_N, \bX_N)$. Let $\bY$ be the $N\times 1$ vector of outcomes, $\bX$ be the data matrix, and suppose that $\bX'\bX$ is invertible. 

Let the vector $\hat{\bY}$ of fitted values and the vector $\hat{\be}$ of errors be given by 
$$
\begin{aligned}
\hat{\bY} & = \bX\hat{\bbeta},\\
\hat{\be} & = \bY- \hat{\bY},
\end{aligned}
$$
where $\hat{\bbeta}$ is the OLS estimator. 

Find the OLS coefficient vector from

1. Regressing $\hat{Y}_i$ on $\bX_i$.
2. Regressing $\hat{e}_i$ on $\bX_i$.

In both cases, express the OLS estimator in terms of $\bY$ and $\bX$. Then express it in terms of $(Y_i, \bX_i)$, $i=1, \dots, N$.

##  A

Suppose that 

## Ratio Slope Estimator

Let $X_i$ and $U_i$ be scalar random variables. Suppose that $X_i$ satisfies $X_i\geq c>0$ for some $c$ (strictly positive and bounded away from 0). Let $Y_i$ be some outcome. Suppose that the following linear causal model holds: the potential outcome $Y_i^x$ is determined as
$$
Y_i^x = \beta x + U_i.
$$
The realized outcome $Y_i$ is determined as $Y_i = Y_i^{X_i}$.

Consider the following estimator for $\beta$:
$$
\tilde{\beta} = \dfrac{1}{N}\sum_{i=1}^N \dfrac{Y_i}{X_i}.
$$

1. Propose conditions under which $\tilde{\beta}$ is consistent for $\beta$ and prove consistency.
2. Derive the asymptotic distribution of $\tilde{\beta}$.
3. Now suppose that the causal model allows heterogeneous effects:
$$
Y_i^x = \beta_i x + U_i.
$$
Under which conditions does $\tilde{\beta}$ consistently estimate $\E[\beta_i]$?
 
## Limit of the Ridge Estimator

See section 6.2.1 in @James2023IntroductionStatisticalLearning about the ridge estimator and regularization techniques in general. 

## Measurement Error



## Omitted Variable Bias

What if htere is 

## Question 3
 

## Applied Exercises

1. @James2023IntroductionStatisticalLearning Exercise 2.9 